{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"On becoming an OBO Semantic Engineer \u00b6 Welcome to our OBO Semantic Engineering Training! Editorial Team \u00b6 James Overton , Knocean Inc. James has been a developer of software to support ontology development in and around the OBO sphere for more than a decade and delivers services to the OBO community through his company, Knocean Inc. Becky Jackson , Bend Informatics. Becky has been a Semantic Software developer and Ontology Pipeline specialist since 2016, now working as an independent consultant. Nicole Vasilevsky , Monarch Initiative. Nicole is a Visiting Associate Research Professor at University of Colorado Anschutz Medical Campus and is an Ontology Curator for ontologies such as Mondo Disease Ontology, the Human Phenotype Ontology, Uberon Anatomy Ontology, and Cell Ontology. Nicolas Matentzoglu , Semanticly Ltd. Nico is an Ontology Engineer and Ontology Pipeline Specialist, being the Principal Ontology Pipeline Developer for the Monarch Initiative. If you would like to contribute to this training, please find out more here . Content \u00b6 Course overview and lessons Getting started (read this first) How to contribute Acknowledgements \u00b6 Critical Path Institute \u00b6 Critical Path Institute (CPI) is an independent, nonprofit organisation dedicated to bringing together experts from regulatory agencies, industry, and academia to collaborate and improve the medical product development process. In April 2021, the CPI has commissioned the first version of this OBO course, contributing not only funding for the preparation and delivery of the materials, but also valuable feedback about the course contents and data for the practical exercises. We thank the CPI for contributing significantly to the OBO community and open science! https://c-path.org/","title":"About the course"},{"location":"#on-becoming-an-obo-semantic-engineer","text":"Welcome to our OBO Semantic Engineering Training!","title":"On becoming an OBO Semantic Engineer"},{"location":"#editorial-team","text":"James Overton , Knocean Inc. James has been a developer of software to support ontology development in and around the OBO sphere for more than a decade and delivers services to the OBO community through his company, Knocean Inc. Becky Jackson , Bend Informatics. Becky has been a Semantic Software developer and Ontology Pipeline specialist since 2016, now working as an independent consultant. Nicole Vasilevsky , Monarch Initiative. Nicole is a Visiting Associate Research Professor at University of Colorado Anschutz Medical Campus and is an Ontology Curator for ontologies such as Mondo Disease Ontology, the Human Phenotype Ontology, Uberon Anatomy Ontology, and Cell Ontology. Nicolas Matentzoglu , Semanticly Ltd. Nico is an Ontology Engineer and Ontology Pipeline Specialist, being the Principal Ontology Pipeline Developer for the Monarch Initiative. If you would like to contribute to this training, please find out more here .","title":"Editorial Team"},{"location":"#content","text":"Course overview and lessons Getting started (read this first) How to contribute","title":"Content"},{"location":"#acknowledgements","text":"","title":"Acknowledgements"},{"location":"#critical-path-institute","text":"Critical Path Institute (CPI) is an independent, nonprofit organisation dedicated to bringing together experts from regulatory agencies, industry, and academia to collaborate and improve the medical product development process. In April 2021, the CPI has commissioned the first version of this OBO course, contributing not only funding for the preparation and delivery of the materials, but also valuable feedback about the course contents and data for the practical exercises. We thank the CPI for contributing significantly to the OBO community and open science! https://c-path.org/","title":"Critical Path Institute"},{"location":"contributing/","text":"Contributing to OBO Semantic Engineering Tutorials \u00b6 We rely on our readers to correct our materials and add to them - the hope is to centralise all the usual teaching materials for OBO ontology professionals in one place. Feel free to: Request new lessons (video/how-to guides) on the issue tracker Make a pull request if you find errors or want to add some clarifying remarks. All files of the OBOAcademy website can be found in the docs directory . The Table of Contents is edited here . Join the OBO slack space ( #obo-training channel) to ask any questions (you can request access on the issue tracker )","title":"Contribute to the course"},{"location":"contributing/#contributing-to-obo-semantic-engineering-tutorials","text":"We rely on our readers to correct our materials and add to them - the hope is to centralise all the usual teaching materials for OBO ontology professionals in one place. Feel free to: Request new lessons (video/how-to guides) on the issue tracker Make a pull request if you find errors or want to add some clarifying remarks. All files of the OBOAcademy website can be found in the docs directory . The Table of Contents is edited here . Join the OBO slack space ( #obo-training channel) to ask any questions (you can request access on the issue tracker )","title":"Contributing to OBO Semantic Engineering Tutorials"},{"location":"getting_started/","text":"How to start with the lessons \u00b6 Before you start with the lessons of this course, keep the following in mind: The materials in this course are all intended to be used for self-study. We sometimes offer flipped-classroom sessions for new members on our teams - this means that we expect them to work through the entire course themselves and then come to use with questions and requests for clarifications. There is no need to reinvent the wheel: there are a lot of great materials out there already. Providing references to these these external resources is an essential part of the course - some lessons primarily comprise external tutorials, blog articles and more - please make sure you take advantage of them. Some of the materials developed by us are a bit rough around the edges, and we need your help to fix and improve them. To that end, we appreciate anything from suggestions for improvement to pull requests . Depending on your specific role and interest, you can choose which lessons are relevant to you. There is no specific order, but if you want to start somewhere, we recommend Contributing to OBO ontologies: Protege and Github and/or Using Ontologies and Ontology Terms The different roles of OBO Semantic Engineering \u00b6 There are a wide variety of entry points into the OBO world, for example: Database Curator : You are \u00b6 using ontologies for annotating datasets, experiments and publications requesting new terms from ontologies Suggest corrections to existing ontologies, such as wrong or missing synonyms, typos and definitions Ontology Curator : You are \u00b6 developing and maintaining ontologies adding terms to ontologies performing changes to ontologies, like adding or correcting synonyms responsible for ontology releases Ontology Engineer/Developer : You are \u00b6 developing design patterns for ontologies, specifying the logical structure of terms responsible for ensuring the specification and consistent application of metadata in your ontologies (which annotation properties to use, minimal metadata standards) defining quality control checks Ontology Pipeline Specialist : You are \u00b6 developing ontology pipelines with make and ROBOT building the release and quality control architecture that Engineers and Curators need to do their work. building infrastructure for application ontologies, implementing dynamic imports modules, transformations of and mappings to other ontologies. Semantic ETL Engineer : You are \u00b6 Building ingests from public life science resources such as Bgee, Panther, UniProt and many more You use ontologies to glue together data from different sources You use ontologies to augment your the information in your data sources through inference (Semantic) Software Engineer : You are \u00b6 using ontologies to generate value to end-user applications (user interfaces, semantic facetted search) building widgets that exploit the logical and graph structure of ontologies, for example phenotypic profile matching building ontology term browsers such as OLS . Of course, many of you will occupy more than one of the above \"hats\" or roles. While they all require specialised training, many shared skill requirements exist. This course is being developed to: Provide basic training for OBO Semantic Engineers of any of the above flavours Provide an entry point for people new to the field, for example as part of onboarding activities for projects working with ontologies Capture some of the typical pitfalls and how-to's guides to address common problems across the OBO-sphere","title":"Getting started with learning"},{"location":"getting_started/#how-to-start-with-the-lessons","text":"Before you start with the lessons of this course, keep the following in mind: The materials in this course are all intended to be used for self-study. We sometimes offer flipped-classroom sessions for new members on our teams - this means that we expect them to work through the entire course themselves and then come to use with questions and requests for clarifications. There is no need to reinvent the wheel: there are a lot of great materials out there already. Providing references to these these external resources is an essential part of the course - some lessons primarily comprise external tutorials, blog articles and more - please make sure you take advantage of them. Some of the materials developed by us are a bit rough around the edges, and we need your help to fix and improve them. To that end, we appreciate anything from suggestions for improvement to pull requests . Depending on your specific role and interest, you can choose which lessons are relevant to you. There is no specific order, but if you want to start somewhere, we recommend Contributing to OBO ontologies: Protege and Github and/or Using Ontologies and Ontology Terms","title":"How to start with the lessons"},{"location":"getting_started/#the-different-roles-of-obo-semantic-engineering","text":"There are a wide variety of entry points into the OBO world, for example:","title":"The different roles of OBO Semantic Engineering"},{"location":"getting_started/#database-curator-you-are","text":"using ontologies for annotating datasets, experiments and publications requesting new terms from ontologies Suggest corrections to existing ontologies, such as wrong or missing synonyms, typos and definitions","title":"Database Curator: You are"},{"location":"getting_started/#ontology-curator-you-are","text":"developing and maintaining ontologies adding terms to ontologies performing changes to ontologies, like adding or correcting synonyms responsible for ontology releases","title":"Ontology Curator: You are"},{"location":"getting_started/#ontology-engineerdeveloper-you-are","text":"developing design patterns for ontologies, specifying the logical structure of terms responsible for ensuring the specification and consistent application of metadata in your ontologies (which annotation properties to use, minimal metadata standards) defining quality control checks","title":"Ontology Engineer/Developer: You are"},{"location":"getting_started/#ontology-pipeline-specialist-you-are","text":"developing ontology pipelines with make and ROBOT building the release and quality control architecture that Engineers and Curators need to do their work. building infrastructure for application ontologies, implementing dynamic imports modules, transformations of and mappings to other ontologies.","title":"Ontology Pipeline Specialist: You are"},{"location":"getting_started/#semantic-etl-engineer-you-are","text":"Building ingests from public life science resources such as Bgee, Panther, UniProt and many more You use ontologies to glue together data from different sources You use ontologies to augment your the information in your data sources through inference","title":"Semantic ETL Engineer: You are"},{"location":"getting_started/#semantic-software-engineer-you-are","text":"using ontologies to generate value to end-user applications (user interfaces, semantic facetted search) building widgets that exploit the logical and graph structure of ontologies, for example phenotypic profile matching building ontology term browsers such as OLS . Of course, many of you will occupy more than one of the above \"hats\" or roles. While they all require specialised training, many shared skill requirements exist. This course is being developed to: Provide basic training for OBO Semantic Engineers of any of the above flavours Provide an entry point for people new to the field, for example as part of onboarding activities for projects working with ontologies Capture some of the typical pitfalls and how-to's guides to address common problems across the OBO-sphere","title":"(Semantic) Software Engineer: You are"},{"location":"overview/","text":"Overview \u00b6 Lessons \u00b6 Using Ontologies and Ontology Terms \u00b6 Target roles : Database Curators skills know what ontologies are good for find good ontologies: ontology repositories, OBO find good terms: ontology browsers assess for use: license, quality map local terms to ontology terms identify missing terms use IRIs, prefixes, CURIEs, labels use Protege? Contributing to OBO ontologies 1: Protege and Github \u00b6 Target roles : Database Curators, Ontology Curator, Ontology Engineer/Developer Builds on: Ontology Term Use Skills: use GitHub: issues, Pull Requests understand basic Open Source etiquette reading READMEs understand basics of ontology development workflows understand ontology design patterns use templates: ROBOT, DOS-DP basics of OWL Ontology Fundamentals \u00b6 Target roles : Ontology Curators, Ontology Engineer/Developer Builds on: Ontology Term Use Skills: RDF RDFS OWL Reasoners basic SPARQL Turtle, JSON-LD Linked Data Analysis \u00b6 Target roles : Ontology Curators, (Semantic) Software Engineer Builds on: Ontology Fundamentals Skills: Advanced SPARQL Term enrichment Semantic similarity Named Entity Recognition more... Ontology Development \u00b6 Builds on: Ontology Fundamentals Contributing to OBO ontologies Skills: Manage GitHub Manage ontology imports Use ROBOT extract: MIREOT, SLME Use ROBOT report Pruning trees Semantic Databases \u00b6 Builds on: Ontology Development skills advanced term mapping ontology terms in SQL terminology table JOINs, constraints convert tables to triples triplestores knowledge graphs Automating Ontology Development Workflows \u00b6 Builds on: Ontology Development Ontology Pipelines Skills: Unix shell make Advanced git, GitHub ROBOT ODK Developing an OBO Reference Ontology \u00b6 Builds on: Ontology Development Automation Skills: Detailed knowledge of OBO principles and best practises Use OBO Dashboard Use OBO Registry Use PURL system Tutorials \u00b6 ROBOT Tutorial 1: Convert, Extract and Template ROBOT Tutorial 2: Annotate, Merge, Reason and Diff Introduction to GitHub Intro to managing and tracking issues in GitHub How-to guides \u00b6 Install Elk 0.5 in Protege Getting set up with Docker and the Ontology Development Kit","title":"Overview of lessons and tutorials"},{"location":"overview/#overview","text":"","title":"Overview"},{"location":"overview/#lessons","text":"","title":"Lessons"},{"location":"overview/#using-ontologies-and-ontology-terms","text":"Target roles : Database Curators skills know what ontologies are good for find good ontologies: ontology repositories, OBO find good terms: ontology browsers assess for use: license, quality map local terms to ontology terms identify missing terms use IRIs, prefixes, CURIEs, labels use Protege?","title":"Using Ontologies and Ontology Terms"},{"location":"overview/#contributing-to-obo-ontologies-1-protege-and-github","text":"Target roles : Database Curators, Ontology Curator, Ontology Engineer/Developer Builds on: Ontology Term Use Skills: use GitHub: issues, Pull Requests understand basic Open Source etiquette reading READMEs understand basics of ontology development workflows understand ontology design patterns use templates: ROBOT, DOS-DP basics of OWL","title":"Contributing to OBO ontologies 1: Protege and Github"},{"location":"overview/#ontology-fundamentals","text":"Target roles : Ontology Curators, Ontology Engineer/Developer Builds on: Ontology Term Use Skills: RDF RDFS OWL Reasoners basic SPARQL Turtle, JSON-LD","title":"Ontology Fundamentals"},{"location":"overview/#linked-data-analysis","text":"Target roles : Ontology Curators, (Semantic) Software Engineer Builds on: Ontology Fundamentals Skills: Advanced SPARQL Term enrichment Semantic similarity Named Entity Recognition more...","title":"Linked Data Analysis"},{"location":"overview/#ontology-development","text":"Builds on: Ontology Fundamentals Contributing to OBO ontologies Skills: Manage GitHub Manage ontology imports Use ROBOT extract: MIREOT, SLME Use ROBOT report Pruning trees","title":"Ontology Development"},{"location":"overview/#semantic-databases","text":"Builds on: Ontology Development skills advanced term mapping ontology terms in SQL terminology table JOINs, constraints convert tables to triples triplestores knowledge graphs","title":"Semantic Databases"},{"location":"overview/#automating-ontology-development-workflows","text":"Builds on: Ontology Development Ontology Pipelines Skills: Unix shell make Advanced git, GitHub ROBOT ODK","title":"Automating Ontology Development Workflows"},{"location":"overview/#developing-an-obo-reference-ontology","text":"Builds on: Ontology Development Automation Skills: Detailed knowledge of OBO principles and best practises Use OBO Dashboard Use OBO Registry Use PURL system","title":"Developing an OBO Reference Ontology"},{"location":"overview/#tutorials","text":"ROBOT Tutorial 1: Convert, Extract and Template ROBOT Tutorial 2: Annotate, Merge, Reason and Diff Introduction to GitHub Intro to managing and tracking issues in GitHub","title":"Tutorials"},{"location":"overview/#how-to-guides","text":"Install Elk 0.5 in Protege Getting set up with Docker and the Ontology Development Kit","title":"How-to guides"},{"location":"config/template/","text":"Course title \u00b6 Prerequisites \u00b6 TBD Preparation \u00b6 TBD What is delivered as part of the course \u00b6 Description: add here Learning objectives \u00b6 add learning objectives here Tutorials \u00b6 in person or video (link videos here as they become available) Additional materials and resources \u00b6 TBD Contributors \u00b6 Add contributors with ORCID here","title":"New lesson template"},{"location":"config/template/#course-title","text":"","title":"Course title"},{"location":"config/template/#prerequisites","text":"TBD","title":"Prerequisites"},{"location":"config/template/#preparation","text":"TBD","title":"Preparation"},{"location":"config/template/#what-is-delivered-as-part-of-the-course","text":"Description: add here","title":"What is delivered as part of the course"},{"location":"config/template/#learning-objectives","text":"add learning objectives here","title":"Learning objectives"},{"location":"config/template/#tutorials","text":"in person or video (link videos here as they become available)","title":"Tutorials"},{"location":"config/template/#additional-materials-and-resources","text":"TBD","title":"Additional materials and resources"},{"location":"config/template/#contributors","text":"Add contributors with ORCID here","title":"Contributors"},{"location":"courses/monarch-obo-training/","text":"Monarch OBO Training \u00b6 Goal \u00b6 The goal of this course is to provide ongoing training for the OBO community. As with previous tutorials, we follow the flipped classroom concept: as organisers, we provide you with materials to look at, and you will work through the materials on your own. During our biweekly meeting, we will answer your questions, provide you with additional demonstrations where needed and go into depth wherever you as a student are curious to learn more. This means that this course can only work if you are actually putting in the time to preparing the materials . That said, we nevertheless welcome anyone to just lurk or ask related questions. You (Students) \u00b6 Read the \"Getting started\" guide Check which lessons interest you in the upcoming schedule - just participate in the ones you care about Prepare the lessons (between 3 and 7 hours of preparation time) Prepare questions for the revision sessions (without questions, the revision sessions will be silent and awkward) Make detailed issues on our issue tracker when materials are broken, hard to follow or need more details - that is really important as we rely on you to help to improve our materials continuously. Request new course units from us using the issue tracker. We (Tutors) \u00b6 Nicole Vasilevsky , University of Colorado Anschutz Medical Campus Nico Matentzoglu , semanticly Ltd Sabrina Toro , University of Colorado Anschutz Medical Campus Prepare the materials for each lesson and provide schedules Build new training materials where needed Organise the course and rooms Schedule \u00b6 Date Lesson Notes 2021/12/14 Lessons learned from troubleshooting ROBOT Open discussion, no advance preparation is needed 2021/11/30 Semantics of object properties (including Relations Ontology) 2021/11/16 SPARQL for OBO ontology development 2021/11/02 Templating: DOSDPs and ROBOT 2021/10/19 Ontology Design 2021/10/05 Cancelled due to overlap with ISB conference 2021/09/21 Ontology Pipelines with ROBOT 2 2021/09/08 Migrating legacy ontology systems to ODK 2021/09/07 Ontology Pipelines with ROBOT 2021/09/01 Manage dynamic imports the ODK 2021/08/25 Ontology Release Management with the ODK 2021/08/24 Contributing to OBO ontologies 2 2021/08/17 Contributing to OBO ontologies Notes \u00b6 Most of materials used by this course were developed by James Overton, Becky Jackson, Nicole Vasilevsky and Nico Matentzoglu as part of a project with the Critical Path Institute (see here ). The materials are improved as part of an internal training program (onboarding and CPD) for the Phenomics First project (NIH / NHGRI #1RM1HG010860-01).","title":"Monarch Ontology Training"},{"location":"courses/monarch-obo-training/#monarch-obo-training","text":"","title":"Monarch OBO Training"},{"location":"courses/monarch-obo-training/#goal","text":"The goal of this course is to provide ongoing training for the OBO community. As with previous tutorials, we follow the flipped classroom concept: as organisers, we provide you with materials to look at, and you will work through the materials on your own. During our biweekly meeting, we will answer your questions, provide you with additional demonstrations where needed and go into depth wherever you as a student are curious to learn more. This means that this course can only work if you are actually putting in the time to preparing the materials . That said, we nevertheless welcome anyone to just lurk or ask related questions.","title":"Goal"},{"location":"courses/monarch-obo-training/#you-students","text":"Read the \"Getting started\" guide Check which lessons interest you in the upcoming schedule - just participate in the ones you care about Prepare the lessons (between 3 and 7 hours of preparation time) Prepare questions for the revision sessions (without questions, the revision sessions will be silent and awkward) Make detailed issues on our issue tracker when materials are broken, hard to follow or need more details - that is really important as we rely on you to help to improve our materials continuously. Request new course units from us using the issue tracker.","title":"You (Students)"},{"location":"courses/monarch-obo-training/#we-tutors","text":"Nicole Vasilevsky , University of Colorado Anschutz Medical Campus Nico Matentzoglu , semanticly Ltd Sabrina Toro , University of Colorado Anschutz Medical Campus Prepare the materials for each lesson and provide schedules Build new training materials where needed Organise the course and rooms","title":"We (Tutors)"},{"location":"courses/monarch-obo-training/#schedule","text":"Date Lesson Notes 2021/12/14 Lessons learned from troubleshooting ROBOT Open discussion, no advance preparation is needed 2021/11/30 Semantics of object properties (including Relations Ontology) 2021/11/16 SPARQL for OBO ontology development 2021/11/02 Templating: DOSDPs and ROBOT 2021/10/19 Ontology Design 2021/10/05 Cancelled due to overlap with ISB conference 2021/09/21 Ontology Pipelines with ROBOT 2 2021/09/08 Migrating legacy ontology systems to ODK 2021/09/07 Ontology Pipelines with ROBOT 2021/09/01 Manage dynamic imports the ODK 2021/08/25 Ontology Release Management with the ODK 2021/08/24 Contributing to OBO ontologies 2 2021/08/17 Contributing to OBO ontologies","title":"Schedule"},{"location":"courses/monarch-obo-training/#notes","text":"Most of materials used by this course were developed by James Overton, Becky Jackson, Nicole Vasilevsky and Nico Matentzoglu as part of a project with the Critical Path Institute (see here ). The materials are improved as part of an internal training program (onboarding and CPD) for the Phenomics First project (NIH / NHGRI #1RM1HG010860-01).","title":"Notes"},{"location":"howto/CL-filter/","text":"Command Line Trick to Filter Text Files \u00b6 Let's say you want to remove some lines from a large text file programmatically. For example, you want to remove every line that contains certain IDs, but you want to keep the rest of the lines intact. You can use the command line utility grep with option -v to find all the lines in the file that do NOT contain your search term(s). You can make a file with a list of several search terms and use that file with grep using the -f option as follows: grep -v -f your_list.txt target_file.tsv | tee out_file.tsv Explanation \u00b6 The target file is your text file from which you wish to remove lines. The text file can be of type csv , tsv , obo etc. For example, you wish to filter a file with these lines: keep this 1 this line is undesired 2, so you do not wish to keep it keep this 3 keep this 4 keep this 5 keep this 6 something undesired 2 this line is undesired 1 keep this 7 The file your_list.txt is a text file with your list of search terms. Format: one search term per line. For example: undesired 1 undesired 2 The utility tee will redirect the standard output to both the terminal and write it out to a file. You expect the out_file.tsv to contain lines: keep this 1 keep this 3 keep this 4 keep this 5 keep this 6 keep this 7 Do the filtering and updating of your target file in one step \u00b6 You can also do a one-step filter-update when you are confident that your filtering works as expected, or if you have a backup copy of your target_file.tsv . Use cat and pipe the contents of your text file as the input for grep . Redirect the results to both your terminal and overwrite your original file so it will contain only the filtered lines. cat target_file.tsv | grep -v -f your_list.txt | tee target_file.tsv","title":"CL filter"},{"location":"howto/CL-filter/#command-line-trick-to-filter-text-files","text":"Let's say you want to remove some lines from a large text file programmatically. For example, you want to remove every line that contains certain IDs, but you want to keep the rest of the lines intact. You can use the command line utility grep with option -v to find all the lines in the file that do NOT contain your search term(s). You can make a file with a list of several search terms and use that file with grep using the -f option as follows: grep -v -f your_list.txt target_file.tsv | tee out_file.tsv","title":"Command Line Trick to Filter Text Files"},{"location":"howto/CL-filter/#explanation","text":"The target file is your text file from which you wish to remove lines. The text file can be of type csv , tsv , obo etc. For example, you wish to filter a file with these lines: keep this 1 this line is undesired 2, so you do not wish to keep it keep this 3 keep this 4 keep this 5 keep this 6 something undesired 2 this line is undesired 1 keep this 7 The file your_list.txt is a text file with your list of search terms. Format: one search term per line. For example: undesired 1 undesired 2 The utility tee will redirect the standard output to both the terminal and write it out to a file. You expect the out_file.tsv to contain lines: keep this 1 keep this 3 keep this 4 keep this 5 keep this 6 keep this 7","title":"Explanation"},{"location":"howto/CL-filter/#do-the-filtering-and-updating-of-your-target-file-in-one-step","text":"You can also do a one-step filter-update when you are confident that your filtering works as expected, or if you have a backup copy of your target_file.tsv . Use cat and pipe the contents of your text file as the input for grep . Redirect the results to both your terminal and overwrite your original file so it will contain only the filtered lines. cat target_file.tsv | grep -v -f your_list.txt | tee target_file.tsv","title":"Do the filtering and updating of your target file in one step"},{"location":"howto/change_files_pull_request/","text":"How to change files in an existing pull request \u00b6 Using GitHub \u00b6 Warning: You should only use this method if the files you are editing are reasonably small (less than 1 MB). This method only works if the file you want to edit has already been editing as part of the pull request. Go to the pull request on GitHub, and click on the \"Files Changed\" tab up top Find the file you want to edit in the pull request. On the right, click on on the three ... , and then \"Edit file\". If this option is greyed out, it means that - you don't have edit rights on the repository - the edit was made from a different fork, and the person that created the pull request did not activate the \"Allow maintainers to make edits\" option when submitting the PR - the pull request has already been merged Do the edits, and then commit changes, usually to the same branch","title":"Change a pull request"},{"location":"howto/change_files_pull_request/#how-to-change-files-in-an-existing-pull-request","text":"","title":"How to change files in an existing pull request"},{"location":"howto/change_files_pull_request/#using-github","text":"Warning: You should only use this method if the files you are editing are reasonably small (less than 1 MB). This method only works if the file you want to edit has already been editing as part of the pull request. Go to the pull request on GitHub, and click on the \"Files Changed\" tab up top Find the file you want to edit in the pull request. On the right, click on on the three ... , and then \"Edit file\". If this option is greyed out, it means that - you don't have edit rights on the repository - the edit was made from a different fork, and the person that created the pull request did not activate the \"Allow maintainers to make edits\" option when submitting the PR - the pull request has already been merged Do the edits, and then commit changes, usually to the same branch","title":"Using GitHub"},{"location":"howto/clone_mondo_repo/","text":"Clone the Mondo repo \u00b6 Open the Mondo GitHub repository Click Code Click 'Open with GitHub Desktop' You will be given an option as to where to save the repository. I have a folder called 'git' where I save all of my local repos. This will open GitHub Desktop and the repo should start downloading. This could take some time (maybe 15 minutes?) Open the Mondo Ontology in Protege \u00b6 Open Protege Go to: File -> Open Navigate to mondo/src/ontology/mondo-edit.obo and open this file in Protege If this all works okay, you are all set. We'll go over the editing workflows when we meet.","title":"Clone a repository"},{"location":"howto/clone_mondo_repo/#clone-the-mondo-repo","text":"Open the Mondo GitHub repository Click Code Click 'Open with GitHub Desktop' You will be given an option as to where to save the repository. I have a folder called 'git' where I save all of my local repos. This will open GitHub Desktop and the repo should start downloading. This could take some time (maybe 15 minutes?)","title":"Clone the Mondo repo"},{"location":"howto/clone_mondo_repo/#open-the-mondo-ontology-in-protege","text":"Open Protege Go to: File -> Open Navigate to mondo/src/ontology/mondo-edit.obo and open this file in Protege If this all works okay, you are all set. We'll go over the editing workflows when we meet.","title":"Open the Mondo Ontology in Protege"},{"location":"howto/deploy_custom_obo_dashboard/","text":"How to deploy a custom OBO dashboard \u00b6 Contributed by @XinsongDu, edited by @matentzn Clone https://github.com/OBOFoundry/obo-nor.github.io and copy all its contents to a new GitHub repo under your account. Ensure that the .gitignore from the obo-nor.github.io repo is also copied to your new repo (it is frequently skipped or hidden from the user in Finder or when using the cp command) and push to everything to GitHub. Pull the Ontology Development Kit from Docker Hub (can take a while): docker pull obolibrary/odkfull Modify the dashboard-config.yml file, in particular the ontologies section: Important: Add your ontology ID to the ID 'id' field Add the path to your ontology to the mirror_from field. Get the \"base uri namespace\" of the ontology using the following steps: a. Open the ontology in Prot\u00e9g\u00e9 b. Select a class and press \"command + u\" (MacOS), the stem of the path would be the base URI namespace (e.g., in EDAM ontology, the base uri namespace is http://edamontology.org/, for Uberon it would be http://purl.obolibrary.org/obo/UBERON_) Add the base uri namespace to 'base_ns' field of your ontology in the dashboard-config.yml (As of October 2021 make sure there are multiple ontologies in the dashboard-config.yml, otherwise errors would be reported while running the code. There are currently some bugs in the dashboard code that require at least 2 or 3 ontologies in the list). In the Makefile uncomment the # before pip install networkx==2.6.2 to ensure the correct network x version is installed. Run sh run-dash.sh (make sure dashboard folder is empty before running, e.g. rm -rf dashboard/* ). When run successfully, push all changes to GitHub. Go to GitHub repo you just created, and go to Settings, then Pages, and select your main/master branch as \"source\", and your root directory. You will see a website URL highlighted in green, where your OBO dashboard is deployed.","title":"Deploy a custom OBO-Dashboard"},{"location":"howto/deploy_custom_obo_dashboard/#how-to-deploy-a-custom-obo-dashboard","text":"Contributed by @XinsongDu, edited by @matentzn Clone https://github.com/OBOFoundry/obo-nor.github.io and copy all its contents to a new GitHub repo under your account. Ensure that the .gitignore from the obo-nor.github.io repo is also copied to your new repo (it is frequently skipped or hidden from the user in Finder or when using the cp command) and push to everything to GitHub. Pull the Ontology Development Kit from Docker Hub (can take a while): docker pull obolibrary/odkfull Modify the dashboard-config.yml file, in particular the ontologies section: Important: Add your ontology ID to the ID 'id' field Add the path to your ontology to the mirror_from field. Get the \"base uri namespace\" of the ontology using the following steps: a. Open the ontology in Prot\u00e9g\u00e9 b. Select a class and press \"command + u\" (MacOS), the stem of the path would be the base URI namespace (e.g., in EDAM ontology, the base uri namespace is http://edamontology.org/, for Uberon it would be http://purl.obolibrary.org/obo/UBERON_) Add the base uri namespace to 'base_ns' field of your ontology in the dashboard-config.yml (As of October 2021 make sure there are multiple ontologies in the dashboard-config.yml, otherwise errors would be reported while running the code. There are currently some bugs in the dashboard code that require at least 2 or 3 ontologies in the list). In the Makefile uncomment the # before pip install networkx==2.6.2 to ensure the correct network x version is installed. Run sh run-dash.sh (make sure dashboard folder is empty before running, e.g. rm -rf dashboard/* ). When run successfully, push all changes to GitHub. Go to GitHub repo you just created, and go to Settings, then Pages, and select your main/master branch as \"source\", and your root directory. You will see a website URL highlighted in green, where your OBO dashboard is deployed.","title":"How to deploy a custom OBO dashboard"},{"location":"howto/fixing-conflicts/","text":"Fixing conflicts in Mondo \u00b6 Watch this video for an example of how to fix a conflict in Mondo. Note, the exact nature of the conflict and the fix may vary, please feel free to ask Nicole or Nico for help.","title":"Fixing conflicts"},{"location":"howto/fixing-conflicts/#fixing-conflicts-in-mondo","text":"Watch this video for an example of how to fix a conflict in Mondo. Note, the exact nature of the conflict and the fix may vary, please feel free to ask Nicole or Nico for help.","title":"Fixing conflicts in Mondo"},{"location":"howto/github_create_fork/","text":"Appendix \u00b6 Create a fork from GitHub \u00b6 Note: Creating a fork allows you to create your copy GitHub repository. This example provides instructions on forking the Mondo GitHub reposiitory. You can't break any of the Mondo files by editing your forked copy. On GitHub, navigate to https://github.com/monarch-initiative/mondo In the top-right corner of the page, click Fork. When prompted 'Where should we fork mondo', choose your own repo (eg Nicole Vasilevsky). Be careful if you have multiple forks (i.e. the original and your own personal fork, as this can cause confusion). Clone your forked repo: If you have GitHub Desktop installed - click Code -> Open with GitHub Desktop How are you planning to use this fork? To contribute to parent project In GitHub Desktop, create a new branch: Click Current Branch - > New Branch Give your branch a name, like c-path-training-1 You will make changes to the Mondo on the branch of your local copy. Further instructions on forking a repo","title":"Creating an GitHub Fork"},{"location":"howto/github_create_fork/#appendix","text":"","title":"Appendix"},{"location":"howto/github_create_fork/#create-a-fork-from-github","text":"Note: Creating a fork allows you to create your copy GitHub repository. This example provides instructions on forking the Mondo GitHub reposiitory. You can't break any of the Mondo files by editing your forked copy. On GitHub, navigate to https://github.com/monarch-initiative/mondo In the top-right corner of the page, click Fork. When prompted 'Where should we fork mondo', choose your own repo (eg Nicole Vasilevsky). Be careful if you have multiple forks (i.e. the original and your own personal fork, as this can cause confusion). Clone your forked repo: If you have GitHub Desktop installed - click Code -> Open with GitHub Desktop How are you planning to use this fork? To contribute to parent project In GitHub Desktop, create a new branch: Click Current Branch - > New Branch Give your branch a name, like c-path-training-1 You will make changes to the Mondo on the branch of your local copy. Further instructions on forking a repo","title":"Create a fork from GitHub"},{"location":"howto/github_create_pull_request/","text":"Create a pull request in GitHub \u00b6 Committing, pushing and making pull requests \u00b6 Review: Changes made to the ontology can be viewed in GitHub Desktop. Before committing, check the diff. Examples of a diff are pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and ask the CL editors team for help instead. Example 1: Commit: Add a meaningful message in the Commit field in the lower left, for example: add new class MONDO:ID episodic angioedema with eosinophilia NOTE: You can use the word 'fixes' or 'closes' in the description of the commit message, followed by the corresponding ticket number (in the format #1234) - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages . Note: 'Fixes' and \"Closes' are case-insensitive. If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes' or use 'Adresses'. The commit will be associated with the correct ticket but the ticket will remain open. 7.NOTE: It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed. Click Commit to [branch]. This will save the changes to the cl-edit.owl file. Push: To incorporate the changes into the remote repository, click Publish branch. Make a Pull Request \u00b6 Click: Create Pull Request in GitHub Desktop This will automatically open GitHub Desktop Click the green button 'Create pull request' You may now add comments to your pull request. The CL editors team will review your PR and either ask for changes or merge it. The changes will be available in the next release.","title":"Creating an GitHub Pull Request"},{"location":"howto/github_create_pull_request/#create-a-pull-request-in-github","text":"","title":"Create a pull request in GitHub"},{"location":"howto/github_create_pull_request/#committing-pushing-and-making-pull-requests","text":"Review: Changes made to the ontology can be viewed in GitHub Desktop. Before committing, check the diff. Examples of a diff are pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and ask the CL editors team for help instead. Example 1: Commit: Add a meaningful message in the Commit field in the lower left, for example: add new class MONDO:ID episodic angioedema with eosinophilia NOTE: You can use the word 'fixes' or 'closes' in the description of the commit message, followed by the corresponding ticket number (in the format #1234) - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages . Note: 'Fixes' and \"Closes' are case-insensitive. If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes' or use 'Adresses'. The commit will be associated with the correct ticket but the ticket will remain open. 7.NOTE: It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed. Click Commit to [branch]. This will save the changes to the cl-edit.owl file. Push: To incorporate the changes into the remote repository, click Publish branch.","title":"Committing, pushing and making pull requests"},{"location":"howto/github_create_pull_request/#make-a-pull-request","text":"Click: Create Pull Request in GitHub Desktop This will automatically open GitHub Desktop Click the green button 'Create pull request' You may now add comments to your pull request. The CL editors team will review your PR and either ask for changes or merge it. The changes will be available in the next release.","title":"Make a Pull Request"},{"location":"howto/installing_elk_in_protege/","text":"Install Elk 0.5 in Protege \u00b6 Click here to get the latest Protege Plugin latest build (this is available on the bottom of ELK pages . This will download a zipped file.) When downloaded, unzip and copy puli and elk jars (two .jar files) in the unpacked directory. Paste these files in your Protege plugin directory. Remove old org.semanticweb.elk.jar Install ELK plugin on Mac: This can be done via one of two ways: Approach 1 Paste these files in your Protege plugin directory. This is in one of two locations: ~/.Protege/plugins (note this is usually hidden from finder, but you can see it in the terminal) or Go to Protege in Applications (Finder), right click, 'Show package contents' -> Java -> plugins Copy and paste the two files into the plugins directory Remove old elk.jar (Ex. org.semanticweb.elk.jar) Restart Protege. You should see ELK 0.5 installed in your Reasoner menu. For a video showing how to install Elk on a Mac, click here Approach 2 In Terminal: open ~/.Protege, then click on plugins Click on plugins Copy and paste the two files into the plugins directory Remove old elk.jar (Ex. org.semanticweb.elk.jar) Restart Protege. You should see ELK 0.5 installed in your Reasoner menu. Important: it seems Elk 0.5. Does not work with all versions of Protege, in particular, 5.2 and below. These instructions were only tested with Protege 5.5.","title":"Installing ELK in Protege"},{"location":"howto/installing_elk_in_protege/#install-elk-05-in-protege","text":"Click here to get the latest Protege Plugin latest build (this is available on the bottom of ELK pages . This will download a zipped file.) When downloaded, unzip and copy puli and elk jars (two .jar files) in the unpacked directory. Paste these files in your Protege plugin directory. Remove old org.semanticweb.elk.jar Install ELK plugin on Mac: This can be done via one of two ways: Approach 1 Paste these files in your Protege plugin directory. This is in one of two locations: ~/.Protege/plugins (note this is usually hidden from finder, but you can see it in the terminal) or Go to Protege in Applications (Finder), right click, 'Show package contents' -> Java -> plugins Copy and paste the two files into the plugins directory Remove old elk.jar (Ex. org.semanticweb.elk.jar) Restart Protege. You should see ELK 0.5 installed in your Reasoner menu. For a video showing how to install Elk on a Mac, click here Approach 2 In Terminal: open ~/.Protege, then click on plugins Click on plugins Copy and paste the two files into the plugins directory Remove old elk.jar (Ex. org.semanticweb.elk.jar) Restart Protege. You should see ELK 0.5 installed in your Reasoner menu. Important: it seems Elk 0.5. Does not work with all versions of Protege, in particular, 5.2 and below. These instructions were only tested with Protege 5.5.","title":"Install Elk 0.5 in Protege"},{"location":"howto/odk_setup/","text":"Getting set up with Docker and the Ontology Development Kit \u00b6 Installation \u00b6 For Windows \u00b6 Follow the instructions here . Note that you should have Windows 10 Professional installed for this to work. We are not sure Docker Desktop works at all with Windows 10 Home, but we have not tried in a while. If you know what you are doing, you could try to configure Docker toolbox, but we have had many issues with it, and do not recommend it unless absolutely necessary. Once installed, you should be able to open your command line and download the ODK. Click on your Windows symbol (usually in bottom left corner of screen), type \"cmd\" and you should be able to see and open the Command Line tool. in the command line type, type docker pull obolibrary/odkfull . This will download the ODK (will take a few minutes, depending on you internet connection). Executing something in a Docker container can be \"wordy\", because the docker container requires quite a few parameters to be run. To make this easier, we prepared a wrapper script here . You can download this file by clicking on Raw , and then, when the file is open in your browser, CTRL+S to save it. Ideally, you save this file in your project directory, the directory you will be using for your exercises, as it will only allow you to edit files in that very same directory (or one of its sub-directories). Setting the memory: Typical issues (WSL 1 vs 2) For Mac/Linux \u00b6 Install docker : Install Docker following the official instructions. Make sure its running properly, for example by typing docker ps in your terminal or command line (CMD). If all is ok, you should be seeing something like: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Run docker pull obolibrary/odkfull on your command line to install the ODK. This will take while. Download an ODK wrapper script . The odk.sh has further instruction on how to best use it. Now you are ready to go to a directory containing the odk.sh wrapper script and running sh odk.sh robot --version to see whether it works. The ODK wrapper script is generally useful to have: you can for example enter a ODK container, similar to a virtual machine, by simply running sh odk.bat bash (to leave the ODK container again, simply run exit from within the container. However, for many of the ontologies we develop, we already ship an ODK wrapper script in the ontology repo, so we dont need the odk.sh or odk.bat file. That file is usually called run.sh or run.bat and can be found in your ontology repo in the src/ontology directory and can be used in the exact same way. Problems with memory (important) \u00b6 One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. This can look like a Java OutOfMemory exception, but more often than not it will appear as something like an Error 137 . There are two places you need to consider to set your memory: Your ODK wrapper script (see above), i.e. odk.bat, odk.sh or src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding robot_java_args: '-Xmx8G' to your src/ontology/cl-odk.yaml file, see for example here . Set your docker memory. By default, it should be about 10-20% more than your robot_java_args variable. You can manage your memory settings by right-clicking on the docker whale in your system bar-->Preferences-->Resources-->Advanced, see picture below.","title":"Setting up the ODK"},{"location":"howto/odk_setup/#getting-set-up-with-docker-and-the-ontology-development-kit","text":"","title":"Getting set up with Docker and the Ontology Development Kit"},{"location":"howto/odk_setup/#installation","text":"","title":"Installation"},{"location":"howto/odk_setup/#for-windows","text":"Follow the instructions here . Note that you should have Windows 10 Professional installed for this to work. We are not sure Docker Desktop works at all with Windows 10 Home, but we have not tried in a while. If you know what you are doing, you could try to configure Docker toolbox, but we have had many issues with it, and do not recommend it unless absolutely necessary. Once installed, you should be able to open your command line and download the ODK. Click on your Windows symbol (usually in bottom left corner of screen), type \"cmd\" and you should be able to see and open the Command Line tool. in the command line type, type docker pull obolibrary/odkfull . This will download the ODK (will take a few minutes, depending on you internet connection). Executing something in a Docker container can be \"wordy\", because the docker container requires quite a few parameters to be run. To make this easier, we prepared a wrapper script here . You can download this file by clicking on Raw , and then, when the file is open in your browser, CTRL+S to save it. Ideally, you save this file in your project directory, the directory you will be using for your exercises, as it will only allow you to edit files in that very same directory (or one of its sub-directories). Setting the memory: Typical issues (WSL 1 vs 2)","title":"For Windows"},{"location":"howto/odk_setup/#for-maclinux","text":"Install docker : Install Docker following the official instructions. Make sure its running properly, for example by typing docker ps in your terminal or command line (CMD). If all is ok, you should be seeing something like: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Run docker pull obolibrary/odkfull on your command line to install the ODK. This will take while. Download an ODK wrapper script . The odk.sh has further instruction on how to best use it. Now you are ready to go to a directory containing the odk.sh wrapper script and running sh odk.sh robot --version to see whether it works. The ODK wrapper script is generally useful to have: you can for example enter a ODK container, similar to a virtual machine, by simply running sh odk.bat bash (to leave the ODK container again, simply run exit from within the container. However, for many of the ontologies we develop, we already ship an ODK wrapper script in the ontology repo, so we dont need the odk.sh or odk.bat file. That file is usually called run.sh or run.bat and can be found in your ontology repo in the src/ontology directory and can be used in the exact same way.","title":"For Mac/Linux"},{"location":"howto/odk_setup/#problems-with-memory-important","text":"One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. This can look like a Java OutOfMemory exception, but more often than not it will appear as something like an Error 137 . There are two places you need to consider to set your memory: Your ODK wrapper script (see above), i.e. odk.bat, odk.sh or src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding robot_java_args: '-Xmx8G' to your src/ontology/cl-odk.yaml file, see for example here . Set your docker memory. By default, it should be about 10-20% more than your robot_java_args variable. You can manage your memory settings by right-clicking on the docker whale in your system bar-->Preferences-->Resources-->Advanced, see picture below.","title":"Problems with memory (important)"},{"location":"howto/protege_browse_search/","text":"Browsing and Searching Mondo \u00b6 Open the Mondo in Prot\u00e9g\u00e9 \u00b6 Note: Windows users should open Protege using run.bat Navigate to where you downloaded the repository and open the mondo-edit.obo file (src/ontology/mondo-edit.obo) When you open Protege, you will be on the Active Ontology tab Note the Ontology IRI field. The IRI is used to identify the ontology on the Web. The Prot\u00e9g\u00e9 UI \u00b6 The Prot\u00e9g\u00e9 interface follows a basic paradigm of Tabs and Panels. By default, Prot\u00e9g\u00e9 launches with the main tabs seen below. The layout of tabs and panels is configurable by the user. The Tab list will have slight differences from version to version, and depending on your configuration. It will also reflect your customizations. To customize your view, go to the Window tab on the toolbar and select Views. Here you can customize which panels you see in each tab. In the tabs view, you can select which tabs you will see. You will commonly want to see the Entities tab, which has the Classes tab and the Object Properties tab. Note: if you open a new ontology while viewing your current ontology, Prot\u00e9g\u00e9 will ask you if you'd like to open it in a new window. For most normal usage you should answer no. This will open in a new window. The panel in the center is the ontology annotations panel. This panel contains basic metadata about the ontology, such as the authors, a short description and license information. Running the reasoner \u00b6 Before browsing or searching an ontology, it is useful to run an OWL reasoner first. This ensures that you can view the full, intended classification and allows you to run queries. Navigate to the query menu, and run the ELK reasoner: Entities tab \u00b6 You will see various tabs along the top of the screen. Each tab provides a different perspective on the ontology. For the purposes of this tutorial, we care mostly about the Entities tab, the DL query tab and the search tool. OWL Entities include Classes (which we are focussed on editing in this tutorial), relations (OWL Object Properties) and Annotation Properties (terms like, 'definition' and 'label' which we use to annotate OWL entities. Select the Entities tab and then the Classes sub-tab. Now choose the inferred view (as shown below). The Entities tab is split into two halves. The left-hand side provides a suite of panels for selecting various entities in your ontology. When a particular entity is selected the panels on the right-hand side display information about that entity. The entities panel is context specific, so if you have a class selected (like Thing) then the panels on the right are aimed at editing classes. The panels on the right are customizable. Based on prior use you may see new panes or alternate arrangements. You should see the class OWL:Thing. You could start browsing from here, but the upper level view of the ontology is too abstract for our purposes. To find something more interesting to look at we need to search or query. Searching in Protege \u00b6 You can search for any entity using the search bar on the right: The search window will open on top of your Protege pane, we recommend resizing it and moving it to the side of the main window so you can view together. Here's an example search for 'COVID-19': It shows results found in display names, definitions, synonyms and more. The default results list is truncated. To see full results check the 'Show all results option'. You may need to resize the box to show all results. Double clicking on a result, displays details about it in the entities tab, e.g. In the Entities, tab, you can browse related types, opening/closing branches and clicking on terms to see details on the right. In the default layout, annotations on a term are displayed in the top panel and logical assertions in the 'Description' panel at the bottom. Try to find these specific classes: - 'congenital heart disease' - 'Kindler syndrome' - 'kidney failure' Note - a cool feature in the search tool in Protege is you can search on partial string matching. For example, if you want to search for \u2018down syndrome\u2019, you could search on a partial string: \u2018do synd\u2019. Try searching for \u2018br car and see what kind of results are returned. Question: The search will also search on synonyms. Try searching for \u2018shingles\u2019 and see what results are returned. Were you able to find the term? Note - if the search is slow, you can uncheck the box \u2018Search in annotation values. Try this and search for a term and note if the search is faster. Then search for \u2018shingles\u2019 again and note what results you get.","title":"Browsing and Searching"},{"location":"howto/protege_browse_search/#browsing-and-searching-mondo","text":"","title":"Browsing and Searching Mondo"},{"location":"howto/protege_browse_search/#open-the-mondo-in-protege","text":"Note: Windows users should open Protege using run.bat Navigate to where you downloaded the repository and open the mondo-edit.obo file (src/ontology/mondo-edit.obo) When you open Protege, you will be on the Active Ontology tab Note the Ontology IRI field. The IRI is used to identify the ontology on the Web.","title":"Open the Mondo in Prot\u00e9g\u00e9"},{"location":"howto/protege_browse_search/#the-protege-ui","text":"The Prot\u00e9g\u00e9 interface follows a basic paradigm of Tabs and Panels. By default, Prot\u00e9g\u00e9 launches with the main tabs seen below. The layout of tabs and panels is configurable by the user. The Tab list will have slight differences from version to version, and depending on your configuration. It will also reflect your customizations. To customize your view, go to the Window tab on the toolbar and select Views. Here you can customize which panels you see in each tab. In the tabs view, you can select which tabs you will see. You will commonly want to see the Entities tab, which has the Classes tab and the Object Properties tab. Note: if you open a new ontology while viewing your current ontology, Prot\u00e9g\u00e9 will ask you if you'd like to open it in a new window. For most normal usage you should answer no. This will open in a new window. The panel in the center is the ontology annotations panel. This panel contains basic metadata about the ontology, such as the authors, a short description and license information.","title":"The Prot\u00e9g\u00e9 UI"},{"location":"howto/protege_browse_search/#running-the-reasoner","text":"Before browsing or searching an ontology, it is useful to run an OWL reasoner first. This ensures that you can view the full, intended classification and allows you to run queries. Navigate to the query menu, and run the ELK reasoner:","title":"Running the reasoner"},{"location":"howto/protege_browse_search/#entities-tab","text":"You will see various tabs along the top of the screen. Each tab provides a different perspective on the ontology. For the purposes of this tutorial, we care mostly about the Entities tab, the DL query tab and the search tool. OWL Entities include Classes (which we are focussed on editing in this tutorial), relations (OWL Object Properties) and Annotation Properties (terms like, 'definition' and 'label' which we use to annotate OWL entities. Select the Entities tab and then the Classes sub-tab. Now choose the inferred view (as shown below). The Entities tab is split into two halves. The left-hand side provides a suite of panels for selecting various entities in your ontology. When a particular entity is selected the panels on the right-hand side display information about that entity. The entities panel is context specific, so if you have a class selected (like Thing) then the panels on the right are aimed at editing classes. The panels on the right are customizable. Based on prior use you may see new panes or alternate arrangements. You should see the class OWL:Thing. You could start browsing from here, but the upper level view of the ontology is too abstract for our purposes. To find something more interesting to look at we need to search or query.","title":"Entities tab"},{"location":"howto/protege_browse_search/#searching-in-protege","text":"You can search for any entity using the search bar on the right: The search window will open on top of your Protege pane, we recommend resizing it and moving it to the side of the main window so you can view together. Here's an example search for 'COVID-19': It shows results found in display names, definitions, synonyms and more. The default results list is truncated. To see full results check the 'Show all results option'. You may need to resize the box to show all results. Double clicking on a result, displays details about it in the entities tab, e.g. In the Entities, tab, you can browse related types, opening/closing branches and clicking on terms to see details on the right. In the default layout, annotations on a term are displayed in the top panel and logical assertions in the 'Description' panel at the bottom. Try to find these specific classes: - 'congenital heart disease' - 'Kindler syndrome' - 'kidney failure' Note - a cool feature in the search tool in Protege is you can search on partial string matching. For example, if you want to search for \u2018down syndrome\u2019, you could search on a partial string: \u2018do synd\u2019. Try searching for \u2018br car and see what kind of results are returned. Question: The search will also search on synonyms. Try searching for \u2018shingles\u2019 and see what results are returned. Were you able to find the term? Note - if the search is slow, you can uncheck the box \u2018Search in annotation values. Try this and search for a term and note if the search is faster. Then search for \u2018shingles\u2019 again and note what results you get.","title":"Searching in Protege"},{"location":"howto/setup_ontology_development_odk/","text":"Getting set up to manage ontology pipelines with the ODK \u00b6 Set up docker and install the ODK ( howto ) To warm up with ODK development, follow the ODK Tutorial here","title":"Get set up for ODK-based ontology development"},{"location":"howto/setup_ontology_development_odk/#getting-set-up-to-manage-ontology-pipelines-with-the-odk","text":"Set up docker and install the ODK ( howto ) To warm up with ODK development, follow the ODK Tutorial here","title":"Getting set up to manage ontology pipelines with the ODK"},{"location":"howto/switching_ontologies/","text":"Switching Ontologies in Protege \u00b6 By: Nicole Vasilevsky Description \u00b6 When you edit an ontology, you need to make sure you are using the correct prefix and your assigned ID range for that on ontology. Protege (unfortunately) does not remember the last prefix or ID range that you used when you switch between ontologies. Therefore we need to manually update this each time we switch ontologies. Instructions \u00b6 When you switch to a new ontology file, open your preferences in Protege (File -> Preferences). Be sure you are on the New entities tab. Add the Prefix for the ontology you are working on. If you don't know your ID range, go to the ID ranges file for that ontology (it should be in src/ontology/[ontology-name]-idranges.owl . (For example, src/ontology/mondo-idranges.owl.) Copy and paste in the start and end values for your ID range. Tips \u00b6 I work on many ontologies, so I keep a note in OneNote (or Evernote) that keeps track of all my ID ranges for quick reference. You don't need to track the last ID that was used, Protege will know to pick the next ID in your range. For example, if your ID range is 8000000 to 8999999, you can enter that as your range, even if you have already added 10 terms within your range. Protege will know to assign the next ID as 8000011. Video Explanation \u00b6 Available here .","title":"Switching Ontologies in Protege"},{"location":"howto/switching_ontologies/#switching-ontologies-in-protege","text":"By: Nicole Vasilevsky","title":"Switching Ontologies in Protege"},{"location":"howto/switching_ontologies/#description","text":"When you edit an ontology, you need to make sure you are using the correct prefix and your assigned ID range for that on ontology. Protege (unfortunately) does not remember the last prefix or ID range that you used when you switch between ontologies. Therefore we need to manually update this each time we switch ontologies.","title":"Description"},{"location":"howto/switching_ontologies/#instructions","text":"When you switch to a new ontology file, open your preferences in Protege (File -> Preferences). Be sure you are on the New entities tab. Add the Prefix for the ontology you are working on. If you don't know your ID range, go to the ID ranges file for that ontology (it should be in src/ontology/[ontology-name]-idranges.owl . (For example, src/ontology/mondo-idranges.owl.) Copy and paste in the start and end values for your ID range.","title":"Instructions"},{"location":"howto/switching_ontologies/#tips","text":"I work on many ontologies, so I keep a note in OneNote (or Evernote) that keeps track of all my ID ranges for quick reference. You don't need to track the last ID that was used, Protege will know to pick the next ID in your range. For example, if your ID range is 8000000 to 8999999, you can enter that as your range, even if you have already added 10 terms within your range. Protege will know to assign the next ID as 8000011.","title":"Tips"},{"location":"howto/switching_ontologies/#video-explanation","text":"Available here .","title":"Video Explanation"},{"location":"images/","text":"About using images in Git/GitHub \u00b6 There are two places you'll probaby want to use images in GitHub, in issue tracker and in markdown files, html etc. The way you handle images in these contexts is quite different, but easy once you get the hang of it. In markdown files (and html etc) \u00b6 All images referenced in static files such as html and markdown need to be referenced using a URL; dragging and dropping is not supported and could actually cause problems. Keeping images in a single directory enables them to be referenced more readily. Sensible file names are highly recommended, preferably without spaces as these are hard to read when encoded. An identical file, named in two different ways is shown as an example below. They render in the same way, but the source \"code\" looks ugly when spaces are used in file names. Eg. encoding needed no encoding needed ![](github%20organizations%20teams%20repos.png ![](github-organizations-teams-repos.png) In this example, the filename is enough of a 'url' because this file (https://ohsu-library.github.io/github-tutorial/howto/images/index.md) and the images are in the same directory https://ohsu-library.github.io/github-tutorial/howto/images/. To reference/embed an image that is not in the same directory, a more careful approach is needed. Referencing images in your repository and elsewhere \u00b6 Absolute path referencing Relative path referencing ![](https://github.com/OHSU-Library/github-tutorial/raw/master/docs/other-images/owl.jpg) ![](other-images/owl.jpg) Each instance of ../ means 'go up one level' in the file tree. It is also possible to reference an image using an external URL outside your control, in another github organization, or anywhere on the web, however this method can be fragile if the URL changes or could lead to unintended changes. Therefore make your own copies and reference those unless: You're sure that referencing the originals will not end in broken links or surprising content. Copying the image is prohibited The images are too large to make copying worth the hassle/expense. For example, it is not clear for how long the image below will manage to persist at this EPA link, or sadly, for how long the image will even be an accurate reflection of the current situation in the arctic. https://www.epa.gov/sites/production/files/styles/microsite_banner/public/2016-12/epa-banner-images/science_banner_arctic.png In GitHub issue tracker \u00b6 Images that are embedded into issues can be dragged and dropped in the GitHub issues interface. Once you've done so, it will look something like this with GitHub assigning an arbitrary URL (githubuserassets) for the image. ![](screenshot-of-images-in-issues.png) Sizing images \u00b6 Ideally, a Markdown document is renderable in a variety of output formats and devices. In some cases, it may be desirable to create non-portable Markdown that uses HTML syntax to position images. This limits the longevity of the artifact, but may be necessary sometimes. We describe how to manage this below. In order to size images, use the native html syntax: width = with the <img src=, as per below. <img src=\"https://github.com/monarch-initiative/monarch-app/raw/master/image/Phenogrid3Compare.png\" width=\"53\"> Back to Home \u00b6","title":"How to add images"},{"location":"images/#about-using-images-in-gitgithub","text":"There are two places you'll probaby want to use images in GitHub, in issue tracker and in markdown files, html etc. The way you handle images in these contexts is quite different, but easy once you get the hang of it.","title":"About using images in Git/GitHub"},{"location":"images/#in-markdown-files-and-html-etc","text":"All images referenced in static files such as html and markdown need to be referenced using a URL; dragging and dropping is not supported and could actually cause problems. Keeping images in a single directory enables them to be referenced more readily. Sensible file names are highly recommended, preferably without spaces as these are hard to read when encoded. An identical file, named in two different ways is shown as an example below. They render in the same way, but the source \"code\" looks ugly when spaces are used in file names. Eg. encoding needed no encoding needed ![](github%20organizations%20teams%20repos.png ![](github-organizations-teams-repos.png) In this example, the filename is enough of a 'url' because this file (https://ohsu-library.github.io/github-tutorial/howto/images/index.md) and the images are in the same directory https://ohsu-library.github.io/github-tutorial/howto/images/. To reference/embed an image that is not in the same directory, a more careful approach is needed.","title":"In markdown files (and html etc)"},{"location":"images/#referencing-images-in-your-repository-and-elsewhere","text":"Absolute path referencing Relative path referencing ![](https://github.com/OHSU-Library/github-tutorial/raw/master/docs/other-images/owl.jpg) ![](other-images/owl.jpg) Each instance of ../ means 'go up one level' in the file tree. It is also possible to reference an image using an external URL outside your control, in another github organization, or anywhere on the web, however this method can be fragile if the URL changes or could lead to unintended changes. Therefore make your own copies and reference those unless: You're sure that referencing the originals will not end in broken links or surprising content. Copying the image is prohibited The images are too large to make copying worth the hassle/expense. For example, it is not clear for how long the image below will manage to persist at this EPA link, or sadly, for how long the image will even be an accurate reflection of the current situation in the arctic. https://www.epa.gov/sites/production/files/styles/microsite_banner/public/2016-12/epa-banner-images/science_banner_arctic.png","title":"Referencing images in your repository and elsewhere"},{"location":"images/#in-github-issue-tracker","text":"Images that are embedded into issues can be dragged and dropped in the GitHub issues interface. Once you've done so, it will look something like this with GitHub assigning an arbitrary URL (githubuserassets) for the image. ![](screenshot-of-images-in-issues.png)","title":"In GitHub issue tracker"},{"location":"images/#sizing-images","text":"Ideally, a Markdown document is renderable in a variety of output formats and devices. In some cases, it may be desirable to create non-portable Markdown that uses HTML syntax to position images. This limits the longevity of the artifact, but may be necessary sometimes. We describe how to manage this below. In order to size images, use the native html syntax: width = with the <img src=, as per below. <img src=\"https://github.com/monarch-initiative/monarch-app/raw/master/image/Phenogrid3Compare.png\" width=\"53\">","title":"Sizing images"},{"location":"images/#back-to-home","text":"","title":"Back to Home"},{"location":"lesson/analysing_linked_data/","text":"Analysing Linked Data (Fundamentals) \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Review tutorial on Ontology Theory Preparation \u00b6 Essential Linked Data Engineering : Week 1 Support Programming Historian Linked Data tutorial Original Whitepaper (Tim Berners Lee et al) Educational curriculum for Linked Data Tools: Browse through the tools and standards listed in the Semantic Engineer Toolbox . Learning objectives \u00b6 Advanced SPARQL Term enrichment Semantic similarity Named Entity Recognition Tutorials \u00b6 The Linked Data landscape from an OBO perspective: Standards, Services and Tools \u00b6 In the following we will look a bit at the general Linked Data landscape, and name some of its flagship projects and standards. It is important to be clear that the Semantic Web field is a very heterogenous one: Flagship projects of the wider Semantic Web community \u00b6 Linked Open Data (LOD) cloud : The flagship project of the Semantic Web. An attempt to make all, or anyways a lot, of Linked Data accessible in one giant knowledge graph. A good overview can be found in this medium article. Note that some people seem to think that the Semantic Web is (or should be) the Linked Open Data cloud. I would question this view, but I am not yet decided what my position is. Schema.org : General purpose vocabulary for entities on the web, founded by Google, Microsoft, Yahoo and Yandex. To get a better sense of the types of entities and relationships covered see here . DBpedia : Project that extracts structured data from Wikipedia and makes it available as a giant knowledge graph. The associated ontology , similar to schema.org, covers entities encountered in common sense knowledge. Wikidata : Free and open knowledge base that can be edited in much the same way as Wikipedia is edited. While these Semantic Web flagship projects are doubtlessly useful, it is sometimes hard to see how they can help for your biomedical research. We rarely make use of them in our day to day work as ontologists, but there are some notable exceptions: - Where our work involves modelling environmental factors, we sometimes use wikidata as a standard way to refer for example to countries. - For some more common sense knowledge use cases, such as nutrition, consider augmenting your knowledge graph with data from wikidata or dbpedia. While they may be a bit more messy and not directly useful for exploration by humans, it is quite possible that Machine Learning approaches can use the additional context provided by these knowledge graphs to improve embeddings and deliver more meaningful link predictions. - Some OBO ontologies are already on Wikidata - perhaps you can find additional synonyms and labels which help with your data mapping problems! Where the OBO and Semantic Web communities are slightly at odds \u00b6 The OBO format is a very popular syntax for representing biomedical ontologies. A lot of tools have been built over the years to hack OBO ontologies on the basis of that format - I still work with it on a daily basis. Although it has semantically been proven to be a subset of OWL (i.e. there is a lossless mapping of OBO into OWL) and can be viewed as just another syntax, it is in many ways ideosyncratic. For starters, you wont find many, if any, IRIs in OBO ontologies. The format itself uses CURIEs which are mapped to the general OBO PURL namespace during transformation to OWL. For example, if you see MONDO:0003847 in an OBO file, and were to translate it to OWL, you will see this term being translated to http://purl.obolibrary.org/obo/MONDO_0003847. Secondly, you have a bunch of built-in properties like BROAD or ABBREVIATION that mapped to a vocabulary called oboInOwl (oio). These are pretty non-standard on the general Semantic Web, and often have to be manually mapped to the more popular counterparts in the Dublin Core or SKOS namespaces. Having URIs as identifiers is not generally popular in the life sciences. As discussed elsewhere, it is much more likely to encounter CURIEs such as MONDO:0003847 than URIs such as http://purl.obolibrary.org/obo/MONDO_0003847 in biomedical databases. Useful tools for biomedical research \u00b6 Why does the biomedical research, and clinical, community care about the Semantic Web and Linked Data? There are endless lists of applications that try to apply semantic technologies to biomedical problems, but for this week, we only want to look at the broader picture. In our experience, the use cases where Semantic Web standards are applied successfully are: Where to find ontologies: Ontology repositories OBO Foundry Ontology Library BioPortal CPT Story . The Current Procedural Terminology was the by far most highly accessed Terminology on Bioportal - for many years. Due to license concerns, it had to be withdrawn from the repository. This story serves a cautionary tale of using terminologies with non-open or non-transparent licensing schemes. AgroPortal : Like BioPortal, but focussed on the Agronomy domain. Linked Open Data Vocabularies (LOV) : Lists the most important vocabularies in the Linked Data space, such as Dublin Core , SKOS and Friend-of-a-Friend (FOAF). Where to find terms: Term browsers OLS : The boss of the current term browsers out there. While the code base is a bit dated, it still gives access to a wide range of relevant open biomedical ontology terms. Note, while being a bit painful, it is possible to set up your own OLS (for your organisation) which only contains those terms/ontologies that are relevant for your work. Ontobee : The default term browser for OBO term purls. For example, click on http://purl.obolibrary.org/obo/OBI_0000070. This will redirect you directly to Ontobee, to show you the terms location in the hierarchy. A key difference between Ontobee and OLS/Bioportal is that Ontobee limits hierarchical relationships to is_a. This means if you are browsing ontologies such as GO, Uberon, CL, ENVO, you will not see part-of links in the hierarchy, and these links are crucial for understanding these ontologies. AberOWL : Another ontology repository and semantic search engine. Some ontologies such as PhenomeNet can only be found on AberOWL, however, I personally prefer OLS. identifiers.org : A centralised registry for identifiers used in the life sciences. This is one of the tools that bridge the gap between CURIEs and URLs, but it does not cover (OBO) ontologies very well, and if so, is not aware of the proper URI prefixes (see for example here , and HP term resolution that does not list the proper persistent URL of the HP identifier (http://purl.obolibrary.org/obo/HP_0000001)). Identifiers.org has mainly good coverage for databases/resources that use CURIE type identifiers. But: you can enter any ID you find in your data and it will tell you what it is associated with. Curate biomedical data. There are a lot of different tools in this space - which we will discuss in a bespoke unit later in the course. Examples: isatools : The open source ISA framework and tools help to manage an increasingly diverse set of life science, environmental and biomedical experiments that employing one or a combination of technologies. RightField : System for curating ontology terms in Excel spreadsheets. CEDAR Templates : Basically a templating system that allows to create templates to record metadata, for example in a lab setting, of course with ontology integration. Other examples of tabular data to RDF converters, but new ones coming up every year. Building ontologies Populous/Webulous : A system to maintain/generate ontologies from spreadsheets. The idea was to basically to define patterns in a (now mostly dead) language called OPPL, and then apply them to spreadsheets to generate OWL axioms. EBI recently discontinued the service, as there is a general exodus to Google Sheets + ROBOT templates instead. ROBOT templates + Google Sheets and Cogs : A lightweight approach based on a set of tools that allows curating ontologies in spreadsheets (e.g. Google Sheets) which are converted into OWL using ROBOT. DOSDP tools + Dead Simple Design Patterns (DOSDP) : Similar to ROBOT templates, DOSDPs (which really should be called DOSDTs, because they are not really design patterns ; they are ontology templates), another system that allows the generation of OWL axioms based on spreadsheet data. Cleaning messy data OpenRefine : I have not myself used this ever, but some of my colleagues have. OpenRefine allows you to upload (spreadsheet) data, explore it and clean it (going as far as reconciling terms using Wikidata concepts). Which biomedical ontologies should we use? \u00b6 As a rule of thumb, for every single problem/term/use case, you will have 3-6 options to choose from, in some cases even more. The criteria for selecting a good ontology are very much dependent on your particular use case, but some concerns are generally relevant. A good first pass is to apply to \" 10 simple rules for selecting a Bio-ontology \" by Malone et al, but I would further recommend to ask yourself the following: Do I need the ontology for grouping and semantic analysis? In this case a high quality hierarchy reflecting biological subsumption is imperative. We will explain later what this means, but in essence, you should be able to ask the following question: \"All instances/occurrences of this concept in the ontology are also instances of all its parent classes. Everything that is true about the parent class is always also true about instances of the children.\" It is important for you to understand that, while OWL semantics imply the above, OWL is difficult and many ontologies \"pretend\" that the subclass link means something else (like a rule of thumb grouping relation). Can I handle multiple inheritance in my analysis? While I personally recommend to always consider multiple inheritance (i.e, allow a term to have more than one parent class), there are some analysis frameworks, in particular in the clinical domain, that make this hard. Some ontologies are inherently ploy-hierarchical (such as Mondo ), while others strive to be single inheritance ( DO , ICD). Are key resources I am interested in using the ontology? Maybe the most important question that will drastically reduce the amount of data mapping work you will have to do: Does the resource you wish to integrate already annotate to a particular ontology? For example, EBI resources will be annotating phenotype data using EFO, which in turn used HPO identifiers. If your use case demands to integrate EBI databases, it is likely a good idea to consider using HPO as the reference ontology for your phenotype data. Aside from aspects of your analysis, there is one more thing you should consider carefully: the open-ness of your ontology in question. As a user, you have quite a bit of power on the future trajectory of the domain, and therefore should seek to endorse and promote open standards as much as possible (for egotistic reasons as well: you don't want to have to suddenly pay for the ontologies that drive your semantic analyses). It is true that ontologies such as SNOMED have some great content, and, even more compellingly, some really great coverage. In fact, I would probably compare SNOMED not with any particular disease ontology, but with the OBO Foundry as a whole, and if you do that, it is a) cleaner, b) better integrated. But this comes at a cost. SNOMED is a commercial product - millions are being payed every year in license fees, and the more millions come, the better SNOMED will become - and the more drastic consequences will the lock-in have if one day you are forced to use SNOMED because OBO has fallen too far behind. Right now, the sum of all OBO ontologies is probably still richer and more valuable, given their use in many of the central biological databases (such as the ones hosted by the EBI ) - but as SNOMED is seeping into the all aspects of genomics now (for example, it will soon be featured on OLS !) it will become increasingly important to actively promote the use of open biomedical ontologies - by contributing to them as well as by using them. We will discuss ontologies in the medical, phenomics and genomics space in more detail in a later session of the course. Other interesting links \u00b6 Linked Data in e-Government Industrial Ontologies Foundry : Something like the OBO Foundry for Industrial Ontologies OntoCommons : An H2020 CSA project dedicated to the standardisation of data documentation across all domains related to materials and manufacturing. Basic Linked data and Semantic Web Concepts for the Semantic Data Engineer in the Biomedical Domain \u00b6 In this section we will discuss the following: - Introductory remarks - The advantages of globally unique identifiers - Some success stories of the Semantic Web in the biomedical domain - Some basic concepts you should probably have heard about - The ecosystem of the Semantic Web: Standards, Technologies and Research Areas - Typical tasks of Semantic Data Engineers in the biomedical domain Introduction \u00b6 Note of caution : No two Semantic Web overviews will be equivalent to each other. Some people claim the Semantic Web as an idea is an utter failure, while others praise it as a great success (in the making) - in the end you will have to make up your own mind. In this section I focus on parts of the Semantic Web step particularly valuable to the biomedical domain, and I will omit many relevant topics in the wider Semantic Web area, such as Enterprise Knowledge Graphs, decentralisation and personalisation, and many more. Also, the reader is expected to be familiar with the basic notions of the Semantic Web, and should use this overview mainly to tie some of the ideas together. The goal of this section is to give the aspiring Semantic Data Engineer in the biomedical domain a rough idea of key concepts around Linked Data and the Semantic Web insofar as they relate to their data science and and data engineering problems. Even after 20 years of Semantic Web research (the seminal paper , conveniently and somewhat ironically behind a paywall, was published in May 2001), the area is still dominated by \"academic types\", although the advent of the Knowledge Graph is already changing that. As I already mentioned above, no two stories of what the Semantic Web is will sound the same. However, there are a few stories that are often told to illustrate why we need semantics. The OpenHPI course names a few: - \"From the web of documents to the web of data\" tells the story of how the original web is essentially a huge heap of (interlinked) natural language text documents which are very hard to understand for search engines: Does the word \"Jaguar\" on this site refer to the car or the cat? Clarifying in your web page that the word Jaguar refers to the concept of \"Jaguar the cat\", for example like this: <span about=\"dbpedia:Jaguar\">Jaguar</span> , will make it easier for the search engine to understand what your site is about and link it to other relevant content. From this kind of mark-up, structured data can be extracted and integrate into a giant, worldwide database, and exposed through SPARQL endpoints, that can then be queried using a suitable query language. - \"From human to machine understandable\": as a Human, we know that a Jaguar is a kind of cat, and all cats have four legs. If you ask a normal search engine: \"Does a Jaguar have four legs?\" it will have a tough time to answer this question correctly (if it cannot find that exact statement anywhere). That is why we need proper semantics , some kind of formalism such that a \"machine\" can deduce from the statements \"Jaguar is a cat; Cat has four legs\" that \"Jaguar has four legs\". - The \"Semantic Layer Cake\": a box/brick diagram showing how Semantic Web Technologies are stacked on top of each other. An engineering centric view that has been used countless times to introduce the Semantic Web, but rarely helped anyone to understand what it is about. I am not entirely sure anymore that any of these ways (web of data, machine understanding, layered stack of matching standards) to motivate the Semantic Web are particularly effective for the average data scientists or engineer. If I had to explain the Semantic Web stack to my junior self, just having finished my undergraduate, I would explain it as follows (no guarantee though it will help you). The Semantic Web / Linked Data stack comprises roughly four components that are useful for the aspiring Semantic (Biomedical) Data Engineer/Scientist to distinguish: A way to refer to things (including entities and relations) in a global namespace. \u00b6 You, as a scientist, might be using the term \"gene\" to refer to basic physical and functional unit of heredity, but me, as a German, prefer the term \"Gen\". In the Semantic Web, instead of natural language words, we prefer to use URIs to refer to things such as https://www.wikidata.org/wiki/Q7187: if you say something using the name https://www.wikidata.org/wiki/Q7187, both your German and Japanese colleagues will \"understand\" what you are referring to. More about that in the next chapter. Lots (loaaaads!) of ways to make statements about things . \u00b6 For example, to express \"a mutation of SHH in humans causes isolated microphthalmia with coloboma-5\" you could say something like (http://purl.obolibrary.org/obo/MONDO_0012709 | \"microphthalmia, isolated, with coloboma 5\")--[http://purl.obolibrary.org/obo/RO_0004020 | \"has basis in dysfunction of\"]-->(https://identifiers.org/HGNC:10848 | \"SSH (gene)\"). Or you could say: (http://purl.obolibrary.org/obo/MONDO_0012709 | \"microphthalmia, isolated, with coloboma 5\")--[http://www.w3.org/2000/01/rdf-schema#subClassOf | \"is a\"]-->(http://purl.obolibrary.org/obo/MONDO_0003847 | \"Mendelian Disease\"). If we use the analogy of \"language\", then the URIs (above) are the words, and the statements are sentences in a language . Unfortunately, there are many languages in the Semantic Web, such as OWL, RDFS, SKOS, SWRL, SHACL, SHEX, and dialects (OWL 2 EL, OWL 2 RL) and a plethora of scripts, or serialisations (you can store the exact same sentence in the same language such as RDF, or OWL, in many different ways)- more about that later. In here lies also one of the largest problems of the Semantic Web - lots of overlapping standards means, lots of incompatible data - which raises the bar for actually being able to seamlessly integrate \"statements about things\" across resources. Collections of statements about things that somehow belong together and provide some meaning, or context, for those things . \u00b6 Examples include: - controlled vocabularies , that define, for example, how to refer to a disease (e.g., we use http://purl.obolibrary.org/obo/MONDO_0012709 to refer to \"isolated microphthalmia with coloboma 5\"), - terminologies which describe how we humans refer to a disease (How is it called in German? Which other synonyms are used in the literature? How is the term defined in the medical literature?), - taxonomies which define how diseases are related hierarchically (\"microphthalmia, isolated, with coloboma 5 is a kind of Mendelian disease\"), - ontologies which further define how diseases are defined in terms of other entities, for example \"microphthalmia, isolated, with coloboma 5 is a Mendelian disease that has its basis in the dysfunction of SSH\". - Note : In practice, when we say \"ontology\", we mean all of the above together - it is, however, good to know that they are somewhat distinct, and that there are different \"languages\" that can be used for each of these distinctions. Tools that do something useful with these collections of statements. \u00b6 For example (as always, non exhaustive): Efficient storage (triple stores, in-memory ontology APIs, other databases). Similar to traditional SQL databases, the Semantic Web comes with a number of database solutions that are optimised to deliver \"semantic content\". Semantically aware querying : Very similar to traditional SQL (which, incidentally is often great to query semantic data), there are various ways to \"interrogate\", or query, your Linked Data, such as SPARQL, DL Querying , Ontology-based data access (OBDA). Subsetting (module/subset extraction): Often, ontologies (or other collections of Linked Data statements) are very large and cover a lot of entities and knowledge that is not important to your work. There are a number of techniques that allow you to extract meaningful subsets for your use case; for example, you may be interested to get all the information you can about Mendelian diseases, but you don't care about common diseases (e.g. see ROBOT extract ). Visualisation : As a Data Scientist, you are used to looking at your data in tabular form - while a lot of information stored in ontologies can still be inspected this way, in general semantic data is best perceived as a graph - which are notoriously hard to visualise. Fortunately, a lot of Linked Data, in particular ontologies in the biomedical domain are predominantly tree-shaped (you have a disease, and underneath sub-diseases). Term browsers like OLS typically render ontologies as trees. Data linking/matching : This is key in particular in the biomedical sciences, as there is almost never just one way to refer to the same thing. In my experience, a good rule of thumb is that there are 3-6, e.g. 3-6 URIs that refer to \"Mendelian Disease\", all of which need to be matched together to integrate data across resources. There are many approaches to ontology matching - none of which are anywhere near perfect. Automated error checking and validation (Syntax, Structure (SHEX, SHACL), logical Consistency (DL Reasoner)): Naturally, writing sentences in any language is hard in the beginning, but this is even more true for highly complex languages such as OWL. In my experience, no-one can write flawless OWL without the help of automatic syntax and semantics checking, at least not consistently. Validation tools are a crucial part of your Semantic Engineering toolbox. Translate between languages: Often we need to translate from one language, for example OWL, to another, for example SKOS to integrate divergent resources. Translations in the Semantic Web context are nearly always lossy (there are always things you can say in one language, but not in another), but they may be necessary nevertheless. Discovery of terms ( OLS , BioPortal ): If you are curating terms, you need to know what ID (URI) to use for \"isolated microphthalmia with coloboma 5\". For that, term browsers such as OLS are perfect. Just type in your natural language search term, and you will find a series of suggestions for URIs you can use for your curation. Discovery of vocabularies OBO Foundry ontology library , BioPortal ): we will have a section later on on how to select appropriate ontologies for your use case, but the general problem of finding vocabularies, or ontologies, is answered by ontology repositories or libraries. Naturally, our favourite ontology library is the OBO Foundry ontology library which contain a lot of high quality ontologies for the biomedical domain. Make implicit knowledge explicit, aka reasoning : Deductive (DL Reasoning, Rule-based reasoning such as Datalog, SWRL). One of the major selling points for OWL, for example, in the biomedical domain is the ability to use logical reasoning in a way that is sound (only gives you correct inferences, at all times) and complete (all hidden implications are found, at all times, by the reasoner) - this is particularly great for medical knowledge where mistakes in computer algorithms can have devastating effects. However, I am slowly coming to the conviction that sound and complete reasoning is not the only form of deductive reasoning that is useful - many rule languages can offer value to your work by unveiling hidden relationships in the data without giving such strong \"logical guarantees\". Inductive (Machine Learning approaches, Knowledge Graph Representation Learning). The new frontier - we will discuss later in the course how our ontology-powered Knowledge Graphs can be leveraged to identify drug targets, novel gene to phenotype associations and more, using a diverse set of Machine Learning-based approaches. This week will focus on 1 (identifiers) and 4 (applications) - 2 (languages and standards) and 3 (controlled vocabularies and ontologies) will be covered in depth in the following weeks. Note on the side : Its not always 100% clear what is meant by Linked Data in regular discourse. There are some supposedly \"clear\" definitions (\" method for publishing structured data \", \" collection of interrelated datasets on the Web \"), but when it comes down to the details, there is plenty of confusion (does an OWL ontology constitute Linked Data when it is published on the Web? Is it Linked Data if it does not use RDF? Is it Linked Data if it is less than 5-star - see below). In practice all these debates are academic and won't mean much to you and your daily work. There are entities , statements (context) being said about these entities using some standard (associated with the Semantic Web, such as OWL or RDFS) and tools that do something useful with the stuff being said. When I say \"Mendelian Disease\" I mean http://purl.obolibrary.org/obo/MONDO_0003847 \u00b6 One of the top 5 features of the Semantic Web (at least in the context of biomedical sciences) is the fact that we can use URIs as a global identifier scheme that is unambiguous, independent of database implementations, independent of language concerns to refer to the entities in our domain. For example, if I want to refer to the concept of \"Mendelian Disease\", I simply refer to http://purl.obolibrary.org/obo/MONDO_0003847 - and everyone, in Japan, Germany, China or South Africa, will be able to \"understand\" or look up what I mean. I don't quite like the word \"understanding\" in this context as it is not actually trivial to explain to a human how a particular ID relates to a thing in the real world (semiotics). In my experience, this process is a bit rough in practice - it requires that there is a concept like \"Mendelian Disease\" in the mental model of the person, and it requires some way to link the ID http://purl.obolibrary.org/obo/MONDO_0003847 to that \"mental\" concept - not always as trivial as in this case (where there are standard textbook definitions). The latter is usually achieved (philosophers and linguists please stop reading) by using an annotation that somehow explains the term - either a label or some kind of formal definition - that a person can understand. In any case, not trivial, but thankfully not the worst problem in the biomedical domain where we do have quite a wide range of shared \"mental models\" (more so in Biology than Medical Science..). Using URIs allows us to facilitate this \"understanding\" process by leaving behind some kind of information at the location that is dereferenced by the URI (basically you click on the URI and see what comes up). Note that there is a huge deal of compromise already happening across communities. In the original Semantic Web community, the hope was somehow that dereferencing the URI (clicking on it, navigating to it) would reveal structured information about the entity in question that could used by machines to understand what the entity is all about. In my experience, this was rarely ever realised in the biomedical domain. Some services like Ontobee expose such machine readable data on request (using a technique called content negotiation), but most URIs simply refer to some website that allow humans to understand what it means - which is already a huge deal. For more on names and identifiers I refer the interested reader to James Overton's OBO tutorial here . Personal note: Some of my experienced friends in the bioinformatics world say that \"IRI have been more pain than benefit\". It is clear that there is no single thing in the Semantic Web is entirely uncontested - everything has its critics and proponents. The advent of the CURIE and the bane of the CURIE map \u00b6 In reality, few biological resources will contain a reference to http://purl.obolibrary.org/obo/MONDO_0003847. More often, you will find something like MONDO:0003847 , which is called a CURIE . You will find CURIEs in many contexts, to make Semantic Web languages easier to read and manage. The premise is basically that your document contains a prefix declaration that says something like this: PREFIX MONDO: <http://purl.obolibrary.org/obo/MONDO_> which allows allows the interpreter to unfold the CURIE into the IRI: MONDO:0003847 -> http://purl.obolibrary.org/obo/MONDO_0003847 In reality, the proliferation of CURIEs has become a big problem for data engineers and data scientists when analysing data. Databases rarely, if ever, ship the CURIE maps with their data required to understand what a prefix effectively stands for, leading to a lot of guess-work in the daily practice of the Semantic Data Engineer (if you ever had to distinguish ICD: ICD10: ICD9: UMLS:, UMLSCUI: without a prefix map, etc you will know what I am talking about). Efforts to bring order to this chaos, essentially globally agreed CURIE maps (e.g. prefixcommons ), or ID management services such as identifiers.org exist, but right now there is no one solution - prepare yourself to having to deal with this issue when dealing with data integration efforts in the biomedical sciences. More likely than not, your organisation will build its own curie map and maintain it for the duration of your project. Semantic Web in the biomedical domain: Success stories \u00b6 There are probably quite a few divergent opinions on this, but I would like to humbly list the following four use cases as among the most impactful applications of Semantic Web Technology in the biomedical domain. Light Semantics for data aggregation . \u00b6 We can use hierarchical relations in ontology to group data. For example, if I know that http://purl.obolibrary.org/obo/MONDO_0012709 (\"microphthalmia, isolated, with coloboma 5\") http://www.w3.org/2000/01/rdf-schema#subClassOf (\"is a\") http://purl.obolibrary.org/obo/MONDO_0003847 (\"Mendelian Disease\"), then a specialised Semantic Web tool called a reasoner will know that, if I ask for all genes associated with Mendelian diseases, you also want to get those associated with \"microphthalmia, isolated, with coloboma 5\" specifically (note that many query engines such as SPARQL with RDFS entailment regime have simple reasoners embedded in them, but we would not call them \"reasoner\" - just query engine). Heavy Semantics for ontology management . \u00b6 Ontologies are extremely hard to manage and profit from the sound logical foundation provided by the Web Ontology Language (OWL). We can logically define our classes in terms of other ontologies, and then use a reasoner to classify our ontology automatically. For example, we can define abnormal biological process phenotypes in terms of biological processes (Gene Ontology) and classify our phenotypes entirely using the classification of biological processes in the Gene Ontology (don't worry if you don't understand a thing - we will get to that in a later week). Globally unique identifiers for data integration . \u00b6 Refer to the same thing the same way. While this goal was never reached in total perfection, we have gotten quite close. In my experience, there are roughly 3-6 ways to refer to entities in the biomedical domain (like say, ENSEMBL, HGNC, Entrez for genes; or SNOMED, NCIT, DO, MONDO, UMLS for diseases). So while the \"refer to the same thing the same way\" did not truly happen, a combination of standard identifiers with terminological mappings , i.e. links between terms, can be used to integrate data across resources (more about Ontology Matching later). Again, many of my colleagues disagree - they don't like IRIs, and unfortunately, you will have to build your own position on that. Personal note : From an evolutionary perspective, I sometimes think that having 2 or 3 competing terminological systems is better than 1, as the competition also drives the improvements in quality, but there is a lot of disagreement on this. Coordinated development of mutually compatible ontologies across the biomedical domain : The Open Biological and Biomedical Ontologies (OBO) Foundry. \u00b6 The OBO Foundry is a community-driven effort to coordinate the development of vocabularies and ontologies across the biomedical domain. It develops standards for the representation of terminological content (like standard properties), and ontological knowledge (shared design patterns) as well as shared systems for quality control. Flagship projects include: - The Relation Ontology (RO) for the standardisation of relationships that connect entities in biomedical ontologies. - The Core Ontology for Biology and Biomedicine (COB) : upper ontology to align key entities used throughout biomedical ontologies. - The OBO Metadata Ontology for aligning ontology metadata properties across OBO ontologies. - The OBO Persistent Identifier System : an Identifier scheme for persistent URIs used by many ontologies on the web. The system is used to refer to terms as well as ontologies and their versions. - OBO Dashboard : a system for the monitoring and continued improvement of OBO ontologies with automated Quality Control checks. Semantic Web and Linked Data: Things you should have heard about \u00b6 The Semantic Web Layer Cake : A iconic, colourful graphic that describes the layered design of the semantic web, from URIs to Logic. Its not particularly useful, but as a Semantic Web Explorer, you should have seen it. Linked Data is mostly referred to as a \"method for publishing data\", a key concept in the Semantic Web domain, coined by Tim Berners Lee in 2006. Related concepts: Linked Data Principles : Use URIs as names for things Use HTTP URIs so that people can look up those names. When someone looks up a URI, provide useful information. Include links to other URIs. so that they can discover more things. 5-Star system make your stuff available on the Web (whatever format) under an open license make it available as structured data (e.g., Excel instead of image scan of a table) use non-proprietary formats (e.g., CSV instead of Excel) use URIs to denote things, so that people can point at your stuff link your data to other data to provide context FAIR data : Principles defined in 2016, somewhat orthogonal to Linked Data Principles. A nice tutorial, also going a bit more in depth into identifiers than what we did in this section, can be found here . The idea of FAIR data is probably more impactful in the biomedical and pharmaceutical world then the idea of Linked Data. While there are some (slighltly irritating) voices on the sidelines that say that \"It can't be FAIR if its not RDF\", it is probably true that a nicely formatted CSV file on the Web is at least as useful as a (hard to understand) RDF dump containing the same data. Worldwide collaborations between major pharmaceutical corporations promoting FAIR data, such as the Pistoia Alliance do mention Semantic Web Technologies in their White papers , but keep the jargon a bit more hidden from the general public. Data, according to the FAIR principles, should be: Findable (machine readable metadata, etc) Accessible (open authentication, authorisation) Interoperable (integrated with other data, closely related to controlled vocabularies and linked data) Reusable (metadata, license, provenance) World Wide Web Consortium (W3C) : The World Wide Web Consortium (W3C) is an international community that develops open standards, in particular many of those (but not all!) pertaining to the Semantic Web. The Ecosystem of Linked Data and Semantic Web: Standards, Technologies and Research Areas \u00b6 In the following, we will list some of the technologies you may find useful, or will be forced to use, as a Semantic Data Engineer. Most of these standards will be covered in the subsequent weeks of this course. Standard Purpose Use case Web Ontology Language (OWL) Representing Knowledge in Biomedical Ontologies All OBO ontologies must be provided in OWL as well. Resource Description Framework (RDF) Model for data interchange. Triples, the fundamental unit of RDF, are ubiquitous on the Semantic Web SPARQL Query Language for RDF A standard query language for RDF and RDFS. Primary query language to interrogate RDF/RDFS/Linked Data on the Web. Simple Knowledge Organization System (SKOS) Another, more lightweight, knowledge organisation system in many ways competing with OWL. Not as widely used in the biomedical domain as OWL, but increasing uptake of \"matching\" vocabulary (skos:exactMatch, etc). RDF-star A key shortcoming of RDF is that, while I can in principle say everything about everything, I cannot directly talk about edges, for example to attribute provenance: \"microphthalmia, isolated, with coloboma 5 is kind of Mendelian disease\"--source: Wikipedia Use cases here . JSON-LD A method to encoding linked data in JSON format. (Very useful to at least know about). RDFa W3C Recommendation to embed rich semantic metadata in HTML (and XML). I have to admit - in 11 years Semantic Web Work I have not come across much use of RDFa in the biomedical domain. But @jamesaoverton is using it in his tools! A thorough overview of all the key standards and tools can be found on the Awesome Semantic Web repo. For a rough sense of current research trends it is always good to look at the accepted papers at one of the major conferences in the area. I like ISWC ( 2020 papers ), but for the aspiring Semantic Data Engineering in the biomedical sphere, it is probably a bit broad and theoretical. Other interesting specialised venues are the Journal of Biomedical Semantics and the International Conference on Biomedical Ontologies , but with the shift of the focus in the whole community towards Knowledge Graphs, other journals and conferences are becoming relevant. Here are a few key research areas, which are, by no means (!), exhaustive. How can we combine data and knowledge from different ontologies/knowledge graphs? Ontology/Knowledge graph alignment : How can we effectively link to ontologies, or knowledge graphs, together? Ontology merging : combine two ontologies by corresponding concepts and relations. Ontology matching: A sub-problem of ontology alignment, namely the problem of determining whether two terms (for example two diseases) from different ontologies should be linked together or not. How can we integrate data from unstructured and semistructured sources such as documents or spreadsheets? Named Entity Recognition (NER) : the process of identifying a named \"thing\" in a text. Entity linking : The task of associating a named entity, for example the result of a Named Entity Recognition algorithm, or the column of a spreadsheet, to a concept in an ontology. For example, the value \"Mendelian Disease\" is linked to the concept http://purl.obolibrary.org/obo/MONDO_0003847. Relationship extraction : Once you have identified the genes and diseases in your Pubmed abstracts, you will want to understand how they related to each other. Is the gene the \"basis in dysfunction of\" the disease? Or just randomly co-occurs in the sentence? Note: Many of the problems in this category are typically associated with the domain of Natural Language Processing rather than Semantic Web. How can we generate insight from semantically integrated data? Knowledge Graphs and Machine Learning Knowledge Graph Embeddings . The number one hype topic in recent years: How do you get from a graph of interrelated entities to a faithful representation in a vector space (basically numbers), so that Machine Learning algorithms can do their magic? Link predication : Based on what we know, which are the best drug targets for my rare disease of interest? Logical reasoning: While the research on deductive reasoning, at least the more \"hard-core\" Description Logic kind, seems to be a bit more quiet in recent years (maybe I am wrong here, I just see much less papers coming through my Google Scholar alerts now then I used to), there is still a lot going on in this domain: more efficient SPARQL engines, rule-based reasoning such as the recently commercialised RDFox reasoner and many more. Other research areas (not in any way exhaustive): Web decentralisation and privacy: Solid : Solid (Social Linked Data) is a web decentralization project led by Tim Berners-Lee, with the aim of true ownership of personal data and improved privacy. \"Pods\" are like secure personal web servers for data from which application can request data. Shape validation : It is very difficult to validate huge Knowledge Graphs of interrelated data efficiently (by validate we can mean a lot of things, such as making sure that your cat does not accidentally end up as someone's \"Mendelian Disease\"). Shape languages such as Shex and SHACL are poised to solve this problem, but the research is ongoing. New standards and tools: There is always someone proposing a new semantic standard for something or building a new kind of triple store, SPARQL extension or similar. Typical Jobs of Semantic Data Engineers in the biomedical domain \u00b6 It is useful to get a picture of the typical tasks a Semantic Data Engineer faces when building ontologies are Knowledge Graphs. In my experience, it is unlikely that any particular set of tools will work in all cases - most likely you will have to try and assemble the right toolchain for your use case and refine it over the lifetime of your project. The following are just a few points for consideration of tasks I regularly encountered - which may or may not overlap with the specific problems you will face. Finding the right ontologies \u00b6 There are no simple answers here and it very heavily depends on your use cases. We are discussing some places to look for ontologies here , but it may also be useful to simply upload the terms you are interested in to a service like Zooma and see what the terms map to at a major database provider like EBI. Finding the right data sources \u00b6 This is much harder still than it should have to be. Scientific databases are scattered across institutions that often do not talk to each other. Prepare for some significant work in researching the appropriate databases that could benefit your work, using Google and the scientific literature. Extending existing ontologies \u00b6 It is rare nowadays that you will have to develop an ontology entirely from scratch - most biomedical sub-domains will have some kind of reasonable ontology to build upon. However, there is often a great need to extend existing ontologies - usually because you have the need of representing certain concepts in much more detail, or your specific problem has not been modelled yet - think for example when how disease ontologies needed to be extended during the Coronavirus Crisis. Extending ontologies usually have two major facets: 1. If at all possible you should seek to contribute new terms, synonyms and relationships to the ontologies you seek to extend directly. Here, you can use GitHub to make issues requesting new terms, but more boldly, you can also add new terms yourself. We will teach you how to do that in one of the next weeks. 2. If the knowledge is considered out of scope for the ontology to be extended (for example because the terms are considered too detailed), then you will maintain your own \"branch\" of the ontology. Many tools such as the Ontology Development Kit and ROBOT can help you maintain such a branch but the general instinct should be: - Make a public GitHub repo. - Reach out to the developers of the main ontology - Stay in touch and coordinate releases Mapping data into ontologies/knowledge graphs \u00b6 Also sometimes more broadly referred to as \"data integration\", this problem involves a variety of tasks, such as: 1. Named Entity Recognition . If you have a set of documents, such as PubMed abstracts or clinical notes, you may have to first identify the parts of speech that refer to clinical entities. 2. Entity Linking : Once you have identified the biomedical entities of interest, you may want to link them to your existing knowledge graph. This process is sometimes called entity mapping or data mapping as well. Very often, this task is not fully automated. We have worked on projects where we used approaches to Entity Linking to suggest good mappings to users, which then had to confirm or reject them. It is also good to understand that not all entity linking must be vertical (i.e. between \"equivalent\" entities). Very often, there is no equivalent entity in your knowledge graph to map to, and here you need to decide whether to (a) create a new entity in the knowledge graph to map to or (b) map to a broader entity (for example \"microphthalmia, isolated, with coloboma 5\" to \"Mendelian Disease\"). What is more efficient / useful solely depends on your use case! Build application ontologies \u00b6 To make your data discoverable, it is often useful to extract a view from the ontologies you are using (for example, Gene Ontology, Disease Ontology) that only contains the terms and relationships of relevance to your data. We usually refer to this kind of ontology as an application ontology, or an ontology specific to your application, which will integrate subsets of other ontologies. This process will typically involve the following: - Define a seed, or a set of terms you want to import from your external ontologies of interest. - Extract relevant subsets from ontologies using this seed (for example using ROBOT extract ). - Combine and potentially link these subsets together. - Frameworks such as the Ontology Development Kit can help with this task, see for example the Coronavirus Vocabulary maintained by EBI. Leverage ontologies and knowledge graphs for you data analysis problems \u00b6 There are many ways your semantic data can be leveraged for data analysis, but in my experience, two are particularly central: 1. Data grouping and search : make data about \"microphthalmia, isolated, with coloboma 5\" available when searching for data about \"Mendelian Disease\". 2. Link prediction : Figure out what additional knowledge is hidden in your data that can drive your research (e.g. possible new therapies or drug targets). Additional materials and resources \u00b6 The open courses of the Hasso Plattner Institute (HPI) offer introductions into the concepts around Linked Data, Semantic Web and Knowledge Engineering. There are three courses of relevance to this weeks topics, all of which overlap significantly. Knowledge Engineering and the Web of Data : The oldest (2015), of the courses, but the most thorough when it comes to logical foundations, semantics and OWL. We will come back to this course in Weeks 4 and 5. Linked Data Engineering : Overlaps a lot with the Knowledge Engineering and the Web of Data course, with a bit more RDF/Linked Data focus. Knowledge Graphs : The most up-to-date of the three courses (2020), and will be referred to again in Week 12 of our course here. Contributors \u00b6 add name/ORCID here","title":"Analysing Linked Data"},{"location":"lesson/analysing_linked_data/#analysing-linked-data-fundamentals","text":"","title":"Analysing Linked Data (Fundamentals)"},{"location":"lesson/analysing_linked_data/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/analysing_linked_data/#prerequisites","text":"Review tutorial on Ontology Theory","title":"Prerequisites"},{"location":"lesson/analysing_linked_data/#preparation","text":"Essential Linked Data Engineering : Week 1 Support Programming Historian Linked Data tutorial Original Whitepaper (Tim Berners Lee et al) Educational curriculum for Linked Data Tools: Browse through the tools and standards listed in the Semantic Engineer Toolbox .","title":"Preparation"},{"location":"lesson/analysing_linked_data/#learning-objectives","text":"Advanced SPARQL Term enrichment Semantic similarity Named Entity Recognition","title":"Learning objectives"},{"location":"lesson/analysing_linked_data/#tutorials","text":"","title":"Tutorials"},{"location":"lesson/analysing_linked_data/#the-linked-data-landscape-from-an-obo-perspective-standards-services-and-tools","text":"In the following we will look a bit at the general Linked Data landscape, and name some of its flagship projects and standards. It is important to be clear that the Semantic Web field is a very heterogenous one:","title":"The Linked Data landscape from an OBO perspective: Standards, Services and Tools"},{"location":"lesson/analysing_linked_data/#flagship-projects-of-the-wider-semantic-web-community","text":"Linked Open Data (LOD) cloud : The flagship project of the Semantic Web. An attempt to make all, or anyways a lot, of Linked Data accessible in one giant knowledge graph. A good overview can be found in this medium article. Note that some people seem to think that the Semantic Web is (or should be) the Linked Open Data cloud. I would question this view, but I am not yet decided what my position is. Schema.org : General purpose vocabulary for entities on the web, founded by Google, Microsoft, Yahoo and Yandex. To get a better sense of the types of entities and relationships covered see here . DBpedia : Project that extracts structured data from Wikipedia and makes it available as a giant knowledge graph. The associated ontology , similar to schema.org, covers entities encountered in common sense knowledge. Wikidata : Free and open knowledge base that can be edited in much the same way as Wikipedia is edited. While these Semantic Web flagship projects are doubtlessly useful, it is sometimes hard to see how they can help for your biomedical research. We rarely make use of them in our day to day work as ontologists, but there are some notable exceptions: - Where our work involves modelling environmental factors, we sometimes use wikidata as a standard way to refer for example to countries. - For some more common sense knowledge use cases, such as nutrition, consider augmenting your knowledge graph with data from wikidata or dbpedia. While they may be a bit more messy and not directly useful for exploration by humans, it is quite possible that Machine Learning approaches can use the additional context provided by these knowledge graphs to improve embeddings and deliver more meaningful link predictions. - Some OBO ontologies are already on Wikidata - perhaps you can find additional synonyms and labels which help with your data mapping problems!","title":"Flagship projects of the wider Semantic Web community"},{"location":"lesson/analysing_linked_data/#where-the-obo-and-semantic-web-communities-are-slightly-at-odds","text":"The OBO format is a very popular syntax for representing biomedical ontologies. A lot of tools have been built over the years to hack OBO ontologies on the basis of that format - I still work with it on a daily basis. Although it has semantically been proven to be a subset of OWL (i.e. there is a lossless mapping of OBO into OWL) and can be viewed as just another syntax, it is in many ways ideosyncratic. For starters, you wont find many, if any, IRIs in OBO ontologies. The format itself uses CURIEs which are mapped to the general OBO PURL namespace during transformation to OWL. For example, if you see MONDO:0003847 in an OBO file, and were to translate it to OWL, you will see this term being translated to http://purl.obolibrary.org/obo/MONDO_0003847. Secondly, you have a bunch of built-in properties like BROAD or ABBREVIATION that mapped to a vocabulary called oboInOwl (oio). These are pretty non-standard on the general Semantic Web, and often have to be manually mapped to the more popular counterparts in the Dublin Core or SKOS namespaces. Having URIs as identifiers is not generally popular in the life sciences. As discussed elsewhere, it is much more likely to encounter CURIEs such as MONDO:0003847 than URIs such as http://purl.obolibrary.org/obo/MONDO_0003847 in biomedical databases.","title":"Where the OBO and Semantic Web communities are slightly at odds"},{"location":"lesson/analysing_linked_data/#useful-tools-for-biomedical-research","text":"Why does the biomedical research, and clinical, community care about the Semantic Web and Linked Data? There are endless lists of applications that try to apply semantic technologies to biomedical problems, but for this week, we only want to look at the broader picture. In our experience, the use cases where Semantic Web standards are applied successfully are: Where to find ontologies: Ontology repositories OBO Foundry Ontology Library BioPortal CPT Story . The Current Procedural Terminology was the by far most highly accessed Terminology on Bioportal - for many years. Due to license concerns, it had to be withdrawn from the repository. This story serves a cautionary tale of using terminologies with non-open or non-transparent licensing schemes. AgroPortal : Like BioPortal, but focussed on the Agronomy domain. Linked Open Data Vocabularies (LOV) : Lists the most important vocabularies in the Linked Data space, such as Dublin Core , SKOS and Friend-of-a-Friend (FOAF). Where to find terms: Term browsers OLS : The boss of the current term browsers out there. While the code base is a bit dated, it still gives access to a wide range of relevant open biomedical ontology terms. Note, while being a bit painful, it is possible to set up your own OLS (for your organisation) which only contains those terms/ontologies that are relevant for your work. Ontobee : The default term browser for OBO term purls. For example, click on http://purl.obolibrary.org/obo/OBI_0000070. This will redirect you directly to Ontobee, to show you the terms location in the hierarchy. A key difference between Ontobee and OLS/Bioportal is that Ontobee limits hierarchical relationships to is_a. This means if you are browsing ontologies such as GO, Uberon, CL, ENVO, you will not see part-of links in the hierarchy, and these links are crucial for understanding these ontologies. AberOWL : Another ontology repository and semantic search engine. Some ontologies such as PhenomeNet can only be found on AberOWL, however, I personally prefer OLS. identifiers.org : A centralised registry for identifiers used in the life sciences. This is one of the tools that bridge the gap between CURIEs and URLs, but it does not cover (OBO) ontologies very well, and if so, is not aware of the proper URI prefixes (see for example here , and HP term resolution that does not list the proper persistent URL of the HP identifier (http://purl.obolibrary.org/obo/HP_0000001)). Identifiers.org has mainly good coverage for databases/resources that use CURIE type identifiers. But: you can enter any ID you find in your data and it will tell you what it is associated with. Curate biomedical data. There are a lot of different tools in this space - which we will discuss in a bespoke unit later in the course. Examples: isatools : The open source ISA framework and tools help to manage an increasingly diverse set of life science, environmental and biomedical experiments that employing one or a combination of technologies. RightField : System for curating ontology terms in Excel spreadsheets. CEDAR Templates : Basically a templating system that allows to create templates to record metadata, for example in a lab setting, of course with ontology integration. Other examples of tabular data to RDF converters, but new ones coming up every year. Building ontologies Populous/Webulous : A system to maintain/generate ontologies from spreadsheets. The idea was to basically to define patterns in a (now mostly dead) language called OPPL, and then apply them to spreadsheets to generate OWL axioms. EBI recently discontinued the service, as there is a general exodus to Google Sheets + ROBOT templates instead. ROBOT templates + Google Sheets and Cogs : A lightweight approach based on a set of tools that allows curating ontologies in spreadsheets (e.g. Google Sheets) which are converted into OWL using ROBOT. DOSDP tools + Dead Simple Design Patterns (DOSDP) : Similar to ROBOT templates, DOSDPs (which really should be called DOSDTs, because they are not really design patterns ; they are ontology templates), another system that allows the generation of OWL axioms based on spreadsheet data. Cleaning messy data OpenRefine : I have not myself used this ever, but some of my colleagues have. OpenRefine allows you to upload (spreadsheet) data, explore it and clean it (going as far as reconciling terms using Wikidata concepts).","title":"Useful tools for biomedical research"},{"location":"lesson/analysing_linked_data/#which-biomedical-ontologies-should-we-use","text":"As a rule of thumb, for every single problem/term/use case, you will have 3-6 options to choose from, in some cases even more. The criteria for selecting a good ontology are very much dependent on your particular use case, but some concerns are generally relevant. A good first pass is to apply to \" 10 simple rules for selecting a Bio-ontology \" by Malone et al, but I would further recommend to ask yourself the following: Do I need the ontology for grouping and semantic analysis? In this case a high quality hierarchy reflecting biological subsumption is imperative. We will explain later what this means, but in essence, you should be able to ask the following question: \"All instances/occurrences of this concept in the ontology are also instances of all its parent classes. Everything that is true about the parent class is always also true about instances of the children.\" It is important for you to understand that, while OWL semantics imply the above, OWL is difficult and many ontologies \"pretend\" that the subclass link means something else (like a rule of thumb grouping relation). Can I handle multiple inheritance in my analysis? While I personally recommend to always consider multiple inheritance (i.e, allow a term to have more than one parent class), there are some analysis frameworks, in particular in the clinical domain, that make this hard. Some ontologies are inherently ploy-hierarchical (such as Mondo ), while others strive to be single inheritance ( DO , ICD). Are key resources I am interested in using the ontology? Maybe the most important question that will drastically reduce the amount of data mapping work you will have to do: Does the resource you wish to integrate already annotate to a particular ontology? For example, EBI resources will be annotating phenotype data using EFO, which in turn used HPO identifiers. If your use case demands to integrate EBI databases, it is likely a good idea to consider using HPO as the reference ontology for your phenotype data. Aside from aspects of your analysis, there is one more thing you should consider carefully: the open-ness of your ontology in question. As a user, you have quite a bit of power on the future trajectory of the domain, and therefore should seek to endorse and promote open standards as much as possible (for egotistic reasons as well: you don't want to have to suddenly pay for the ontologies that drive your semantic analyses). It is true that ontologies such as SNOMED have some great content, and, even more compellingly, some really great coverage. In fact, I would probably compare SNOMED not with any particular disease ontology, but with the OBO Foundry as a whole, and if you do that, it is a) cleaner, b) better integrated. But this comes at a cost. SNOMED is a commercial product - millions are being payed every year in license fees, and the more millions come, the better SNOMED will become - and the more drastic consequences will the lock-in have if one day you are forced to use SNOMED because OBO has fallen too far behind. Right now, the sum of all OBO ontologies is probably still richer and more valuable, given their use in many of the central biological databases (such as the ones hosted by the EBI ) - but as SNOMED is seeping into the all aspects of genomics now (for example, it will soon be featured on OLS !) it will become increasingly important to actively promote the use of open biomedical ontologies - by contributing to them as well as by using them. We will discuss ontologies in the medical, phenomics and genomics space in more detail in a later session of the course.","title":"Which biomedical ontologies should we use?"},{"location":"lesson/analysing_linked_data/#other-interesting-links","text":"Linked Data in e-Government Industrial Ontologies Foundry : Something like the OBO Foundry for Industrial Ontologies OntoCommons : An H2020 CSA project dedicated to the standardisation of data documentation across all domains related to materials and manufacturing.","title":"Other interesting links"},{"location":"lesson/analysing_linked_data/#basic-linked-data-and-semantic-web-concepts-for-the-semantic-data-engineer-in-the-biomedical-domain","text":"In this section we will discuss the following: - Introductory remarks - The advantages of globally unique identifiers - Some success stories of the Semantic Web in the biomedical domain - Some basic concepts you should probably have heard about - The ecosystem of the Semantic Web: Standards, Technologies and Research Areas - Typical tasks of Semantic Data Engineers in the biomedical domain","title":"Basic Linked data and Semantic Web Concepts for the Semantic Data Engineer in the Biomedical Domain"},{"location":"lesson/analysing_linked_data/#introduction","text":"Note of caution : No two Semantic Web overviews will be equivalent to each other. Some people claim the Semantic Web as an idea is an utter failure, while others praise it as a great success (in the making) - in the end you will have to make up your own mind. In this section I focus on parts of the Semantic Web step particularly valuable to the biomedical domain, and I will omit many relevant topics in the wider Semantic Web area, such as Enterprise Knowledge Graphs, decentralisation and personalisation, and many more. Also, the reader is expected to be familiar with the basic notions of the Semantic Web, and should use this overview mainly to tie some of the ideas together. The goal of this section is to give the aspiring Semantic Data Engineer in the biomedical domain a rough idea of key concepts around Linked Data and the Semantic Web insofar as they relate to their data science and and data engineering problems. Even after 20 years of Semantic Web research (the seminal paper , conveniently and somewhat ironically behind a paywall, was published in May 2001), the area is still dominated by \"academic types\", although the advent of the Knowledge Graph is already changing that. As I already mentioned above, no two stories of what the Semantic Web is will sound the same. However, there are a few stories that are often told to illustrate why we need semantics. The OpenHPI course names a few: - \"From the web of documents to the web of data\" tells the story of how the original web is essentially a huge heap of (interlinked) natural language text documents which are very hard to understand for search engines: Does the word \"Jaguar\" on this site refer to the car or the cat? Clarifying in your web page that the word Jaguar refers to the concept of \"Jaguar the cat\", for example like this: <span about=\"dbpedia:Jaguar\">Jaguar</span> , will make it easier for the search engine to understand what your site is about and link it to other relevant content. From this kind of mark-up, structured data can be extracted and integrate into a giant, worldwide database, and exposed through SPARQL endpoints, that can then be queried using a suitable query language. - \"From human to machine understandable\": as a Human, we know that a Jaguar is a kind of cat, and all cats have four legs. If you ask a normal search engine: \"Does a Jaguar have four legs?\" it will have a tough time to answer this question correctly (if it cannot find that exact statement anywhere). That is why we need proper semantics , some kind of formalism such that a \"machine\" can deduce from the statements \"Jaguar is a cat; Cat has four legs\" that \"Jaguar has four legs\". - The \"Semantic Layer Cake\": a box/brick diagram showing how Semantic Web Technologies are stacked on top of each other. An engineering centric view that has been used countless times to introduce the Semantic Web, but rarely helped anyone to understand what it is about. I am not entirely sure anymore that any of these ways (web of data, machine understanding, layered stack of matching standards) to motivate the Semantic Web are particularly effective for the average data scientists or engineer. If I had to explain the Semantic Web stack to my junior self, just having finished my undergraduate, I would explain it as follows (no guarantee though it will help you). The Semantic Web / Linked Data stack comprises roughly four components that are useful for the aspiring Semantic (Biomedical) Data Engineer/Scientist to distinguish:","title":"Introduction"},{"location":"lesson/analysing_linked_data/#a-way-to-refer-to-things-including-entities-and-relations-in-a-global-namespace","text":"You, as a scientist, might be using the term \"gene\" to refer to basic physical and functional unit of heredity, but me, as a German, prefer the term \"Gen\". In the Semantic Web, instead of natural language words, we prefer to use URIs to refer to things such as https://www.wikidata.org/wiki/Q7187: if you say something using the name https://www.wikidata.org/wiki/Q7187, both your German and Japanese colleagues will \"understand\" what you are referring to. More about that in the next chapter.","title":"A way to refer to things (including entities and relations) in a global namespace."},{"location":"lesson/analysing_linked_data/#lots-loaaaads-of-ways-to-make-statements-about-things","text":"For example, to express \"a mutation of SHH in humans causes isolated microphthalmia with coloboma-5\" you could say something like (http://purl.obolibrary.org/obo/MONDO_0012709 | \"microphthalmia, isolated, with coloboma 5\")--[http://purl.obolibrary.org/obo/RO_0004020 | \"has basis in dysfunction of\"]-->(https://identifiers.org/HGNC:10848 | \"SSH (gene)\"). Or you could say: (http://purl.obolibrary.org/obo/MONDO_0012709 | \"microphthalmia, isolated, with coloboma 5\")--[http://www.w3.org/2000/01/rdf-schema#subClassOf | \"is a\"]-->(http://purl.obolibrary.org/obo/MONDO_0003847 | \"Mendelian Disease\"). If we use the analogy of \"language\", then the URIs (above) are the words, and the statements are sentences in a language . Unfortunately, there are many languages in the Semantic Web, such as OWL, RDFS, SKOS, SWRL, SHACL, SHEX, and dialects (OWL 2 EL, OWL 2 RL) and a plethora of scripts, or serialisations (you can store the exact same sentence in the same language such as RDF, or OWL, in many different ways)- more about that later. In here lies also one of the largest problems of the Semantic Web - lots of overlapping standards means, lots of incompatible data - which raises the bar for actually being able to seamlessly integrate \"statements about things\" across resources.","title":"Lots (loaaaads!) of ways to make statements about things."},{"location":"lesson/analysing_linked_data/#collections-of-statements-about-things-that-somehow-belong-together-and-provide-some-meaning-or-context-for-those-things","text":"Examples include: - controlled vocabularies , that define, for example, how to refer to a disease (e.g., we use http://purl.obolibrary.org/obo/MONDO_0012709 to refer to \"isolated microphthalmia with coloboma 5\"), - terminologies which describe how we humans refer to a disease (How is it called in German? Which other synonyms are used in the literature? How is the term defined in the medical literature?), - taxonomies which define how diseases are related hierarchically (\"microphthalmia, isolated, with coloboma 5 is a kind of Mendelian disease\"), - ontologies which further define how diseases are defined in terms of other entities, for example \"microphthalmia, isolated, with coloboma 5 is a Mendelian disease that has its basis in the dysfunction of SSH\". - Note : In practice, when we say \"ontology\", we mean all of the above together - it is, however, good to know that they are somewhat distinct, and that there are different \"languages\" that can be used for each of these distinctions.","title":"Collections of statements about things that somehow belong together and provide some meaning, or context, for those things."},{"location":"lesson/analysing_linked_data/#tools-that-do-something-useful-with-these-collections-of-statements","text":"For example (as always, non exhaustive): Efficient storage (triple stores, in-memory ontology APIs, other databases). Similar to traditional SQL databases, the Semantic Web comes with a number of database solutions that are optimised to deliver \"semantic content\". Semantically aware querying : Very similar to traditional SQL (which, incidentally is often great to query semantic data), there are various ways to \"interrogate\", or query, your Linked Data, such as SPARQL, DL Querying , Ontology-based data access (OBDA). Subsetting (module/subset extraction): Often, ontologies (or other collections of Linked Data statements) are very large and cover a lot of entities and knowledge that is not important to your work. There are a number of techniques that allow you to extract meaningful subsets for your use case; for example, you may be interested to get all the information you can about Mendelian diseases, but you don't care about common diseases (e.g. see ROBOT extract ). Visualisation : As a Data Scientist, you are used to looking at your data in tabular form - while a lot of information stored in ontologies can still be inspected this way, in general semantic data is best perceived as a graph - which are notoriously hard to visualise. Fortunately, a lot of Linked Data, in particular ontologies in the biomedical domain are predominantly tree-shaped (you have a disease, and underneath sub-diseases). Term browsers like OLS typically render ontologies as trees. Data linking/matching : This is key in particular in the biomedical sciences, as there is almost never just one way to refer to the same thing. In my experience, a good rule of thumb is that there are 3-6, e.g. 3-6 URIs that refer to \"Mendelian Disease\", all of which need to be matched together to integrate data across resources. There are many approaches to ontology matching - none of which are anywhere near perfect. Automated error checking and validation (Syntax, Structure (SHEX, SHACL), logical Consistency (DL Reasoner)): Naturally, writing sentences in any language is hard in the beginning, but this is even more true for highly complex languages such as OWL. In my experience, no-one can write flawless OWL without the help of automatic syntax and semantics checking, at least not consistently. Validation tools are a crucial part of your Semantic Engineering toolbox. Translate between languages: Often we need to translate from one language, for example OWL, to another, for example SKOS to integrate divergent resources. Translations in the Semantic Web context are nearly always lossy (there are always things you can say in one language, but not in another), but they may be necessary nevertheless. Discovery of terms ( OLS , BioPortal ): If you are curating terms, you need to know what ID (URI) to use for \"isolated microphthalmia with coloboma 5\". For that, term browsers such as OLS are perfect. Just type in your natural language search term, and you will find a series of suggestions for URIs you can use for your curation. Discovery of vocabularies OBO Foundry ontology library , BioPortal ): we will have a section later on on how to select appropriate ontologies for your use case, but the general problem of finding vocabularies, or ontologies, is answered by ontology repositories or libraries. Naturally, our favourite ontology library is the OBO Foundry ontology library which contain a lot of high quality ontologies for the biomedical domain. Make implicit knowledge explicit, aka reasoning : Deductive (DL Reasoning, Rule-based reasoning such as Datalog, SWRL). One of the major selling points for OWL, for example, in the biomedical domain is the ability to use logical reasoning in a way that is sound (only gives you correct inferences, at all times) and complete (all hidden implications are found, at all times, by the reasoner) - this is particularly great for medical knowledge where mistakes in computer algorithms can have devastating effects. However, I am slowly coming to the conviction that sound and complete reasoning is not the only form of deductive reasoning that is useful - many rule languages can offer value to your work by unveiling hidden relationships in the data without giving such strong \"logical guarantees\". Inductive (Machine Learning approaches, Knowledge Graph Representation Learning). The new frontier - we will discuss later in the course how our ontology-powered Knowledge Graphs can be leveraged to identify drug targets, novel gene to phenotype associations and more, using a diverse set of Machine Learning-based approaches. This week will focus on 1 (identifiers) and 4 (applications) - 2 (languages and standards) and 3 (controlled vocabularies and ontologies) will be covered in depth in the following weeks. Note on the side : Its not always 100% clear what is meant by Linked Data in regular discourse. There are some supposedly \"clear\" definitions (\" method for publishing structured data \", \" collection of interrelated datasets on the Web \"), but when it comes down to the details, there is plenty of confusion (does an OWL ontology constitute Linked Data when it is published on the Web? Is it Linked Data if it does not use RDF? Is it Linked Data if it is less than 5-star - see below). In practice all these debates are academic and won't mean much to you and your daily work. There are entities , statements (context) being said about these entities using some standard (associated with the Semantic Web, such as OWL or RDFS) and tools that do something useful with the stuff being said.","title":"Tools that do something useful with these collections of statements."},{"location":"lesson/analysing_linked_data/#when-i-say-mendelian-disease-i-mean-httppurlobolibraryorgobomondo_0003847","text":"One of the top 5 features of the Semantic Web (at least in the context of biomedical sciences) is the fact that we can use URIs as a global identifier scheme that is unambiguous, independent of database implementations, independent of language concerns to refer to the entities in our domain. For example, if I want to refer to the concept of \"Mendelian Disease\", I simply refer to http://purl.obolibrary.org/obo/MONDO_0003847 - and everyone, in Japan, Germany, China or South Africa, will be able to \"understand\" or look up what I mean. I don't quite like the word \"understanding\" in this context as it is not actually trivial to explain to a human how a particular ID relates to a thing in the real world (semiotics). In my experience, this process is a bit rough in practice - it requires that there is a concept like \"Mendelian Disease\" in the mental model of the person, and it requires some way to link the ID http://purl.obolibrary.org/obo/MONDO_0003847 to that \"mental\" concept - not always as trivial as in this case (where there are standard textbook definitions). The latter is usually achieved (philosophers and linguists please stop reading) by using an annotation that somehow explains the term - either a label or some kind of formal definition - that a person can understand. In any case, not trivial, but thankfully not the worst problem in the biomedical domain where we do have quite a wide range of shared \"mental models\" (more so in Biology than Medical Science..). Using URIs allows us to facilitate this \"understanding\" process by leaving behind some kind of information at the location that is dereferenced by the URI (basically you click on the URI and see what comes up). Note that there is a huge deal of compromise already happening across communities. In the original Semantic Web community, the hope was somehow that dereferencing the URI (clicking on it, navigating to it) would reveal structured information about the entity in question that could used by machines to understand what the entity is all about. In my experience, this was rarely ever realised in the biomedical domain. Some services like Ontobee expose such machine readable data on request (using a technique called content negotiation), but most URIs simply refer to some website that allow humans to understand what it means - which is already a huge deal. For more on names and identifiers I refer the interested reader to James Overton's OBO tutorial here . Personal note: Some of my experienced friends in the bioinformatics world say that \"IRI have been more pain than benefit\". It is clear that there is no single thing in the Semantic Web is entirely uncontested - everything has its critics and proponents.","title":"When I say \"Mendelian Disease\" I mean http://purl.obolibrary.org/obo/MONDO_0003847"},{"location":"lesson/analysing_linked_data/#the-advent-of-the-curie-and-the-bane-of-the-curie-map","text":"In reality, few biological resources will contain a reference to http://purl.obolibrary.org/obo/MONDO_0003847. More often, you will find something like MONDO:0003847 , which is called a CURIE . You will find CURIEs in many contexts, to make Semantic Web languages easier to read and manage. The premise is basically that your document contains a prefix declaration that says something like this: PREFIX MONDO: <http://purl.obolibrary.org/obo/MONDO_> which allows allows the interpreter to unfold the CURIE into the IRI: MONDO:0003847 -> http://purl.obolibrary.org/obo/MONDO_0003847 In reality, the proliferation of CURIEs has become a big problem for data engineers and data scientists when analysing data. Databases rarely, if ever, ship the CURIE maps with their data required to understand what a prefix effectively stands for, leading to a lot of guess-work in the daily practice of the Semantic Data Engineer (if you ever had to distinguish ICD: ICD10: ICD9: UMLS:, UMLSCUI: without a prefix map, etc you will know what I am talking about). Efforts to bring order to this chaos, essentially globally agreed CURIE maps (e.g. prefixcommons ), or ID management services such as identifiers.org exist, but right now there is no one solution - prepare yourself to having to deal with this issue when dealing with data integration efforts in the biomedical sciences. More likely than not, your organisation will build its own curie map and maintain it for the duration of your project.","title":"The advent of the CURIE and the bane of the CURIE map"},{"location":"lesson/analysing_linked_data/#semantic-web-in-the-biomedical-domain-success-stories","text":"There are probably quite a few divergent opinions on this, but I would like to humbly list the following four use cases as among the most impactful applications of Semantic Web Technology in the biomedical domain.","title":"Semantic Web in the biomedical domain: Success stories"},{"location":"lesson/analysing_linked_data/#light-semantics-for-data-aggregation","text":"We can use hierarchical relations in ontology to group data. For example, if I know that http://purl.obolibrary.org/obo/MONDO_0012709 (\"microphthalmia, isolated, with coloboma 5\") http://www.w3.org/2000/01/rdf-schema#subClassOf (\"is a\") http://purl.obolibrary.org/obo/MONDO_0003847 (\"Mendelian Disease\"), then a specialised Semantic Web tool called a reasoner will know that, if I ask for all genes associated with Mendelian diseases, you also want to get those associated with \"microphthalmia, isolated, with coloboma 5\" specifically (note that many query engines such as SPARQL with RDFS entailment regime have simple reasoners embedded in them, but we would not call them \"reasoner\" - just query engine).","title":"Light Semantics for data aggregation."},{"location":"lesson/analysing_linked_data/#heavy-semantics-for-ontology-management","text":"Ontologies are extremely hard to manage and profit from the sound logical foundation provided by the Web Ontology Language (OWL). We can logically define our classes in terms of other ontologies, and then use a reasoner to classify our ontology automatically. For example, we can define abnormal biological process phenotypes in terms of biological processes (Gene Ontology) and classify our phenotypes entirely using the classification of biological processes in the Gene Ontology (don't worry if you don't understand a thing - we will get to that in a later week).","title":"Heavy Semantics for ontology management."},{"location":"lesson/analysing_linked_data/#globally-unique-identifiers-for-data-integration","text":"Refer to the same thing the same way. While this goal was never reached in total perfection, we have gotten quite close. In my experience, there are roughly 3-6 ways to refer to entities in the biomedical domain (like say, ENSEMBL, HGNC, Entrez for genes; or SNOMED, NCIT, DO, MONDO, UMLS for diseases). So while the \"refer to the same thing the same way\" did not truly happen, a combination of standard identifiers with terminological mappings , i.e. links between terms, can be used to integrate data across resources (more about Ontology Matching later). Again, many of my colleagues disagree - they don't like IRIs, and unfortunately, you will have to build your own position on that. Personal note : From an evolutionary perspective, I sometimes think that having 2 or 3 competing terminological systems is better than 1, as the competition also drives the improvements in quality, but there is a lot of disagreement on this.","title":"Globally unique identifiers for data integration."},{"location":"lesson/analysing_linked_data/#coordinated-development-of-mutually-compatible-ontologies-across-the-biomedical-domain-the-open-biological-and-biomedical-ontologies-obo-foundry","text":"The OBO Foundry is a community-driven effort to coordinate the development of vocabularies and ontologies across the biomedical domain. It develops standards for the representation of terminological content (like standard properties), and ontological knowledge (shared design patterns) as well as shared systems for quality control. Flagship projects include: - The Relation Ontology (RO) for the standardisation of relationships that connect entities in biomedical ontologies. - The Core Ontology for Biology and Biomedicine (COB) : upper ontology to align key entities used throughout biomedical ontologies. - The OBO Metadata Ontology for aligning ontology metadata properties across OBO ontologies. - The OBO Persistent Identifier System : an Identifier scheme for persistent URIs used by many ontologies on the web. The system is used to refer to terms as well as ontologies and their versions. - OBO Dashboard : a system for the monitoring and continued improvement of OBO ontologies with automated Quality Control checks.","title":"Coordinated development of mutually compatible ontologies across the biomedical domain: The Open Biological and Biomedical Ontologies (OBO) Foundry."},{"location":"lesson/analysing_linked_data/#semantic-web-and-linked-data-things-you-should-have-heard-about","text":"The Semantic Web Layer Cake : A iconic, colourful graphic that describes the layered design of the semantic web, from URIs to Logic. Its not particularly useful, but as a Semantic Web Explorer, you should have seen it. Linked Data is mostly referred to as a \"method for publishing data\", a key concept in the Semantic Web domain, coined by Tim Berners Lee in 2006. Related concepts: Linked Data Principles : Use URIs as names for things Use HTTP URIs so that people can look up those names. When someone looks up a URI, provide useful information. Include links to other URIs. so that they can discover more things. 5-Star system make your stuff available on the Web (whatever format) under an open license make it available as structured data (e.g., Excel instead of image scan of a table) use non-proprietary formats (e.g., CSV instead of Excel) use URIs to denote things, so that people can point at your stuff link your data to other data to provide context FAIR data : Principles defined in 2016, somewhat orthogonal to Linked Data Principles. A nice tutorial, also going a bit more in depth into identifiers than what we did in this section, can be found here . The idea of FAIR data is probably more impactful in the biomedical and pharmaceutical world then the idea of Linked Data. While there are some (slighltly irritating) voices on the sidelines that say that \"It can't be FAIR if its not RDF\", it is probably true that a nicely formatted CSV file on the Web is at least as useful as a (hard to understand) RDF dump containing the same data. Worldwide collaborations between major pharmaceutical corporations promoting FAIR data, such as the Pistoia Alliance do mention Semantic Web Technologies in their White papers , but keep the jargon a bit more hidden from the general public. Data, according to the FAIR principles, should be: Findable (machine readable metadata, etc) Accessible (open authentication, authorisation) Interoperable (integrated with other data, closely related to controlled vocabularies and linked data) Reusable (metadata, license, provenance) World Wide Web Consortium (W3C) : The World Wide Web Consortium (W3C) is an international community that develops open standards, in particular many of those (but not all!) pertaining to the Semantic Web.","title":"Semantic Web and Linked Data: Things you should have heard about"},{"location":"lesson/analysing_linked_data/#the-ecosystem-of-linked-data-and-semantic-web-standards-technologies-and-research-areas","text":"In the following, we will list some of the technologies you may find useful, or will be forced to use, as a Semantic Data Engineer. Most of these standards will be covered in the subsequent weeks of this course. Standard Purpose Use case Web Ontology Language (OWL) Representing Knowledge in Biomedical Ontologies All OBO ontologies must be provided in OWL as well. Resource Description Framework (RDF) Model for data interchange. Triples, the fundamental unit of RDF, are ubiquitous on the Semantic Web SPARQL Query Language for RDF A standard query language for RDF and RDFS. Primary query language to interrogate RDF/RDFS/Linked Data on the Web. Simple Knowledge Organization System (SKOS) Another, more lightweight, knowledge organisation system in many ways competing with OWL. Not as widely used in the biomedical domain as OWL, but increasing uptake of \"matching\" vocabulary (skos:exactMatch, etc). RDF-star A key shortcoming of RDF is that, while I can in principle say everything about everything, I cannot directly talk about edges, for example to attribute provenance: \"microphthalmia, isolated, with coloboma 5 is kind of Mendelian disease\"--source: Wikipedia Use cases here . JSON-LD A method to encoding linked data in JSON format. (Very useful to at least know about). RDFa W3C Recommendation to embed rich semantic metadata in HTML (and XML). I have to admit - in 11 years Semantic Web Work I have not come across much use of RDFa in the biomedical domain. But @jamesaoverton is using it in his tools! A thorough overview of all the key standards and tools can be found on the Awesome Semantic Web repo. For a rough sense of current research trends it is always good to look at the accepted papers at one of the major conferences in the area. I like ISWC ( 2020 papers ), but for the aspiring Semantic Data Engineering in the biomedical sphere, it is probably a bit broad and theoretical. Other interesting specialised venues are the Journal of Biomedical Semantics and the International Conference on Biomedical Ontologies , but with the shift of the focus in the whole community towards Knowledge Graphs, other journals and conferences are becoming relevant. Here are a few key research areas, which are, by no means (!), exhaustive. How can we combine data and knowledge from different ontologies/knowledge graphs? Ontology/Knowledge graph alignment : How can we effectively link to ontologies, or knowledge graphs, together? Ontology merging : combine two ontologies by corresponding concepts and relations. Ontology matching: A sub-problem of ontology alignment, namely the problem of determining whether two terms (for example two diseases) from different ontologies should be linked together or not. How can we integrate data from unstructured and semistructured sources such as documents or spreadsheets? Named Entity Recognition (NER) : the process of identifying a named \"thing\" in a text. Entity linking : The task of associating a named entity, for example the result of a Named Entity Recognition algorithm, or the column of a spreadsheet, to a concept in an ontology. For example, the value \"Mendelian Disease\" is linked to the concept http://purl.obolibrary.org/obo/MONDO_0003847. Relationship extraction : Once you have identified the genes and diseases in your Pubmed abstracts, you will want to understand how they related to each other. Is the gene the \"basis in dysfunction of\" the disease? Or just randomly co-occurs in the sentence? Note: Many of the problems in this category are typically associated with the domain of Natural Language Processing rather than Semantic Web. How can we generate insight from semantically integrated data? Knowledge Graphs and Machine Learning Knowledge Graph Embeddings . The number one hype topic in recent years: How do you get from a graph of interrelated entities to a faithful representation in a vector space (basically numbers), so that Machine Learning algorithms can do their magic? Link predication : Based on what we know, which are the best drug targets for my rare disease of interest? Logical reasoning: While the research on deductive reasoning, at least the more \"hard-core\" Description Logic kind, seems to be a bit more quiet in recent years (maybe I am wrong here, I just see much less papers coming through my Google Scholar alerts now then I used to), there is still a lot going on in this domain: more efficient SPARQL engines, rule-based reasoning such as the recently commercialised RDFox reasoner and many more. Other research areas (not in any way exhaustive): Web decentralisation and privacy: Solid : Solid (Social Linked Data) is a web decentralization project led by Tim Berners-Lee, with the aim of true ownership of personal data and improved privacy. \"Pods\" are like secure personal web servers for data from which application can request data. Shape validation : It is very difficult to validate huge Knowledge Graphs of interrelated data efficiently (by validate we can mean a lot of things, such as making sure that your cat does not accidentally end up as someone's \"Mendelian Disease\"). Shape languages such as Shex and SHACL are poised to solve this problem, but the research is ongoing. New standards and tools: There is always someone proposing a new semantic standard for something or building a new kind of triple store, SPARQL extension or similar.","title":"The Ecosystem of Linked Data and Semantic Web: Standards, Technologies and Research Areas"},{"location":"lesson/analysing_linked_data/#typical-jobs-of-semantic-data-engineers-in-the-biomedical-domain","text":"It is useful to get a picture of the typical tasks a Semantic Data Engineer faces when building ontologies are Knowledge Graphs. In my experience, it is unlikely that any particular set of tools will work in all cases - most likely you will have to try and assemble the right toolchain for your use case and refine it over the lifetime of your project. The following are just a few points for consideration of tasks I regularly encountered - which may or may not overlap with the specific problems you will face.","title":"Typical Jobs of Semantic Data Engineers in the biomedical domain"},{"location":"lesson/analysing_linked_data/#finding-the-right-ontologies","text":"There are no simple answers here and it very heavily depends on your use cases. We are discussing some places to look for ontologies here , but it may also be useful to simply upload the terms you are interested in to a service like Zooma and see what the terms map to at a major database provider like EBI.","title":"Finding the right ontologies"},{"location":"lesson/analysing_linked_data/#finding-the-right-data-sources","text":"This is much harder still than it should have to be. Scientific databases are scattered across institutions that often do not talk to each other. Prepare for some significant work in researching the appropriate databases that could benefit your work, using Google and the scientific literature.","title":"Finding the right data sources"},{"location":"lesson/analysing_linked_data/#extending-existing-ontologies","text":"It is rare nowadays that you will have to develop an ontology entirely from scratch - most biomedical sub-domains will have some kind of reasonable ontology to build upon. However, there is often a great need to extend existing ontologies - usually because you have the need of representing certain concepts in much more detail, or your specific problem has not been modelled yet - think for example when how disease ontologies needed to be extended during the Coronavirus Crisis. Extending ontologies usually have two major facets: 1. If at all possible you should seek to contribute new terms, synonyms and relationships to the ontologies you seek to extend directly. Here, you can use GitHub to make issues requesting new terms, but more boldly, you can also add new terms yourself. We will teach you how to do that in one of the next weeks. 2. If the knowledge is considered out of scope for the ontology to be extended (for example because the terms are considered too detailed), then you will maintain your own \"branch\" of the ontology. Many tools such as the Ontology Development Kit and ROBOT can help you maintain such a branch but the general instinct should be: - Make a public GitHub repo. - Reach out to the developers of the main ontology - Stay in touch and coordinate releases","title":"Extending existing ontologies"},{"location":"lesson/analysing_linked_data/#mapping-data-into-ontologiesknowledge-graphs","text":"Also sometimes more broadly referred to as \"data integration\", this problem involves a variety of tasks, such as: 1. Named Entity Recognition . If you have a set of documents, such as PubMed abstracts or clinical notes, you may have to first identify the parts of speech that refer to clinical entities. 2. Entity Linking : Once you have identified the biomedical entities of interest, you may want to link them to your existing knowledge graph. This process is sometimes called entity mapping or data mapping as well. Very often, this task is not fully automated. We have worked on projects where we used approaches to Entity Linking to suggest good mappings to users, which then had to confirm or reject them. It is also good to understand that not all entity linking must be vertical (i.e. between \"equivalent\" entities). Very often, there is no equivalent entity in your knowledge graph to map to, and here you need to decide whether to (a) create a new entity in the knowledge graph to map to or (b) map to a broader entity (for example \"microphthalmia, isolated, with coloboma 5\" to \"Mendelian Disease\"). What is more efficient / useful solely depends on your use case!","title":"Mapping data into ontologies/knowledge graphs"},{"location":"lesson/analysing_linked_data/#build-application-ontologies","text":"To make your data discoverable, it is often useful to extract a view from the ontologies you are using (for example, Gene Ontology, Disease Ontology) that only contains the terms and relationships of relevance to your data. We usually refer to this kind of ontology as an application ontology, or an ontology specific to your application, which will integrate subsets of other ontologies. This process will typically involve the following: - Define a seed, or a set of terms you want to import from your external ontologies of interest. - Extract relevant subsets from ontologies using this seed (for example using ROBOT extract ). - Combine and potentially link these subsets together. - Frameworks such as the Ontology Development Kit can help with this task, see for example the Coronavirus Vocabulary maintained by EBI.","title":"Build application ontologies"},{"location":"lesson/analysing_linked_data/#leverage-ontologies-and-knowledge-graphs-for-you-data-analysis-problems","text":"There are many ways your semantic data can be leveraged for data analysis, but in my experience, two are particularly central: 1. Data grouping and search : make data about \"microphthalmia, isolated, with coloboma 5\" available when searching for data about \"Mendelian Disease\". 2. Link prediction : Figure out what additional knowledge is hidden in your data that can drive your research (e.g. possible new therapies or drug targets).","title":"Leverage ontologies and knowledge graphs for you data analysis problems"},{"location":"lesson/analysing_linked_data/#additional-materials-and-resources","text":"The open courses of the Hasso Plattner Institute (HPI) offer introductions into the concepts around Linked Data, Semantic Web and Knowledge Engineering. There are three courses of relevance to this weeks topics, all of which overlap significantly. Knowledge Engineering and the Web of Data : The oldest (2015), of the courses, but the most thorough when it comes to logical foundations, semantics and OWL. We will come back to this course in Weeks 4 and 5. Linked Data Engineering : Overlaps a lot with the Knowledge Engineering and the Web of Data course, with a bit more RDF/Linked Data focus. Knowledge Graphs : The most up-to-date of the three courses (2020), and will be referred to again in Week 12 of our course here.","title":"Additional materials and resources"},{"location":"lesson/analysing_linked_data/#contributors","text":"add name/ORCID here","title":"Contributors"},{"location":"lesson/application_ontology_development/","text":"Application Ontology Development \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Ontology Design Ontology Pipelines What is delivered as part of the course \u00b6 Description: add here Learning objectives \u00b6 add learning objectives here Self-study curriculum \u00b6 Tutorials \u00b6 in person or video (link videos here as they become available) Additional materials and resources \u00b6 TBD Contributors \u00b6 Add contributors with ORCID here","title":"Application Ontology Development"},{"location":"lesson/application_ontology_development/#application-ontology-development","text":"","title":"Application Ontology Development"},{"location":"lesson/application_ontology_development/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/application_ontology_development/#prerequisites","text":"Ontology Design Ontology Pipelines","title":"Prerequisites"},{"location":"lesson/application_ontology_development/#what-is-delivered-as-part-of-the-course","text":"Description: add here","title":"What is delivered as part of the course"},{"location":"lesson/application_ontology_development/#learning-objectives","text":"add learning objectives here","title":"Learning objectives"},{"location":"lesson/application_ontology_development/#self-study-curriculum","text":"","title":"Self-study curriculum"},{"location":"lesson/application_ontology_development/#tutorials","text":"in person or video (link videos here as they become available)","title":"Tutorials"},{"location":"lesson/application_ontology_development/#additional-materials-and-resources","text":"TBD","title":"Additional materials and resources"},{"location":"lesson/application_ontology_development/#contributors","text":"Add contributors with ORCID here","title":"Contributors"},{"location":"lesson/automating_ontology_workflows/","text":"Automating Ontology Development Workflows: Make, Shell and Automation Thinking \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Ontology Pipelines What is delivered as part of the course \u00b6 In this course, you will learn the basics of automation in and around the OBO ontology world - and beyond. The primary goal is to enable ontology pipeline developers to plan the automation of their ontology workflows and data pipelines, but some of the materials are very general and apply to scientific computing more widely. The course serves also as a prerequisite for advanced application ontology development. Learning objectives \u00b6 Unix shell make Advanced Git, GitHub Preparation \u00b6 Please complete the following tutorials. - The Unix Shell (~4.5 hours) - Version Control with Git (~3 hours) - Introduction to GitHub Tutorials \u00b6 Thinking \"Automation\" \u00b6 By: James Overton Automation is part of the foundation of the modern world. The key to using and building automation is a certain way of thinking about processes, how they can be divided into simple steps, and how they operate on inputs and outputs that must be exactly the same in some respects but different in others. In this article I want to make some basic points about automation and how to think about it. The focus is on automation with software and data, but not on any particular software or data. Some of these points may seem too basic, especially for experienced programmers, but in 20+ years of programming I've never seen anybody lay out these basic points in quite this way. I hope it's useful. The Basics \u00b6 \"automatos\" from the Greek: \"acting of itself\" Automation has two key aspects: make the input the same process the inputs in the same way The second part is more visible, and tends to get more attention, but the first part is at least as important. While automation makes much of the modern world possible, it is not new, and there are serious pitfalls to avoid. No system is completely automatic, so it's best to think of automation on a spectrum, and starting thinking about automation at the beginning of a new project. Examples of Automation \u00b6 To my mind, the word \"automation\" brings images of car factories, with conveyor belts and robotic arms moving parts and welding them together. Soon they might be assembling self-driving (\"autonomous\") cars. Henry Ford is famous for making cars affordable by building the first assembly lines, long before there were any robots. The essential steps for Ford were standardizing the inputs and the processes to get from raw materials to a completed car. The history of the 20th century is full of examples of automation in factories of all sorts. Automation was essential to the Industrial Revolution, but it didn't start then. We can look to the printing press. We can look to clocks, which regimented lives in monasteries and villages. We can think of recipes, textiles, the logistics of armies, advances in agriculture, banking, the administration of empires, and so on. The scientific revolution was built on repeatable experiments published in letters and journal articles. I think that the humble checklist is also an important relative of automation. Automation is not new, but it's an increasingly important part of our work and our lives. Software Automation is Special \u00b6 Software is almost always written as source code in text files that are compiled and/or interpreted as machine code for a specific set of hardware. Software can drive machines of all sorts, but a lot of software automation stays inside the computer, working on data in files and databases, and across networks. We'll be focused on this kind of software automation, transforming data into data. The interesting thing about this is that source code is a kind of data, so there are software automation workflows that operate on data that defines software. The upshot is that you can have automation that modifies itself. Doing this on a large scale introduces a lot of complexity, but doing it on a small scale can be a clean solution to certain problems. Another interesting thing about software is that once we solve an automation problem once we can copy that solution and apply it again and again for almost zero cost. We don't need to build a new factory or a new threshing machine. We can just download a program and run it. Henry Ford could make an accurate estimate of how long it would take to build a car on his assembly line, but software development is not like working on the assembly line, and estimating time and budget for software development is notoriously hard. I think this is because software developers aren't just executing automation, they're building new automation for each new project. Although we talk about \"bit rot\", and software does require maintenance of a sort, software doesn't break down or wear out in the same ways that physical machines do. So while the Industrial Revolution eliminated many jobs, it also created different jobs, building and maintaining the machines. It's not clear that software automation will work the same way. Software automation is special because it can operate on itself, and once complete can be cheaply copied. Software development is largely about building automated systems of various sorts, usually out of many existing pieces. We spend most of our time building new systems, or modifying an existing system to handle new inputs, or adapting existing software to a new use case. The Dangers of Automation \u00b6 To err is human; to really foul things up requires a computer. An obvious danger of automation is that machines are faster than humans, so broken automation can often do more damage more quickly than a human can. A related problem is that humans usually have much more context and depth of experience, which we might call \"common sense\", and a wider range of sensory inputs than most automated systems. This makes humans much better at recognizing that something has gone wrong with a process and that it's time to stop. New programmers soon learn that a simple program that performs perfectly when the input is in exactly the right format, becomes a complex program once it's updated to handle a wide range of error conditions. In other words, it's almost always much harder to build automation that can gracefully handler errors and problems than it is to automate just the \"happy path\". Old programmers have learned through bitter experience that it's often practically impossible to predict all the things that can go wrong with an automated system in practise. I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail. -- Abraham Maslow A less obvious danger of automation comes from the sameness requirement. When you've built a great piece of automation, perfectly suited to inputs of a certain type, it's very tempting to apply that automation more generally. You start paying too much attention to how things are the same, and not enough attention to their differences. You may begin to ignore important differences. You may surrender your common sense and good judgment, to save yourself the work of changing the automated system or making an exception. Bureaucracies are a form of automation. Everyone has had a bad experience filling out some form that ignores critical information, and with some bureaucrat who would not apply common sense and make an exception. Keep all this in mind as you build automated systems: a broken machine can do a lot of damage very quickly, and a system built around bad assumptions can do a lot of hidden damage. A Spectrum of Automation \u00b6 Let's consider a simple case of automation with software, and build from the most basic sort of automation to a full-fledged system. Say you have a bunch of text files in a directory, each containing minutes from meetings that we had together over the years. You can remember that I talked about a particular software package that might solve a problem that you just discovered, but you can't remember the name. 1. Ad Hoc \u00b6 The first thing you try is to just search the directory. On a Mac you would open the Finder, navigate to the directory, and type \"James\" into the search bar. Unfortunately that gives too many results: all the files with the minutes for a meeting where I said something. The next thing to do is double click some text files, which would open them in Text Edit program, and skim them. You might get lucky! You know that I the meeting was in 2019, so you can try and filter for files modified in that year. Unfortunately the files have been updated at different times, so the file dates aren't useful. Now if each file was named with a consistent pattern, including the meeting date, then it would be simple to filter for files with \"2019\" in the name. This isn't automation, but it's the first step in the right direction. Consistent file names are one way to make inputs the same so that you can process them in the same way. Let's say it works: you filter for files from 2019 with \"James\" in them, skim a few, and find a note where I recommended using Pandoc to convert between document formats. Mission accomplished! 2. Notes \u00b6 Next week you need to do something very similar: Becky mentioned a website where you can find an important dataset. It's basically the same problem with different inputs. If you remember exactly what you did last time, then you can get the job done quickly. As the job gets more complicated and more distant in time, and as you find yourself doing similar tasks more often, it's nice to have notes about what you did and how you did it. If I'm using a graphical user interface (GUI) then for each step I'll note the program I used, and the menu item or button I clicked, e.g. \"Preferences > General > Font Size\", or \"Search\" or \"Run\". If I'm using a command-line interface (CLI) then I'll copy-paste the commands into my notes. I often keep informal notes like this in a text file in the relevant directory. I name the file \"notes.txt\". A \"README\" file is similar. It's used to describe the contents of a directory, often saying which files are which, or what the column headers for a given table mean. Often the task is more complicated and requires one or more pieces of software that I don't use every day. If there's relevant documentation, I'll put a link to it in my notes, and then a short summmary of exactly what I did. In this example I look in the directory of minutes and see my \"notes.txt\" file. I read that and remember how I filtered on \"2019\" and searched for \"James\". This time I filter on \"2020\" and search for \"Becky\", and I find the website for the dataset quickly enough. As a rule of thumb, it might take you three times longer to find your notes file, write down the steps you took, and provide a short description, than it would to just do the job without taking notes. When you're just taking notes for yourself, this often feels like a waste of time (you'll remember, right?!), and sometimes it is a bit of a waste. If you end up using your notes to help with similar tasks in the future, then this will likely be time well spent. As a rule of thumb, it might take three times longer to write notes for a broader audience than notes for just yourself. This is because you need to take into account the background knowledge of your reader, including her skills and assumptions and context, and especially the possible misunderstandings that you can try to avoid with careful writing. I often start with notes for just myself and then expand them for a wider audience only when needed. 3. Checklist \u00b6 When tasks get more complicated or more important then informal notes are not enough. The next step on the spectrum of automation is the humble checklist. The most basic checklists are for making sure that each item has been handled. Often the order isn't important, but lists are naturally ordered from top to bottom, and in many cases that order is useful. For example, my mother lays out her shopping lists in the order of the aisles in her local grocery store, making it easier to get each item and check it off without skipping around and perhaps having to backtrack. I think of a checklist as a basic form of automation. It's like a recipe. It should lay out the things you need to start, then proceed through the required steps in enough detail that you can reproduce them. In some sense, by using the checklist you are becoming the \"machine\". You are executing an algorithm that should take you from the expected inputs to the expected output. Humble as the checklist is, there's a reason that astronauts, pilots, and surgical teams live by their checklists. Even when the stakes are not so high, it's often nice to \"put your brain on autopilot\" and just work the checklist without having to remember and reconsider the details of each step. A good checklist is more focused than a file full of notes. A checklist has a goal at the end. It has specific starting conditions. The steps have been carefully considered, so that they have the proper sequence, and none are missing. Perhaps most importantly, a checklist helps you break a complex task down into simple parts. If one of the parts is still too complex, then break it down again into a nested checklist (really a sort of tree structure). Checklists sometimes include another key element of automation: conditionals. A shopping list might say \"if there's a sale on crackers, then buy three boxes\". If-then conditions let our automated systems adapt to circumstances. The \"then\" part is just another step, but the \"if\" part is a little different. It's a test to determine whether a condition holds. We almost always want the result of the test to be a simple True or False. Given a bunch of inputs, some of which pass the test and some of which fail it, we can think of the test as determining some way in which all the things that pass are the same and all the things that fail are the same . Programmers will also be familiar with more complex conditionals such as if-then-else, if-elseif-else, and \"case\", which divide process execution across multiple \"branches\". As a rule of thumb, turning notes into a checklist will likely take at least three times as long as simply writing the notes. If the checklist is for a wider audience, expect it to take three times as long to write, for the same reasons mentioned above for notes. If a task is simple and I can hold all the steps in my head, and I can finish it in one sitting without distractions, then I won't bother with a checklist. But more and more I find myself writing myself a checklist before I begin any non-trivial tasks. I use bullet points in my favourite text editor, or sometimes the Notes app on my iPhone. I lay out the steps in the expected order, and I check them off as I go. Sometimes I start making the checklist days before I need it, so I have lots of time to think about it and improve it. If there's a job that I'm worried about, breaking it down into smaller pieces usually helps to make the job feel more manageable. Actually, I try to start every workday by skimming my (long) To Do list, picking the most important tasks, and making a checklist for what I want to get done by quitting time. 3. Checkscript \u00b6 \"Checkscript\" is a word that I think I made up, based on insights from a couple of sources, primarily this blog post on \"Do-nothing scripting: the key to gradual automation\" This is where \"real\" automation kicks in, writing \"real\" code and stuff, but hopefully you'll see that it's just one more step on the spectrum of automation that I'm describing. The notes and checklists we've been discussing are just text in your favourite text editor. A checkscript is a program. It can be written in whatever programming language you prefer. I'll give examples in Posix Shell, but that blog post uses Python, and it really doesn't matter. You start with a checklist (in your mind at least). The first version of your program should just walk you through your checklist. The program should walk you through each step of your checklist, one by one. That's it. Here's a checkscript based on the example above. It just prints the first step ( echo ), waits for you to press any key ( read ), then prints the next step, and so on. ###!/bin/sh echo \"1. Use Finder to filter for files with '2019' in the name\" read -p \"Press enter to continue\" echo \"2. Use finder to search file content for 'James'\" read -p \"Press enter to continue\" echo \"3. Open files in Text Edit and search for 'James'\" read -p \"Press enter to continue\" echo \"Done!\" So far this is just a more annoying way to use a checklist. The magic happens once you break the steps down into small enough pieces and realize that you know how to tell the computer to do some of the steps instead of doing them all yourself. For example, you know that the command-line tool grep is used for searching the contents of files, and that you can use \"fileglob\"s to select just the files that you want to search, and that you can send the output of grep to another file to read in your favourite text editor. Now you know how to automate the first two steps. The computer can just do that work without waiting for you: ###!/bin/sh grep \"James\" *2019* > search_results.txt echo \"1. Open 'search_results.txt' in Text Edit and search for 'James'\" read -p \"Press enter to continue\" echo \"Done!\" Before we were using the Finder, and it is possible to write code to tell the Finder to filter and seach for files. The key advantage of grep here is that we send the search results to another file that we can read now or save for later. This is also a good time to mention the advantage of text files over word processor files. If the minutes were stored in Word files, for example, then Finder could probably search them and you could use Word to read them, but you wouldn't be able to use grep or easily output the results to another file. Unix tools such as grep treat all text files the same, whether they're source code or meeting minutes, which means that these tools work pretty much the same on any text file. By keeping your data in Word you restrict yourself to a much smaller set of tools and make it harder to automate you work with simple scripts like this one. Even if you can't get the computer to run any of the steps for you automatically, a checkscript can still be useful by using variables instead of repeating yourself: ###!/bin/sh FILE_PATTERN=\"*2019*\" FILE_CONTENTS=\"James\" echo \"1. Use Finder to filter for files with '${FILE_PATTERN}' in the name\" read -p \"Press enter to continue\" echo \"2. Use finder to search file content for '${FILE_CONTENTS}'\" read -p \"Press enter to continue\" echo \"3. Open files in Text Edit and search for '${FILE_CONTENTS}'\" read -p \"Press enter to continue\" echo \"Done!\" Now if I want to search for \"Becky\" I can just change the FILE_CONTENTS variable in one place. I find this especially useful for dates and version numbers. This is pretty simple for a checkscript, with very few steps. A more realistic example would be if there were many directories containing the minutes of many meetings, maybe in different file formats and with different naming conventions. In order to be sure that we're searching all of them we might need a longer checkscript. Writing and using a checkscript instead of a checklist will likely take (you guessed it) about three times as long. But the magic of the checkscript is in the title of the blog post I mentioned: \"gradual automation\". Once you have a checkscript, you can run through it all manually, but you can also automate bits a pieces of the task, saving yourself time and effort next time. 5. Script \u00b6 A \"script\" is a kind of program that's easy to edit and run. There are technical distinctions to be made between \"compiled\" programs and \"interpreted\" programs, but they turn out to be more complicated and less helpful than they seem at first. Technically, a checkscript is just a script that waits for you to do the hard parts. In this section I want to talk about \"fully automated\" or \"standalone\" scripts that you just provide some input and execute. Most useful programs are useful because they call other programs (in the right ways). I like shell scripts because they're basically just commands that are copied and pasted from work I was doing on the command-line. It's really easy to call other programs. To continue our example, say that our minutes were stored in Word files. There are Python libraries for this, such as python-docx . You can write a little script using this library that works like grep to search for specified text in selected files, and output the results to a search results file. As you add more and more functionality to a script it can become unwieldy. Scripts work best when they have a simple \"flow\" from beginning to end. They may have some conditionals and some loops, but once you start seeing nested conditionals and loops, then your script is doing too much. There are two main options to consider: break your script into smaller, simpler scripts build a specialized tool: the next step on the spectrum of automation The key difference between a checkscript and a \"standalone\" script is handling problems. A checkscript relies on you to supervise it. A standalone script is expected to work properly without supervision. So the script has to be designed to handle a wider range of inputs and fail gracefully when it gets into trouble. This is a typical case of the \"80% rule\": the last 20% takes 80% of the time. As a rule of thumb, expect it to take three times as long to write a script that can run unsupervised than it takes you to write a checkscript that does \"almost\" the same thing. 6. Specialized Tool \u00b6 When your script needs nested conditionals and loops, then it's probably time to reach for a programming language that's designed to write code \"in the large\". Some languages such as Python can make a pretty smooth transition from a script in a single file to a set of files in a module, working together nicely. You might also choose another language that can provide better performance or efficiency. It's not just the size and the logical complexity of your script, consider its purpose . The specialized tools that I have in mind have a clear purpose that helps guide their design. This also makes them easier to reuse across multiple projects. I often divide my specialized tools into two parts: a library and a command-line interface. The library can be used in other programs, and contains the most distinctive and important functionality. But the command-line interface is essential, because it lets me use my specialized tool in the shell and in scripts, so I can build more automation on top of it. Writing a tool in Java or C++ or Rust usually takes longer than a script in shell or Python because there are more details to worry about such as types and efficient memory management. In return you usually get more reliability and efficiency. But as a rule of thumb, expect it to take three times as long to write a specialized tool than it would to \"just\" write the script. On the other hand, if you already have a script that does most of what you want, and you're already familiar with the target you are moving to, then it can be fairly straightforward to translate from the script to the specialized tool. That's why it's often most efficient to write a prototype script first, do lots of quick experiments to explore the design space, and when you're happy with the design then start on the \"production\" version. 7. Workflow \u00b6 The last step in the spectrum of automation is to bring together all your scripts into a single \"workflow\". My favourite tool for this is the venerable Make . A Makefile is essentially a bunch of small scripts with their input and output files carefully specified. When you ask Make to build a given output file, it will look at the whole tree of scripts, figure out which input files are required to build your requested output file, then which files are required to build those files, and so on until it has determined a sequence of steps. Make is also smart enough to check whether some of the dependencies are already up-to-date, and can skip those steps. Looking at a Makefile you can see everything broken down into simple steps and organized into a tree, through which you can trace various paths. You can make changes at any point, and run Make again to update your project. I've done this all so many times that now I often start with a Makefile in an empty directory and build from there. I try experiments on the command line. I make notes. I break the larger task into parts with a checklist. I automate the easy parts first, and leave some parts as manual steps with instructions. I write little scripts in the Makefile . I write larger scripts in the src/ directory. If these get too big or complex, I start thinking about building a specialized tool. (And of course, I store everything in version control.) It takes more time at the beginning, but I think that I usually save time later, because I have a nice place to put everything from the start. In other words, I start thinking about automation at the very beginning of the project, assuming from the start that it will grow, and that I'll need to go back and change things. With a mindset for automation, from the start I'm thinking about how the inputs I care about are the same and different, which similarities I can use for my tests and code, and which differences are important or unimportant. Conclusion \u00b6 In the end, my project isn't ever completely automated. It doesn't \"act of itself\". But by making everything clear and explicit I'm telling the computer how to do a lot of the work and other humans (or just my future self) how to do the rest of it. The final secret of automation, especially when it comes to software and data, is communication : expressing things clearly for humans and machines so they can see and do exactly what you did. Scientific Computing: An Overview \u00b6 By: James Overton By \"scientific computing\" we mean using computers to help with key aspect of science such as data collection, cleaning, interpretation, analysis, and visualization. Some people use \"scientific computing\" to mean something more specific, focusing on computational modelling or computationally intensive analysis. We'll be focusing on more general and day-to-day topics: how can a scientist make best use of a computer to do their work well? These three things apply to lots of fields, but are particularly important to scientists: reliability reproducibility communication It should be no surprise that automation can help with all of these. When working properly, computers make fewer mistakes than people, and the mistakes they do make are more predictable. If we're careful, our software systems can be easily reproduced, which means that an entire data analysis pipeline can be copied and run by another lab to confirm the results. And scientific publications are increasingly including data and code as part of the review and final publication process. Clear code is one of the best ways to communicate detailed steps. Automation is critical to scientific instruments and experiments, but we'll focus on the data processing and analysis side: after the data has been generated, how should you deal with it. Basic information management is always important: community standard file formats consistent file naming documentation, READMEs backups version control More advanced data management is part of this course: consistent use of versioned software reference data terminology controlled vocabularies data dictionaries ontologies Some simple rules of thumb can help reduce complexity and confusion: make space firm foundations one-way data flow plan for change test from the start documentation is also for you Make Space \u00b6 When starting a new project, make a nice clean new space for it. Try for that \"new project smell\". I always create a new directory on my computer. I almost always create a new GitHub repository. I usually create a README and a Makefile, right away. It's not always clear when a project is really \"new\" or just a new phase of an old project. But try to clear some space to make a fresh start. Firm Foundations \u00b6 A lot of data analysis starts with a reference data set. It might be a genome or a proteome. It might be a corpus. It might be a set of papers or data from those papers. Start by finding that data and selecting a particular version of it. Write that down clearly in your notes. If possible, include a unique identifier such as a ( persistent ) URL or DOI. If that's not possible, write down the steps you took. If the data isn't too big, keep a copy of it in your fresh new project directory. If the data is a bit too big, keep a compressed copy in a zip or gz file. A lot of software is perfectly happy to read directly from compressed files, and you can compress or uncompress data using piped commands in your shell or script. If the data is really too big, then be extra careful to keep notes on exactly where you can find it again. Consider storing just the hashes of the big files, so you can confirm that they have exactly the same contents. If you know from the start that you will need to compare your results with someone else's, make sure that you're using the same reference data that they are. This may require a conversation, but trust me that it's better to have this conversation now than later. One-Way Data Flow \u00b6 It's much easier to think about processes that flow in one direction. Branches are a little trickier, but usually fine. The real trouble comes with loops. Once a process loops back on itself it's much more difficult to reason about what's happening. Loops are powerful, but with great power comes great responsibility. Keep the systems you design as simple as possible (but no simpler). In practical terms: Try not to read then write to the same file. If you have to, try to append rather than overwrite. This is one reason why I prefer tables on disk to databases. Don't hesitate to write intermediate files. These are very useful for testing and debugging. When you're \"finished\" you can comment out these steps. Plan for Change \u00b6 It's very tempting: you could automate this step, or you could just do it manually. It might take three times as long to automate it, right? So you can save yourself some precious time by just opening Excel and \"fixing\" things by hand. Sometimes that bet will pay off, but I lose that bet most of the time. I tend to realize my mistake only at the last minute. The submission deadline is tomorrow but the core lab \"fixed\" something and they have a new version of the dataset that we need to use for the figures. Now I really don't have time to automate, so I'm up late clicking through Excel again and hoping that I remembered to redo all the changes that I made last time. Automating the process would have actually saved me time, but more importantly it would have avoided a lot of stress. By now I should know that the dataset will almost certainly be revised at the last minute. If I have the automation set up, then I just update the data, run the automation again, and quickly check the results. Test from the Start \u00b6 Tests are another thing that take time to implement. One of the key benefits to tests is (again) communication. When assessing or trying out some new piece of software I often look to the test files to see examples of how the code is really used, and the shape of the inputs and outputs. There's a spectrum of tests that apply to different parts of your system: unit tests: individual functions and methods regression tests: ensure that fixed bugs do not reappear integration tests: end-to-end functionality performance tests: system speed and resource usage acceptance tests: whether the overall system meets its design goals Tests should be automated. The test suite should either pass or fail, and if it fails something needs to be fixed before any more development is done. The automated test suite should run before each new version is committed to version control, and ideally more often during development. Tests come with costs: development cost of writing the tests time and resources spent running the tests maintenance costs of updating the tests The first is obvious but the other two often more important. A slow test suite is annoying to run, and so it won't get run. A test suite that's hard to update won't get updated, and then failures will be ignored, which defeats the entire purpose. Documentation is also for You \u00b6 I tend to forget how bad a memory I have. In the moment, when I'm writing brilliant code nothing could be more obvious than the perfect solution that is pouring forth from my mind all over my keyboard. But when I come back to that code weeks, months, or years later, I often wonder what the heck I was thinking. We think about the documentation we write as being for other people, but for a lot of small projects it's really for your future self. Be kind to your future self. They may be even more tired, even more stressed than you are today. There's a range of different forms of documentation, worth a whole discussion of its own. I like this four-way distinction : tutorials: getting started, basic concepts, an overview how-to guides: how to do common tasks explanation: why does it work this way? reference: looking up the details You don't need all of these for your small project, but consider a brief explanation of why it works the way it does (aimed at a colleague who knows your field well), and some brief notes on how-to do the stuff this project is for. These could both go in the README of a small project. Additional materials and resources \u00b6 A whirlwind introduction to the command line Programming with Python Oh My Git! Contributors \u00b6 Nico Matentzoglu James Overton","title":"Automating Ontology Workflows"},{"location":"lesson/automating_ontology_workflows/#automating-ontology-development-workflows-make-shell-and-automation-thinking","text":"","title":"Automating Ontology Development Workflows: Make, Shell and Automation Thinking"},{"location":"lesson/automating_ontology_workflows/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/automating_ontology_workflows/#prerequisites","text":"Ontology Pipelines","title":"Prerequisites"},{"location":"lesson/automating_ontology_workflows/#what-is-delivered-as-part-of-the-course","text":"In this course, you will learn the basics of automation in and around the OBO ontology world - and beyond. The primary goal is to enable ontology pipeline developers to plan the automation of their ontology workflows and data pipelines, but some of the materials are very general and apply to scientific computing more widely. The course serves also as a prerequisite for advanced application ontology development.","title":"What is delivered as part of the course"},{"location":"lesson/automating_ontology_workflows/#learning-objectives","text":"Unix shell make Advanced Git, GitHub","title":"Learning objectives"},{"location":"lesson/automating_ontology_workflows/#preparation","text":"Please complete the following tutorials. - The Unix Shell (~4.5 hours) - Version Control with Git (~3 hours) - Introduction to GitHub","title":"Preparation"},{"location":"lesson/automating_ontology_workflows/#tutorials","text":"","title":"Tutorials"},{"location":"lesson/automating_ontology_workflows/#thinking-automation","text":"By: James Overton Automation is part of the foundation of the modern world. The key to using and building automation is a certain way of thinking about processes, how they can be divided into simple steps, and how they operate on inputs and outputs that must be exactly the same in some respects but different in others. In this article I want to make some basic points about automation and how to think about it. The focus is on automation with software and data, but not on any particular software or data. Some of these points may seem too basic, especially for experienced programmers, but in 20+ years of programming I've never seen anybody lay out these basic points in quite this way. I hope it's useful.","title":"Thinking \"Automation\""},{"location":"lesson/automating_ontology_workflows/#the-basics","text":"\"automatos\" from the Greek: \"acting of itself\" Automation has two key aspects: make the input the same process the inputs in the same way The second part is more visible, and tends to get more attention, but the first part is at least as important. While automation makes much of the modern world possible, it is not new, and there are serious pitfalls to avoid. No system is completely automatic, so it's best to think of automation on a spectrum, and starting thinking about automation at the beginning of a new project.","title":"The Basics"},{"location":"lesson/automating_ontology_workflows/#examples-of-automation","text":"To my mind, the word \"automation\" brings images of car factories, with conveyor belts and robotic arms moving parts and welding them together. Soon they might be assembling self-driving (\"autonomous\") cars. Henry Ford is famous for making cars affordable by building the first assembly lines, long before there were any robots. The essential steps for Ford were standardizing the inputs and the processes to get from raw materials to a completed car. The history of the 20th century is full of examples of automation in factories of all sorts. Automation was essential to the Industrial Revolution, but it didn't start then. We can look to the printing press. We can look to clocks, which regimented lives in monasteries and villages. We can think of recipes, textiles, the logistics of armies, advances in agriculture, banking, the administration of empires, and so on. The scientific revolution was built on repeatable experiments published in letters and journal articles. I think that the humble checklist is also an important relative of automation. Automation is not new, but it's an increasingly important part of our work and our lives.","title":"Examples of Automation"},{"location":"lesson/automating_ontology_workflows/#software-automation-is-special","text":"Software is almost always written as source code in text files that are compiled and/or interpreted as machine code for a specific set of hardware. Software can drive machines of all sorts, but a lot of software automation stays inside the computer, working on data in files and databases, and across networks. We'll be focused on this kind of software automation, transforming data into data. The interesting thing about this is that source code is a kind of data, so there are software automation workflows that operate on data that defines software. The upshot is that you can have automation that modifies itself. Doing this on a large scale introduces a lot of complexity, but doing it on a small scale can be a clean solution to certain problems. Another interesting thing about software is that once we solve an automation problem once we can copy that solution and apply it again and again for almost zero cost. We don't need to build a new factory or a new threshing machine. We can just download a program and run it. Henry Ford could make an accurate estimate of how long it would take to build a car on his assembly line, but software development is not like working on the assembly line, and estimating time and budget for software development is notoriously hard. I think this is because software developers aren't just executing automation, they're building new automation for each new project. Although we talk about \"bit rot\", and software does require maintenance of a sort, software doesn't break down or wear out in the same ways that physical machines do. So while the Industrial Revolution eliminated many jobs, it also created different jobs, building and maintaining the machines. It's not clear that software automation will work the same way. Software automation is special because it can operate on itself, and once complete can be cheaply copied. Software development is largely about building automated systems of various sorts, usually out of many existing pieces. We spend most of our time building new systems, or modifying an existing system to handle new inputs, or adapting existing software to a new use case.","title":"Software Automation is Special"},{"location":"lesson/automating_ontology_workflows/#the-dangers-of-automation","text":"To err is human; to really foul things up requires a computer. An obvious danger of automation is that machines are faster than humans, so broken automation can often do more damage more quickly than a human can. A related problem is that humans usually have much more context and depth of experience, which we might call \"common sense\", and a wider range of sensory inputs than most automated systems. This makes humans much better at recognizing that something has gone wrong with a process and that it's time to stop. New programmers soon learn that a simple program that performs perfectly when the input is in exactly the right format, becomes a complex program once it's updated to handle a wide range of error conditions. In other words, it's almost always much harder to build automation that can gracefully handler errors and problems than it is to automate just the \"happy path\". Old programmers have learned through bitter experience that it's often practically impossible to predict all the things that can go wrong with an automated system in practise. I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail. -- Abraham Maslow A less obvious danger of automation comes from the sameness requirement. When you've built a great piece of automation, perfectly suited to inputs of a certain type, it's very tempting to apply that automation more generally. You start paying too much attention to how things are the same, and not enough attention to their differences. You may begin to ignore important differences. You may surrender your common sense and good judgment, to save yourself the work of changing the automated system or making an exception. Bureaucracies are a form of automation. Everyone has had a bad experience filling out some form that ignores critical information, and with some bureaucrat who would not apply common sense and make an exception. Keep all this in mind as you build automated systems: a broken machine can do a lot of damage very quickly, and a system built around bad assumptions can do a lot of hidden damage.","title":"The Dangers of Automation"},{"location":"lesson/automating_ontology_workflows/#a-spectrum-of-automation","text":"Let's consider a simple case of automation with software, and build from the most basic sort of automation to a full-fledged system. Say you have a bunch of text files in a directory, each containing minutes from meetings that we had together over the years. You can remember that I talked about a particular software package that might solve a problem that you just discovered, but you can't remember the name.","title":"A Spectrum of Automation"},{"location":"lesson/automating_ontology_workflows/#1-ad-hoc","text":"The first thing you try is to just search the directory. On a Mac you would open the Finder, navigate to the directory, and type \"James\" into the search bar. Unfortunately that gives too many results: all the files with the minutes for a meeting where I said something. The next thing to do is double click some text files, which would open them in Text Edit program, and skim them. You might get lucky! You know that I the meeting was in 2019, so you can try and filter for files modified in that year. Unfortunately the files have been updated at different times, so the file dates aren't useful. Now if each file was named with a consistent pattern, including the meeting date, then it would be simple to filter for files with \"2019\" in the name. This isn't automation, but it's the first step in the right direction. Consistent file names are one way to make inputs the same so that you can process them in the same way. Let's say it works: you filter for files from 2019 with \"James\" in them, skim a few, and find a note where I recommended using Pandoc to convert between document formats. Mission accomplished!","title":"1. Ad Hoc"},{"location":"lesson/automating_ontology_workflows/#2-notes","text":"Next week you need to do something very similar: Becky mentioned a website where you can find an important dataset. It's basically the same problem with different inputs. If you remember exactly what you did last time, then you can get the job done quickly. As the job gets more complicated and more distant in time, and as you find yourself doing similar tasks more often, it's nice to have notes about what you did and how you did it. If I'm using a graphical user interface (GUI) then for each step I'll note the program I used, and the menu item or button I clicked, e.g. \"Preferences > General > Font Size\", or \"Search\" or \"Run\". If I'm using a command-line interface (CLI) then I'll copy-paste the commands into my notes. I often keep informal notes like this in a text file in the relevant directory. I name the file \"notes.txt\". A \"README\" file is similar. It's used to describe the contents of a directory, often saying which files are which, or what the column headers for a given table mean. Often the task is more complicated and requires one or more pieces of software that I don't use every day. If there's relevant documentation, I'll put a link to it in my notes, and then a short summmary of exactly what I did. In this example I look in the directory of minutes and see my \"notes.txt\" file. I read that and remember how I filtered on \"2019\" and searched for \"James\". This time I filter on \"2020\" and search for \"Becky\", and I find the website for the dataset quickly enough. As a rule of thumb, it might take you three times longer to find your notes file, write down the steps you took, and provide a short description, than it would to just do the job without taking notes. When you're just taking notes for yourself, this often feels like a waste of time (you'll remember, right?!), and sometimes it is a bit of a waste. If you end up using your notes to help with similar tasks in the future, then this will likely be time well spent. As a rule of thumb, it might take three times longer to write notes for a broader audience than notes for just yourself. This is because you need to take into account the background knowledge of your reader, including her skills and assumptions and context, and especially the possible misunderstandings that you can try to avoid with careful writing. I often start with notes for just myself and then expand them for a wider audience only when needed.","title":"2. Notes"},{"location":"lesson/automating_ontology_workflows/#3-checklist","text":"When tasks get more complicated or more important then informal notes are not enough. The next step on the spectrum of automation is the humble checklist. The most basic checklists are for making sure that each item has been handled. Often the order isn't important, but lists are naturally ordered from top to bottom, and in many cases that order is useful. For example, my mother lays out her shopping lists in the order of the aisles in her local grocery store, making it easier to get each item and check it off without skipping around and perhaps having to backtrack. I think of a checklist as a basic form of automation. It's like a recipe. It should lay out the things you need to start, then proceed through the required steps in enough detail that you can reproduce them. In some sense, by using the checklist you are becoming the \"machine\". You are executing an algorithm that should take you from the expected inputs to the expected output. Humble as the checklist is, there's a reason that astronauts, pilots, and surgical teams live by their checklists. Even when the stakes are not so high, it's often nice to \"put your brain on autopilot\" and just work the checklist without having to remember and reconsider the details of each step. A good checklist is more focused than a file full of notes. A checklist has a goal at the end. It has specific starting conditions. The steps have been carefully considered, so that they have the proper sequence, and none are missing. Perhaps most importantly, a checklist helps you break a complex task down into simple parts. If one of the parts is still too complex, then break it down again into a nested checklist (really a sort of tree structure). Checklists sometimes include another key element of automation: conditionals. A shopping list might say \"if there's a sale on crackers, then buy three boxes\". If-then conditions let our automated systems adapt to circumstances. The \"then\" part is just another step, but the \"if\" part is a little different. It's a test to determine whether a condition holds. We almost always want the result of the test to be a simple True or False. Given a bunch of inputs, some of which pass the test and some of which fail it, we can think of the test as determining some way in which all the things that pass are the same and all the things that fail are the same . Programmers will also be familiar with more complex conditionals such as if-then-else, if-elseif-else, and \"case\", which divide process execution across multiple \"branches\". As a rule of thumb, turning notes into a checklist will likely take at least three times as long as simply writing the notes. If the checklist is for a wider audience, expect it to take three times as long to write, for the same reasons mentioned above for notes. If a task is simple and I can hold all the steps in my head, and I can finish it in one sitting without distractions, then I won't bother with a checklist. But more and more I find myself writing myself a checklist before I begin any non-trivial tasks. I use bullet points in my favourite text editor, or sometimes the Notes app on my iPhone. I lay out the steps in the expected order, and I check them off as I go. Sometimes I start making the checklist days before I need it, so I have lots of time to think about it and improve it. If there's a job that I'm worried about, breaking it down into smaller pieces usually helps to make the job feel more manageable. Actually, I try to start every workday by skimming my (long) To Do list, picking the most important tasks, and making a checklist for what I want to get done by quitting time.","title":"3. Checklist"},{"location":"lesson/automating_ontology_workflows/#3-checkscript","text":"\"Checkscript\" is a word that I think I made up, based on insights from a couple of sources, primarily this blog post on \"Do-nothing scripting: the key to gradual automation\" This is where \"real\" automation kicks in, writing \"real\" code and stuff, but hopefully you'll see that it's just one more step on the spectrum of automation that I'm describing. The notes and checklists we've been discussing are just text in your favourite text editor. A checkscript is a program. It can be written in whatever programming language you prefer. I'll give examples in Posix Shell, but that blog post uses Python, and it really doesn't matter. You start with a checklist (in your mind at least). The first version of your program should just walk you through your checklist. The program should walk you through each step of your checklist, one by one. That's it. Here's a checkscript based on the example above. It just prints the first step ( echo ), waits for you to press any key ( read ), then prints the next step, and so on. ###!/bin/sh echo \"1. Use Finder to filter for files with '2019' in the name\" read -p \"Press enter to continue\" echo \"2. Use finder to search file content for 'James'\" read -p \"Press enter to continue\" echo \"3. Open files in Text Edit and search for 'James'\" read -p \"Press enter to continue\" echo \"Done!\" So far this is just a more annoying way to use a checklist. The magic happens once you break the steps down into small enough pieces and realize that you know how to tell the computer to do some of the steps instead of doing them all yourself. For example, you know that the command-line tool grep is used for searching the contents of files, and that you can use \"fileglob\"s to select just the files that you want to search, and that you can send the output of grep to another file to read in your favourite text editor. Now you know how to automate the first two steps. The computer can just do that work without waiting for you: ###!/bin/sh grep \"James\" *2019* > search_results.txt echo \"1. Open 'search_results.txt' in Text Edit and search for 'James'\" read -p \"Press enter to continue\" echo \"Done!\" Before we were using the Finder, and it is possible to write code to tell the Finder to filter and seach for files. The key advantage of grep here is that we send the search results to another file that we can read now or save for later. This is also a good time to mention the advantage of text files over word processor files. If the minutes were stored in Word files, for example, then Finder could probably search them and you could use Word to read them, but you wouldn't be able to use grep or easily output the results to another file. Unix tools such as grep treat all text files the same, whether they're source code or meeting minutes, which means that these tools work pretty much the same on any text file. By keeping your data in Word you restrict yourself to a much smaller set of tools and make it harder to automate you work with simple scripts like this one. Even if you can't get the computer to run any of the steps for you automatically, a checkscript can still be useful by using variables instead of repeating yourself: ###!/bin/sh FILE_PATTERN=\"*2019*\" FILE_CONTENTS=\"James\" echo \"1. Use Finder to filter for files with '${FILE_PATTERN}' in the name\" read -p \"Press enter to continue\" echo \"2. Use finder to search file content for '${FILE_CONTENTS}'\" read -p \"Press enter to continue\" echo \"3. Open files in Text Edit and search for '${FILE_CONTENTS}'\" read -p \"Press enter to continue\" echo \"Done!\" Now if I want to search for \"Becky\" I can just change the FILE_CONTENTS variable in one place. I find this especially useful for dates and version numbers. This is pretty simple for a checkscript, with very few steps. A more realistic example would be if there were many directories containing the minutes of many meetings, maybe in different file formats and with different naming conventions. In order to be sure that we're searching all of them we might need a longer checkscript. Writing and using a checkscript instead of a checklist will likely take (you guessed it) about three times as long. But the magic of the checkscript is in the title of the blog post I mentioned: \"gradual automation\". Once you have a checkscript, you can run through it all manually, but you can also automate bits a pieces of the task, saving yourself time and effort next time.","title":"3. Checkscript"},{"location":"lesson/automating_ontology_workflows/#5-script","text":"A \"script\" is a kind of program that's easy to edit and run. There are technical distinctions to be made between \"compiled\" programs and \"interpreted\" programs, but they turn out to be more complicated and less helpful than they seem at first. Technically, a checkscript is just a script that waits for you to do the hard parts. In this section I want to talk about \"fully automated\" or \"standalone\" scripts that you just provide some input and execute. Most useful programs are useful because they call other programs (in the right ways). I like shell scripts because they're basically just commands that are copied and pasted from work I was doing on the command-line. It's really easy to call other programs. To continue our example, say that our minutes were stored in Word files. There are Python libraries for this, such as python-docx . You can write a little script using this library that works like grep to search for specified text in selected files, and output the results to a search results file. As you add more and more functionality to a script it can become unwieldy. Scripts work best when they have a simple \"flow\" from beginning to end. They may have some conditionals and some loops, but once you start seeing nested conditionals and loops, then your script is doing too much. There are two main options to consider: break your script into smaller, simpler scripts build a specialized tool: the next step on the spectrum of automation The key difference between a checkscript and a \"standalone\" script is handling problems. A checkscript relies on you to supervise it. A standalone script is expected to work properly without supervision. So the script has to be designed to handle a wider range of inputs and fail gracefully when it gets into trouble. This is a typical case of the \"80% rule\": the last 20% takes 80% of the time. As a rule of thumb, expect it to take three times as long to write a script that can run unsupervised than it takes you to write a checkscript that does \"almost\" the same thing.","title":"5. Script"},{"location":"lesson/automating_ontology_workflows/#6-specialized-tool","text":"When your script needs nested conditionals and loops, then it's probably time to reach for a programming language that's designed to write code \"in the large\". Some languages such as Python can make a pretty smooth transition from a script in a single file to a set of files in a module, working together nicely. You might also choose another language that can provide better performance or efficiency. It's not just the size and the logical complexity of your script, consider its purpose . The specialized tools that I have in mind have a clear purpose that helps guide their design. This also makes them easier to reuse across multiple projects. I often divide my specialized tools into two parts: a library and a command-line interface. The library can be used in other programs, and contains the most distinctive and important functionality. But the command-line interface is essential, because it lets me use my specialized tool in the shell and in scripts, so I can build more automation on top of it. Writing a tool in Java or C++ or Rust usually takes longer than a script in shell or Python because there are more details to worry about such as types and efficient memory management. In return you usually get more reliability and efficiency. But as a rule of thumb, expect it to take three times as long to write a specialized tool than it would to \"just\" write the script. On the other hand, if you already have a script that does most of what you want, and you're already familiar with the target you are moving to, then it can be fairly straightforward to translate from the script to the specialized tool. That's why it's often most efficient to write a prototype script first, do lots of quick experiments to explore the design space, and when you're happy with the design then start on the \"production\" version.","title":"6. Specialized Tool"},{"location":"lesson/automating_ontology_workflows/#7-workflow","text":"The last step in the spectrum of automation is to bring together all your scripts into a single \"workflow\". My favourite tool for this is the venerable Make . A Makefile is essentially a bunch of small scripts with their input and output files carefully specified. When you ask Make to build a given output file, it will look at the whole tree of scripts, figure out which input files are required to build your requested output file, then which files are required to build those files, and so on until it has determined a sequence of steps. Make is also smart enough to check whether some of the dependencies are already up-to-date, and can skip those steps. Looking at a Makefile you can see everything broken down into simple steps and organized into a tree, through which you can trace various paths. You can make changes at any point, and run Make again to update your project. I've done this all so many times that now I often start with a Makefile in an empty directory and build from there. I try experiments on the command line. I make notes. I break the larger task into parts with a checklist. I automate the easy parts first, and leave some parts as manual steps with instructions. I write little scripts in the Makefile . I write larger scripts in the src/ directory. If these get too big or complex, I start thinking about building a specialized tool. (And of course, I store everything in version control.) It takes more time at the beginning, but I think that I usually save time later, because I have a nice place to put everything from the start. In other words, I start thinking about automation at the very beginning of the project, assuming from the start that it will grow, and that I'll need to go back and change things. With a mindset for automation, from the start I'm thinking about how the inputs I care about are the same and different, which similarities I can use for my tests and code, and which differences are important or unimportant.","title":"7. Workflow"},{"location":"lesson/automating_ontology_workflows/#conclusion","text":"In the end, my project isn't ever completely automated. It doesn't \"act of itself\". But by making everything clear and explicit I'm telling the computer how to do a lot of the work and other humans (or just my future self) how to do the rest of it. The final secret of automation, especially when it comes to software and data, is communication : expressing things clearly for humans and machines so they can see and do exactly what you did.","title":"Conclusion"},{"location":"lesson/automating_ontology_workflows/#scientific-computing-an-overview","text":"By: James Overton By \"scientific computing\" we mean using computers to help with key aspect of science such as data collection, cleaning, interpretation, analysis, and visualization. Some people use \"scientific computing\" to mean something more specific, focusing on computational modelling or computationally intensive analysis. We'll be focusing on more general and day-to-day topics: how can a scientist make best use of a computer to do their work well? These three things apply to lots of fields, but are particularly important to scientists: reliability reproducibility communication It should be no surprise that automation can help with all of these. When working properly, computers make fewer mistakes than people, and the mistakes they do make are more predictable. If we're careful, our software systems can be easily reproduced, which means that an entire data analysis pipeline can be copied and run by another lab to confirm the results. And scientific publications are increasingly including data and code as part of the review and final publication process. Clear code is one of the best ways to communicate detailed steps. Automation is critical to scientific instruments and experiments, but we'll focus on the data processing and analysis side: after the data has been generated, how should you deal with it. Basic information management is always important: community standard file formats consistent file naming documentation, READMEs backups version control More advanced data management is part of this course: consistent use of versioned software reference data terminology controlled vocabularies data dictionaries ontologies Some simple rules of thumb can help reduce complexity and confusion: make space firm foundations one-way data flow plan for change test from the start documentation is also for you","title":"Scientific Computing: An Overview"},{"location":"lesson/automating_ontology_workflows/#make-space","text":"When starting a new project, make a nice clean new space for it. Try for that \"new project smell\". I always create a new directory on my computer. I almost always create a new GitHub repository. I usually create a README and a Makefile, right away. It's not always clear when a project is really \"new\" or just a new phase of an old project. But try to clear some space to make a fresh start.","title":"Make Space"},{"location":"lesson/automating_ontology_workflows/#firm-foundations","text":"A lot of data analysis starts with a reference data set. It might be a genome or a proteome. It might be a corpus. It might be a set of papers or data from those papers. Start by finding that data and selecting a particular version of it. Write that down clearly in your notes. If possible, include a unique identifier such as a ( persistent ) URL or DOI. If that's not possible, write down the steps you took. If the data isn't too big, keep a copy of it in your fresh new project directory. If the data is a bit too big, keep a compressed copy in a zip or gz file. A lot of software is perfectly happy to read directly from compressed files, and you can compress or uncompress data using piped commands in your shell or script. If the data is really too big, then be extra careful to keep notes on exactly where you can find it again. Consider storing just the hashes of the big files, so you can confirm that they have exactly the same contents. If you know from the start that you will need to compare your results with someone else's, make sure that you're using the same reference data that they are. This may require a conversation, but trust me that it's better to have this conversation now than later.","title":"Firm Foundations"},{"location":"lesson/automating_ontology_workflows/#one-way-data-flow","text":"It's much easier to think about processes that flow in one direction. Branches are a little trickier, but usually fine. The real trouble comes with loops. Once a process loops back on itself it's much more difficult to reason about what's happening. Loops are powerful, but with great power comes great responsibility. Keep the systems you design as simple as possible (but no simpler). In practical terms: Try not to read then write to the same file. If you have to, try to append rather than overwrite. This is one reason why I prefer tables on disk to databases. Don't hesitate to write intermediate files. These are very useful for testing and debugging. When you're \"finished\" you can comment out these steps.","title":"One-Way Data Flow"},{"location":"lesson/automating_ontology_workflows/#plan-for-change","text":"It's very tempting: you could automate this step, or you could just do it manually. It might take three times as long to automate it, right? So you can save yourself some precious time by just opening Excel and \"fixing\" things by hand. Sometimes that bet will pay off, but I lose that bet most of the time. I tend to realize my mistake only at the last minute. The submission deadline is tomorrow but the core lab \"fixed\" something and they have a new version of the dataset that we need to use for the figures. Now I really don't have time to automate, so I'm up late clicking through Excel again and hoping that I remembered to redo all the changes that I made last time. Automating the process would have actually saved me time, but more importantly it would have avoided a lot of stress. By now I should know that the dataset will almost certainly be revised at the last minute. If I have the automation set up, then I just update the data, run the automation again, and quickly check the results.","title":"Plan for Change"},{"location":"lesson/automating_ontology_workflows/#test-from-the-start","text":"Tests are another thing that take time to implement. One of the key benefits to tests is (again) communication. When assessing or trying out some new piece of software I often look to the test files to see examples of how the code is really used, and the shape of the inputs and outputs. There's a spectrum of tests that apply to different parts of your system: unit tests: individual functions and methods regression tests: ensure that fixed bugs do not reappear integration tests: end-to-end functionality performance tests: system speed and resource usage acceptance tests: whether the overall system meets its design goals Tests should be automated. The test suite should either pass or fail, and if it fails something needs to be fixed before any more development is done. The automated test suite should run before each new version is committed to version control, and ideally more often during development. Tests come with costs: development cost of writing the tests time and resources spent running the tests maintenance costs of updating the tests The first is obvious but the other two often more important. A slow test suite is annoying to run, and so it won't get run. A test suite that's hard to update won't get updated, and then failures will be ignored, which defeats the entire purpose.","title":"Test from the Start"},{"location":"lesson/automating_ontology_workflows/#documentation-is-also-for-you","text":"I tend to forget how bad a memory I have. In the moment, when I'm writing brilliant code nothing could be more obvious than the perfect solution that is pouring forth from my mind all over my keyboard. But when I come back to that code weeks, months, or years later, I often wonder what the heck I was thinking. We think about the documentation we write as being for other people, but for a lot of small projects it's really for your future self. Be kind to your future self. They may be even more tired, even more stressed than you are today. There's a range of different forms of documentation, worth a whole discussion of its own. I like this four-way distinction : tutorials: getting started, basic concepts, an overview how-to guides: how to do common tasks explanation: why does it work this way? reference: looking up the details You don't need all of these for your small project, but consider a brief explanation of why it works the way it does (aimed at a colleague who knows your field well), and some brief notes on how-to do the stuff this project is for. These could both go in the README of a small project.","title":"Documentation is also for You"},{"location":"lesson/automating_ontology_workflows/#additional-materials-and-resources","text":"A whirlwind introduction to the command line Programming with Python Oh My Git!","title":"Additional materials and resources"},{"location":"lesson/automating_ontology_workflows/#contributors","text":"Nico Matentzoglu James Overton","title":"Contributors"},{"location":"lesson/contributing_to_obo_ontologies/","text":"Contributing to OBO ontologies \u00b6 Prerequisites \u00b6 Participants will need to have access to the following resources and tools prior to the training: - GitHub account - register for a free GitHub account here - Protege - Install Protege 5.5, download it here - Install ELK 0.5 - Install GitHub Desktop Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop Preparation \u00b6 Review tutorial on Ontology Term Use What is delivered as part of the course \u00b6 Description: How to contribute terms to existing ontologies. Learning objectives \u00b6 How to use GitHub GitHub workflows Branch vs Fork How to create GitHub Issues Understand basic Open Source etiquette Reading READMEs Understand basics of ontology development workflows Browsing and Searching in Protege Add new terms to an ontology Initial Protege setup Protege editing The Class description view Use GitHub: make pull requests Understand ontology design patterns Use templates: ROBOT, DOSDP ( under development ) Basics of OWL Logic and debugging Tutorials \u00b6 N/A Additional materials and resources \u00b6 Contributors \u00b6 Nicole Vasilevsky Rebecca Jackson Melissa Haendel Chris Mungall David Osumi-Sutherland Matt Yoder Carlo Torniai Simon Jupp Use GitHub \u00b6 GitHub workflows \u00b6 GitHub - distributed version control (Git) + social media for geeks who like to build code/documented collaboratively. A Git repo consists of a set of branches each with a complete history of all changes ever made to the files and directories. This is true for a local copy you check out to your computer from GitHub or for a copy (fork) you make on GitHub. A Git repo typically has a master or main branch that is not directly editing. Changes are made by creating a branch from Master (complete copy of the Master + its history). Branch vs Fork \u00b6 You can copy (fork) any GitHub repo to some other location on GitHub without having to ask permission from the owners. If you modify some files in that repo, e.g. to fix a bug in some code, or a typo in a document, you can then suggest to the owners (via a Pull Request) that they adopt (merge) you your changes back into their repo. If you have permission from the owners, you can instead make a new branch. For this training, we gave you access to the repository. See the Appendix for instructions on how to make a fork. Create GitHub Issues \u00b6 Go to GitHub tracker for the ontology where you'd like to create an issue Select New issue Pick appropriate template (if applicable) Fill in the information that is requested on the template below each header For a new term request, please include: A definition in the proper format Sources/cross references for synonyms Your ORCID Add any additional comments at the end If you use a template, an ontology curator may automatically be assigned. See this example video : creating a new term request to the Mondo Disease Ontology Basic Open Source etiquette \u00b6 Keep in mind that open source ontology repositories on GitHub are public and open to all. Be respectful in your requests and comments. Do not include any private information. GitHub sends notifications to your email, and you can respond via your email client. Keep in mind, the responses are posted publicly. Be sure to delete your email signature that includes any personal information, like your email address or phone number. Many ontologies have limited resources and personnel for development and maintenance. Please be patient with your requests. If your ticket/request has been unanswered for a long period of time, feel free to kindly check in by commenting on the ticket. Including a deadline or priority on the ticket can help the ontology curators with triaging tickets. Reading READMEs \u00b6 A README is a text file that introduces and explains a project. It is intended for everyone , not just the software or ontology developers. Ideally, the README file will include detailed information about the ontology, how to get started with using any of the files, license information and other details. The README is usually on the front page of the GitHub repository. Basics of ontology development workflows \u00b6 Ontology development workflows \u00b6 The steps below describe how to make changes to an ontology. Go to the GitHub repository for your ontology, and clone the repository. The example below describes how to clone the Mondo Disease Ontology repo, but this can be applied to any ontology that is stored in GitHub. Clone the Mondo repo \u00b6 Open the Mondo GitHub repository Click Code Click 'Open with GitHub Desktop' You will be given an option as to where to save the repository. I have a folder called 'git' where I save all of my local repos. This will open GitHub Desktop and the repo should start downloading. This could take some time depending on how big the file is and how much memory your computer has. Create a branch using GitHub Desktop \u00b6 Click the little arrow in Current Branch Click New Branch Give your branch a name: training-initials (ie training-NV ) Open the Ontology edit file in Protege \u00b6 Open Protege Go to: File -> Open Navigate to [ontology-name]/src/ontology/[ontology-name]-edit.obo and open this file in Protege. For example: mondo/src/ontology/mondo-edit.obo Note: all ontologies that use the Ontology Development Kit (ODK) will have the 'edit' files stored in the same folder path: src/ontology/[ontology-name]-edit.owl (or [ontology-name]-edit.obo) Browsing and Searching in Protege \u00b6 The instructions below are using the Mondo Disease Ontology as an example, but this can be applied to any ontology. Open the Mondo in Prot\u00e9g\u00e9 \u00b6 Note: Windows users should open Protege using run.bat Navigate to where you downloaded the repository and open the mondo-edit.obo file (src/ontology/mondo-edit.obo) When you open Protege, you will be on the Active Ontology tab Note the Ontology IRI field. The IRI is used to identify the ontology on the Web. The Prot\u00e9g\u00e9 UI \u00b6 The Prot\u00e9g\u00e9 interface follows a basic paradigm of Tabs and Panels. By default, Prot\u00e9g\u00e9 launches with the main tabs seen below. The layout of tabs and panels is configurable by the user. The Tab list will have slight differences from version to version, and depending on your configuration. It will also reflect your customizations. To customize your view, go to the Window tab on the toolbar and select Views. Here you can customize which panels you see in each tab. In the tabs view, you can select which tabs you will see. You will commonly want to see the Entities tab, which has the Classes tab and the Object Properties tab. Note: if you open a new ontology while viewing your current ontology, Prot\u00e9g\u00e9 will ask you if you'd like to open it in a new window. For most normal usage you should answer no. This will open in a new window. The panel in the center is the ontology annotations panel. This panel contains basic metadata about the ontology, such as the authors, a short description and license information. Running the reasoner \u00b6 Before browsing or searching an ontology, it is useful to run an OWL reasoner first. This ensures that you can view the full, intended classification and allows you to run queries. Navigate to the query menu, and run the ELK reasoner: Entities tab \u00b6 You will see various tabs along the top of the screen. Each tab provides a different perspective on the ontology. For the purposes of this tutorial, we care mostly about the Entities tab, the DL query tab and the search tool. OWL Entities include Classes (which we are focussed on editing in this tutorial), relations (OWL Object Properties) and Annotation Properties (terms like, 'definition' and 'label' which we use to annotate OWL entities. Select the Entities tab and then the Classes sub-tab. Now choose the inferred view (as shown below). The Entities tab is split into two halves. The left-hand side provides a suite of panels for selecting various entities in your ontology. When a particular entity is selected the panels on the right-hand side display information about that entity. The entities panel is context specific, so if you have a class selected (like Thing) then the panels on the right are aimed at editing classes. The panels on the right are customizable. Based on prior use you may see new panes or alternate arrangements. You should see the class OWL:Thing. You could start browsing from here, but the upper level view of the ontology is too abstract for our purposes. To find something more interesting to look at we need to search or query. Searching in Protege \u00b6 You can search for any entity using the search bar on the right: The search window will open on top of your Protege pane, we recommend resizing it and moving it to the side of the main window so you can view together. Here's an example search for 'COVID-19': It shows results found in display names, definitions, synonyms and more. The default results list is truncated. To see full results check the 'Show all results option'. You may need to resize the box to show all results. Double clicking on a result, displays details about it in the entities tab, e.g. In the Entities, tab, you can browse related types, opening/closing branches and clicking on terms to see details on the right. In the default layout, annotations on a term are displayed in the top panel and logical assertions in the 'Description' panel at the bottom. Try to find these specific classes: - 'congenital heart disease' - 'Kindler syndrome' - 'kidney failure' Note - a cool feature in the search tool in Protege is you can search on partial string matching. For example, if you want to search for \u2018down syndrome\u2019, you could search on a partial string: \u2018do synd\u2019. Try searching for \u2018br car and see what kind of results are returned. Question: The search will also search on synonyms. Try searching for \u2018shingles\u2019 and see what results are returned. Were you able to find the term? Note - if the search is slow, you can uncheck the box \u2018Search in annotation values. Try this and search for a term and note if the search is faster. Then search for \u2018shingles\u2019 again and note what results you get. Use GitHub: make pull requests \u00b6 Committing, pushing and making pull requests \u00b6 Changes made to the ontology can be viewed in GitHub Desktop. Before committing, check the diff. Examples of a diff are pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and ask the ontology editors for help instead. Example 1: Commit: Add a meaningful message in the Commit field in the lower left, for example: add new class MONDO:0001012 episodic angioedema with eosinophilia NOTE: You can use the word 'fixes' or 'closes' in the description of the commit message, followed by the corresponding ticket number (in the format #1234) - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages . Note: 'Fixes' and \"Closes' are case-insensitive. If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes' or use 'Adresses'. The commit will be associated with the correct ticket but the ticket will remain open. NOTE: It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed. Click Commit to [branch]. This will save the changes to the cl-edit.owl file. Push: To incorporate the changes into the remote repository, click Publish branch. Add New Terms to an Ontology: \u00b6 The instructions below are using the Mondo Disease Ontology as an example, but this can be applied to any ontology. Setup \u00b6 Setting Preferences for New entities \u00b6 Ontology terms have separate names and IDs. The names are annotation values (labels) and the IDs are represented using IRIs. The OBO foundry has a policy on IRI (or ID) generation ( http://www.obofoundry.org/principles/fp-003-uris.html ). You can set an ID strategy using the \"New Entities\" tab under the Prot\u00e9g\u00e9 Preferences -- on the top toolbar, click the \"Prot\u00e9g\u00e9 dropdown, then click Preferences. Set your new entity preferences precisely as in the following screenshot of the New Entities tab. Note - you have been assigned an ID range in the Mondo idranges file - you should be able to find your own range assigned there. DIY (only if you know what you are doing!) To add your own ID ranges: Go into src/ontology create a branch Find and edit mondo-idranges.owl by adding the following: Datatype: idrange:10 #update this to next following integer from previous Annotations: allocatedto: \"Your Name\" #change to your name EquivalentTo: xsd:integer[>= 0806000 , <= 0806999]. #add a range of 999 above the previous integer create a pull request and add matentzn or nicolevasilevsky as a reviewer proceed to settting up as below: Specified IRI: http://purl.obolibrary.org/obo/ Note - if you edit more than one ontology in Protege, you will need to update your Preferences for each ontology before you edit. Setting Preferences for User details \u00b6 User name: click Use supplied user name and enter your username in the field below Click Use Git user name when available In the ORCID field, add your ORCID ID (in the format 0000-0000-0000-0000) Setting Preferences for New entities metadata \u00b6 You can have Protege add a 'created by' annotation with your ORCID or GitHub username for every new term that you create. Set your preferences to match the screenshot below, in the New entities metadata tab (under preferences). If you do not have an ORCID, register for for free here: https://orcid.org/ Protege editing \u00b6 Creating a new class \u00b6 Before you start: make sure you are working on a branch - see quick guide here . make sure you have the editor's file open in Protege as detailed here . New classes are created in the Class hierarchy panel on the left. There are three buttons at the top of the class hierarchy view. These allow you to add a subclass (L-shaped icon), add a sibling class (c-shaped icon), or delete a selected class (x'd circle). Practice adding a new term: We will work on these two tickets: 1. https://github.com/monarch-initiative/mondo/issues/616 2. https://github.com/monarch-initiative/mondo/issues/2541 https://github.com/monarch-initiative/mondo/issues/616 \u00b6 Search for the parent term 'hypereosinophilic syndrome' (see search guide if you are unsure how to do this). When you are clicked on the term in the Class hierarchy pane, click the add subclass button to add a child class to 'hypereosinophilic syndrome' A dialog will popup. Name this new subclass: migratory muscle precursor. Click \"OK\" to add the class. Adding annotations \u00b6 Using Prot\u00e9g\u00e9 you can add annotations such as labels, definitions, synonyms, database cross references (dbxrefs) to any OWL entity. The panel on the right, named Annotations, is where these annotations are added. CL includes a pre-declared set of annotation properties. The most commonly used annotations are below. rdfs:label definition has_exact_synonym has_broad_synonym has_narrow_synonym has_related synonym database_cross_reference rdfs:comment Note, most of these are bold in the annotation property list: Use this panel to add a definition to the class you created. Select the + button to add an annotation to the selected entity. Click on the annotation 'definition' on the left and copy and paste in the definition to the white editing box on the right. Click OK. Definition: A disorder characterized by episodes of swelling under the skin (angioedema) and an elevated number of the white blood cells known as eosinophils (eosinophilia). During these episodes, symptoms of hives (urticaria), fever, swelling, weight gain and eosinophilia may occur. Symptoms usually appear every 3-4 weeks and resolve on their own within several days. Other cells may be elevated during the episodes, such as neutrophils and lymphocytes. Although the syndrome is often considered a subtype of the idiopathic hypereosinophilic syndromes, it does not typically have organ involvement or lead to other health concerns. Definitions in Mondo should have a 'database cross reference' (dbxref), which is a reference to the definition source, such as a paper from the primary literature or another database. For references to papers, we cross reference the PubMed Identifier in the format, PMID:XXXXXXXX. (Note, no space) To add a dbxref to the definition: Click the @ symbol next to the definition Click the + button next in the pop-up window Scroll up on the left hand side until you find 'database_cross_reference', and click it Add the PMID in the editing box (PMID:25527564). _Note: the PMID should not have any spaces) Click OK Add the additional dbxref: GARD:0013029 The dbxrefs should appear as below. Add Synonyms and Database cross reference \u00b6 Add synonyms Click the add annotations button Add the following synonyms as 'has_exact_synonym': EAE Gleich's syndrome Gleich syndrome All synonyms in Mondo should have a dbxref on the synonym Click the @ symbol next to the synonym Click the + button Add the dbxref to each synonym: GARD:0013029 Add database cross reference Click the add annotations button Add the following database_cross_reference': GARD:0013029 Click the @ symbol next to the synonym Click the + button Add source: MONDO:equivalentTo The Class description view \u00b6 We have seen how to add sub/superclasses and annotate the class hierarchy. Another way to do the same thing is via the Class description view. When an OWL class is selected in the entities view, the right-hand side of the tab shows the class description panel. If we select the 'vertebral column disease' class, we see in the class description view that this class is a \"SubClass Of\" (= has a SuperClass) the 'musculoskeletal system disease' class. Using the (+) button beside \"SubClass Of\" we could add another superclass to the 'skeletal system disease' class. Note the Anonymous Ancestors. These are superclasses that are inherited from the parents. If you hover over the Subclass Of (Anonymous Ancestor) you can see the parent that the class inherited the superclass from. Revising a superclass: \u00b6 If you want to revise the superclass, click the 'o' symbol next to the superclass and replace the text. Try to revise 'musculoskeletal system disease' to 'disease by anatomical system'. If you want to delete a superclass, click the 'x' button next to the superclass. Delete the 'disease by anatomical system' superclass. Close this window without saving. Save your work. Make a Pull Request \u00b6 Click: Create Pull Request in GitHub Desktop This will automatically open GitHub Desktop Click the green button 'Create pull request' You may now add comments to your pull request. The CL editors team will review your PR and either ask for changes or merge it. The changes will be available in the next release. Ontology design patterns \u00b6 Dead Simple Ontology Design Patterns (DOSDPs) are specifications, written in yaml format, that specify how ontology terms should be created (see article here ). They can be used to: - generate documentation - generate new terms - retrofit existing ontology terms DOSDPs have some key features: - Description : that describes the purpose of the patterns - Examples : Provides examples of terms that use the Patterns - Declared classes : these are the classes that are used in the pattern. Any subclass of the declared class can be used in this pattern. - Declared relationships : the relationships used in the logical axioms - vars : the variable classes that are used in the pattern. Any subclass of the 'var' can be used in this pattern. - Pattern for class name, annotations, text definition and equivalentTo (logical definition) : Ontology classes are used as 'fillers' to create new classes that conform to the specific patterns. Examples of design patterns are available here: - uPheno pattern library - Mondo Disease Ontology pattern library Use templates: ROBOT, DOSDP \u00b6 under development Basics of OWL \u00b6 Clone the Ontologies 101 repository, then open the folder BDK14_exercises from your file system Open basic-subclass/chromosome-parts.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Subclass Hierarchy (review) - make sure to look at the \"detailed instructions\" for adding annotations here, as it will go over adding annotations on annotation assertions Disjointness Object Properties - note that you will rarely, if ever, be making object properties, as most of the properties you'll ever need are defined in the Relation Ontology OWL Class Restrictions Open basic-restriction/er-sec-complex.owl in Prot\u00e9g\u00e9, then do the following exercise: Basic Restrictions Open basic-dl-query/cc.owl in Prot\u00e9g\u00e9, then do the following exercises: DL Query Tab - note that owl:Nothing is defined as the very bottom node of an ontology, therefore the DL query results will show owl:Nothing as a subclass. This is expected and does not mean there is a problem with your ontology! It's only bad when something is a subclass of owl:Nothing and therefore unsatisfiable (more on that below). Basic DL Queries Open basic-classification/ubiq-ligase-complex.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Classification Read (I can't get no) satisfiability (~10 minutes) Optional : Open a new ontology in Prot\u00e9g\u00e9. Try creating an inconsistent ontology using the classes and instances in the first Pets example (hint: you'll also need to create the \"eats\" object property)... what happens when you run the reasoner? Logic and debugging \u00b6 Below are exercises to demonstrate how to: - Add equivalent axioms (logical definitions) to ontology terms - Run the reasoner and view the inferred hierarchy - Debugging and viewing explanations Practice adding logic and debugging \u00b6 These instructions will use the Mondo disease ontology as an example. Practice (Part 1) \u00b6 Add New Terms with an Equivalance Axiom to Mondo: \u00b6 Creating a new class \u00b6 New classes are created in the Class hierarchy panel on the left. There are three buttons at the top of the class hierarchy view. These allow you to add a subclass (L-shaped icon), add a sibling class (c-shaped icon), or delete a selected class (x'd circle). Practice adding a new term: \u00b6 Add the new term 'mycotoxin allergy' \u00b6 Navigate to the Mondo repo in GitHub Desktop, create a branch, and open mondo-edit.obo in Protege. Search for the parent term 'allergic disease' (see search guide if you are unsure how to do this). When you are clicked on the term in the Class hierarchy pane, click the add subclass button to add a child class to 'allergic disease' A dialog will popup. Name this new subclass: mycotoxin allergy. Click \"OK\" to add the class. Add annotations and a logical axiom \u00b6 Equivalence axioms in Mondo are added according to Dead Simple Ontology Design Patterns (DOSDPs). You can view all of the design patterns in Mondo by going to code/src/patterns/dosdp-patterns/ For this class, we want to follow the design pattern for allergy . Review this pattern before proceeding. Based on the pattern specifications, add a text definition to this term. Add the database cross reference to this term: MONDO:patterns/allergy Add a synonym that is consistent with this pattern. Add the equivalence axiom according to the pattern specifications. Run the reasoner View the inferred hierarchy. You should see a child of owl:Nothing (and you should see this in the Description pane as well.) Click on the ? button next to the owl:Nothing in the Description view The explanation tells you the reason why this is unsatisfiable Mycotoxin is a child of a 'specifically dependent continuant' and and 'independent continuant' is disjoint with 'specifically dependent continuant'. This logical axiom uses the relation 'realized in response to stimulus' Click on this relation in Protege (you can click on the relation name in the equivalence axiom, or you can click on the Object properties tab and search for this relation. There are domain and range restrictions on this property, where the range has to be a material enity. Chebi mycotoxin is actually a role, so it is not a material entity. Go back to the Classes or Entities pane and remove the equivalence axiom and run the reasoner again. You should now see no unsatisfiable classes. Practice (Part 2) \u00b6 Add a New Term with an Equivalance Axiom to Mondo: \u00b6 Add annotations and logical definition (equivalence axiom) to MONDO:0004549 'cork-handlers' disease' \u00b6 Create a new branch (named cork-handler) and open mondo-edit.obo in Protege. Search for the term cork-handlers' disease For this class, we want to follow the design pattern for specific_infectious_disease_by_agent with a slight modification, this will be modeled after MONDO:0002266 'malt worker's lung' . Add the synonym: cork worker's lung Add the equivalence axiom according to the pattern specifications. 'extrinsic allergic alveolitis' and 'infectious disease' and ('disease has infectious agent' some Penicillium glabrum) Run the reasoner View the inferred hierarchy. Confirm that there are no unsatisifiable classes and everything looks okay. Save your work and create a pull request and reference issue #1639. Add annotations and logical definition (equivalence axiom) to \u00b6 Create a new branch (named farmer-lung) and open mondo-edit.obo in Protege. Search for the term 'farmer's lung disease' Similar to above, we want to add a logical axiom. In this case though, Farmer's lung can be caused by more than one infectious agent, so we'll add this as a subclassOf axiom: 'disease has infectious agent' some Penicillium glabrum Run the reasoner View the inferred hierarchy. Confirm that there are no unsatisifiable classes and everything looks okay. Does the new inferred class make sense? Save your work and create a pull request and reference issue #1639 . Great further reading \u00b6 Chris Mungall on how to write great textual definitions","title":"Contributing to OBO ontologies"},{"location":"lesson/contributing_to_obo_ontologies/#contributing-to-obo-ontologies","text":"","title":"Contributing to OBO ontologies"},{"location":"lesson/contributing_to_obo_ontologies/#prerequisites","text":"Participants will need to have access to the following resources and tools prior to the training: - GitHub account - register for a free GitHub account here - Protege - Install Protege 5.5, download it here - Install ELK 0.5 - Install GitHub Desktop Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop","title":"Prerequisites"},{"location":"lesson/contributing_to_obo_ontologies/#preparation","text":"Review tutorial on Ontology Term Use","title":"Preparation"},{"location":"lesson/contributing_to_obo_ontologies/#what-is-delivered-as-part-of-the-course","text":"Description: How to contribute terms to existing ontologies.","title":"What is delivered as part of the course"},{"location":"lesson/contributing_to_obo_ontologies/#learning-objectives","text":"How to use GitHub GitHub workflows Branch vs Fork How to create GitHub Issues Understand basic Open Source etiquette Reading READMEs Understand basics of ontology development workflows Browsing and Searching in Protege Add new terms to an ontology Initial Protege setup Protege editing The Class description view Use GitHub: make pull requests Understand ontology design patterns Use templates: ROBOT, DOSDP ( under development ) Basics of OWL Logic and debugging","title":"Learning objectives"},{"location":"lesson/contributing_to_obo_ontologies/#tutorials","text":"N/A","title":"Tutorials"},{"location":"lesson/contributing_to_obo_ontologies/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/contributing_to_obo_ontologies/#contributors","text":"Nicole Vasilevsky Rebecca Jackson Melissa Haendel Chris Mungall David Osumi-Sutherland Matt Yoder Carlo Torniai Simon Jupp","title":"Contributors"},{"location":"lesson/contributing_to_obo_ontologies/#use-github","text":"","title":"Use GitHub"},{"location":"lesson/contributing_to_obo_ontologies/#github-workflows","text":"GitHub - distributed version control (Git) + social media for geeks who like to build code/documented collaboratively. A Git repo consists of a set of branches each with a complete history of all changes ever made to the files and directories. This is true for a local copy you check out to your computer from GitHub or for a copy (fork) you make on GitHub. A Git repo typically has a master or main branch that is not directly editing. Changes are made by creating a branch from Master (complete copy of the Master + its history).","title":"GitHub workflows"},{"location":"lesson/contributing_to_obo_ontologies/#branch-vs-fork","text":"You can copy (fork) any GitHub repo to some other location on GitHub without having to ask permission from the owners. If you modify some files in that repo, e.g. to fix a bug in some code, or a typo in a document, you can then suggest to the owners (via a Pull Request) that they adopt (merge) you your changes back into their repo. If you have permission from the owners, you can instead make a new branch. For this training, we gave you access to the repository. See the Appendix for instructions on how to make a fork.","title":"Branch vs Fork"},{"location":"lesson/contributing_to_obo_ontologies/#create-github-issues","text":"Go to GitHub tracker for the ontology where you'd like to create an issue Select New issue Pick appropriate template (if applicable) Fill in the information that is requested on the template below each header For a new term request, please include: A definition in the proper format Sources/cross references for synonyms Your ORCID Add any additional comments at the end If you use a template, an ontology curator may automatically be assigned. See this example video : creating a new term request to the Mondo Disease Ontology","title":"Create GitHub Issues"},{"location":"lesson/contributing_to_obo_ontologies/#basic-open-source-etiquette","text":"Keep in mind that open source ontology repositories on GitHub are public and open to all. Be respectful in your requests and comments. Do not include any private information. GitHub sends notifications to your email, and you can respond via your email client. Keep in mind, the responses are posted publicly. Be sure to delete your email signature that includes any personal information, like your email address or phone number. Many ontologies have limited resources and personnel for development and maintenance. Please be patient with your requests. If your ticket/request has been unanswered for a long period of time, feel free to kindly check in by commenting on the ticket. Including a deadline or priority on the ticket can help the ontology curators with triaging tickets.","title":"Basic Open Source etiquette"},{"location":"lesson/contributing_to_obo_ontologies/#reading-readmes","text":"A README is a text file that introduces and explains a project. It is intended for everyone , not just the software or ontology developers. Ideally, the README file will include detailed information about the ontology, how to get started with using any of the files, license information and other details. The README is usually on the front page of the GitHub repository.","title":"Reading READMEs"},{"location":"lesson/contributing_to_obo_ontologies/#basics-of-ontology-development-workflows","text":"","title":"Basics of ontology development workflows"},{"location":"lesson/contributing_to_obo_ontologies/#ontology-development-workflows","text":"The steps below describe how to make changes to an ontology. Go to the GitHub repository for your ontology, and clone the repository. The example below describes how to clone the Mondo Disease Ontology repo, but this can be applied to any ontology that is stored in GitHub.","title":"Ontology development workflows"},{"location":"lesson/contributing_to_obo_ontologies/#clone-the-mondo-repo","text":"Open the Mondo GitHub repository Click Code Click 'Open with GitHub Desktop' You will be given an option as to where to save the repository. I have a folder called 'git' where I save all of my local repos. This will open GitHub Desktop and the repo should start downloading. This could take some time depending on how big the file is and how much memory your computer has.","title":"Clone the Mondo repo"},{"location":"lesson/contributing_to_obo_ontologies/#create-a-branch-using-github-desktop","text":"Click the little arrow in Current Branch Click New Branch Give your branch a name: training-initials (ie training-NV )","title":"Create a branch using GitHub Desktop"},{"location":"lesson/contributing_to_obo_ontologies/#open-the-ontology-edit-file-in-protege","text":"Open Protege Go to: File -> Open Navigate to [ontology-name]/src/ontology/[ontology-name]-edit.obo and open this file in Protege. For example: mondo/src/ontology/mondo-edit.obo Note: all ontologies that use the Ontology Development Kit (ODK) will have the 'edit' files stored in the same folder path: src/ontology/[ontology-name]-edit.owl (or [ontology-name]-edit.obo)","title":"Open the Ontology edit file in Protege"},{"location":"lesson/contributing_to_obo_ontologies/#browsing-and-searching-in-protege","text":"The instructions below are using the Mondo Disease Ontology as an example, but this can be applied to any ontology.","title":"Browsing and Searching in Protege"},{"location":"lesson/contributing_to_obo_ontologies/#open-the-mondo-in-protege","text":"Note: Windows users should open Protege using run.bat Navigate to where you downloaded the repository and open the mondo-edit.obo file (src/ontology/mondo-edit.obo) When you open Protege, you will be on the Active Ontology tab Note the Ontology IRI field. The IRI is used to identify the ontology on the Web.","title":"Open the Mondo in Prot\u00e9g\u00e9"},{"location":"lesson/contributing_to_obo_ontologies/#the-protege-ui","text":"The Prot\u00e9g\u00e9 interface follows a basic paradigm of Tabs and Panels. By default, Prot\u00e9g\u00e9 launches with the main tabs seen below. The layout of tabs and panels is configurable by the user. The Tab list will have slight differences from version to version, and depending on your configuration. It will also reflect your customizations. To customize your view, go to the Window tab on the toolbar and select Views. Here you can customize which panels you see in each tab. In the tabs view, you can select which tabs you will see. You will commonly want to see the Entities tab, which has the Classes tab and the Object Properties tab. Note: if you open a new ontology while viewing your current ontology, Prot\u00e9g\u00e9 will ask you if you'd like to open it in a new window. For most normal usage you should answer no. This will open in a new window. The panel in the center is the ontology annotations panel. This panel contains basic metadata about the ontology, such as the authors, a short description and license information.","title":"The Prot\u00e9g\u00e9 UI"},{"location":"lesson/contributing_to_obo_ontologies/#running-the-reasoner","text":"Before browsing or searching an ontology, it is useful to run an OWL reasoner first. This ensures that you can view the full, intended classification and allows you to run queries. Navigate to the query menu, and run the ELK reasoner:","title":"Running the reasoner"},{"location":"lesson/contributing_to_obo_ontologies/#entities-tab","text":"You will see various tabs along the top of the screen. Each tab provides a different perspective on the ontology. For the purposes of this tutorial, we care mostly about the Entities tab, the DL query tab and the search tool. OWL Entities include Classes (which we are focussed on editing in this tutorial), relations (OWL Object Properties) and Annotation Properties (terms like, 'definition' and 'label' which we use to annotate OWL entities. Select the Entities tab and then the Classes sub-tab. Now choose the inferred view (as shown below). The Entities tab is split into two halves. The left-hand side provides a suite of panels for selecting various entities in your ontology. When a particular entity is selected the panels on the right-hand side display information about that entity. The entities panel is context specific, so if you have a class selected (like Thing) then the panels on the right are aimed at editing classes. The panels on the right are customizable. Based on prior use you may see new panes or alternate arrangements. You should see the class OWL:Thing. You could start browsing from here, but the upper level view of the ontology is too abstract for our purposes. To find something more interesting to look at we need to search or query.","title":"Entities tab"},{"location":"lesson/contributing_to_obo_ontologies/#searching-in-protege","text":"You can search for any entity using the search bar on the right: The search window will open on top of your Protege pane, we recommend resizing it and moving it to the side of the main window so you can view together. Here's an example search for 'COVID-19': It shows results found in display names, definitions, synonyms and more. The default results list is truncated. To see full results check the 'Show all results option'. You may need to resize the box to show all results. Double clicking on a result, displays details about it in the entities tab, e.g. In the Entities, tab, you can browse related types, opening/closing branches and clicking on terms to see details on the right. In the default layout, annotations on a term are displayed in the top panel and logical assertions in the 'Description' panel at the bottom. Try to find these specific classes: - 'congenital heart disease' - 'Kindler syndrome' - 'kidney failure' Note - a cool feature in the search tool in Protege is you can search on partial string matching. For example, if you want to search for \u2018down syndrome\u2019, you could search on a partial string: \u2018do synd\u2019. Try searching for \u2018br car and see what kind of results are returned. Question: The search will also search on synonyms. Try searching for \u2018shingles\u2019 and see what results are returned. Were you able to find the term? Note - if the search is slow, you can uncheck the box \u2018Search in annotation values. Try this and search for a term and note if the search is faster. Then search for \u2018shingles\u2019 again and note what results you get.","title":"Searching in Protege"},{"location":"lesson/contributing_to_obo_ontologies/#use-github-make-pull-requests","text":"","title":"Use GitHub: make pull requests"},{"location":"lesson/contributing_to_obo_ontologies/#committing-pushing-and-making-pull-requests","text":"Changes made to the ontology can be viewed in GitHub Desktop. Before committing, check the diff. Examples of a diff are pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and ask the ontology editors for help instead. Example 1: Commit: Add a meaningful message in the Commit field in the lower left, for example: add new class MONDO:0001012 episodic angioedema with eosinophilia NOTE: You can use the word 'fixes' or 'closes' in the description of the commit message, followed by the corresponding ticket number (in the format #1234) - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages . Note: 'Fixes' and \"Closes' are case-insensitive. If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes' or use 'Adresses'. The commit will be associated with the correct ticket but the ticket will remain open. NOTE: It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed. Click Commit to [branch]. This will save the changes to the cl-edit.owl file. Push: To incorporate the changes into the remote repository, click Publish branch.","title":"Committing, pushing and making pull requests"},{"location":"lesson/contributing_to_obo_ontologies/#add-new-terms-to-an-ontology","text":"The instructions below are using the Mondo Disease Ontology as an example, but this can be applied to any ontology.","title":"Add New Terms to an Ontology:"},{"location":"lesson/contributing_to_obo_ontologies/#setup","text":"","title":"Setup"},{"location":"lesson/contributing_to_obo_ontologies/#setting-preferences-for-new-entities","text":"Ontology terms have separate names and IDs. The names are annotation values (labels) and the IDs are represented using IRIs. The OBO foundry has a policy on IRI (or ID) generation ( http://www.obofoundry.org/principles/fp-003-uris.html ). You can set an ID strategy using the \"New Entities\" tab under the Prot\u00e9g\u00e9 Preferences -- on the top toolbar, click the \"Prot\u00e9g\u00e9 dropdown, then click Preferences. Set your new entity preferences precisely as in the following screenshot of the New Entities tab. Note - you have been assigned an ID range in the Mondo idranges file - you should be able to find your own range assigned there. DIY (only if you know what you are doing!) To add your own ID ranges: Go into src/ontology create a branch Find and edit mondo-idranges.owl by adding the following: Datatype: idrange:10 #update this to next following integer from previous Annotations: allocatedto: \"Your Name\" #change to your name EquivalentTo: xsd:integer[>= 0806000 , <= 0806999]. #add a range of 999 above the previous integer create a pull request and add matentzn or nicolevasilevsky as a reviewer proceed to settting up as below: Specified IRI: http://purl.obolibrary.org/obo/ Note - if you edit more than one ontology in Protege, you will need to update your Preferences for each ontology before you edit.","title":"Setting Preferences for New entities"},{"location":"lesson/contributing_to_obo_ontologies/#setting-preferences-for-user-details","text":"User name: click Use supplied user name and enter your username in the field below Click Use Git user name when available In the ORCID field, add your ORCID ID (in the format 0000-0000-0000-0000)","title":"Setting Preferences for User details"},{"location":"lesson/contributing_to_obo_ontologies/#setting-preferences-for-new-entities-metadata","text":"You can have Protege add a 'created by' annotation with your ORCID or GitHub username for every new term that you create. Set your preferences to match the screenshot below, in the New entities metadata tab (under preferences). If you do not have an ORCID, register for for free here: https://orcid.org/","title":"Setting Preferences for New entities metadata"},{"location":"lesson/contributing_to_obo_ontologies/#protege-editing","text":"","title":"Protege editing"},{"location":"lesson/contributing_to_obo_ontologies/#creating-a-new-class","text":"Before you start: make sure you are working on a branch - see quick guide here . make sure you have the editor's file open in Protege as detailed here . New classes are created in the Class hierarchy panel on the left. There are three buttons at the top of the class hierarchy view. These allow you to add a subclass (L-shaped icon), add a sibling class (c-shaped icon), or delete a selected class (x'd circle). Practice adding a new term: We will work on these two tickets: 1. https://github.com/monarch-initiative/mondo/issues/616 2. https://github.com/monarch-initiative/mondo/issues/2541","title":"Creating a new class"},{"location":"lesson/contributing_to_obo_ontologies/#httpsgithubcommonarch-initiativemondoissues616","text":"Search for the parent term 'hypereosinophilic syndrome' (see search guide if you are unsure how to do this). When you are clicked on the term in the Class hierarchy pane, click the add subclass button to add a child class to 'hypereosinophilic syndrome' A dialog will popup. Name this new subclass: migratory muscle precursor. Click \"OK\" to add the class.","title":"https://github.com/monarch-initiative/mondo/issues/616"},{"location":"lesson/contributing_to_obo_ontologies/#adding-annotations","text":"Using Prot\u00e9g\u00e9 you can add annotations such as labels, definitions, synonyms, database cross references (dbxrefs) to any OWL entity. The panel on the right, named Annotations, is where these annotations are added. CL includes a pre-declared set of annotation properties. The most commonly used annotations are below. rdfs:label definition has_exact_synonym has_broad_synonym has_narrow_synonym has_related synonym database_cross_reference rdfs:comment Note, most of these are bold in the annotation property list: Use this panel to add a definition to the class you created. Select the + button to add an annotation to the selected entity. Click on the annotation 'definition' on the left and copy and paste in the definition to the white editing box on the right. Click OK. Definition: A disorder characterized by episodes of swelling under the skin (angioedema) and an elevated number of the white blood cells known as eosinophils (eosinophilia). During these episodes, symptoms of hives (urticaria), fever, swelling, weight gain and eosinophilia may occur. Symptoms usually appear every 3-4 weeks and resolve on their own within several days. Other cells may be elevated during the episodes, such as neutrophils and lymphocytes. Although the syndrome is often considered a subtype of the idiopathic hypereosinophilic syndromes, it does not typically have organ involvement or lead to other health concerns. Definitions in Mondo should have a 'database cross reference' (dbxref), which is a reference to the definition source, such as a paper from the primary literature or another database. For references to papers, we cross reference the PubMed Identifier in the format, PMID:XXXXXXXX. (Note, no space) To add a dbxref to the definition: Click the @ symbol next to the definition Click the + button next in the pop-up window Scroll up on the left hand side until you find 'database_cross_reference', and click it Add the PMID in the editing box (PMID:25527564). _Note: the PMID should not have any spaces) Click OK Add the additional dbxref: GARD:0013029 The dbxrefs should appear as below.","title":"Adding annotations"},{"location":"lesson/contributing_to_obo_ontologies/#add-synonyms-and-database-cross-reference","text":"Add synonyms Click the add annotations button Add the following synonyms as 'has_exact_synonym': EAE Gleich's syndrome Gleich syndrome All synonyms in Mondo should have a dbxref on the synonym Click the @ symbol next to the synonym Click the + button Add the dbxref to each synonym: GARD:0013029 Add database cross reference Click the add annotations button Add the following database_cross_reference': GARD:0013029 Click the @ symbol next to the synonym Click the + button Add source: MONDO:equivalentTo","title":"Add Synonyms and Database cross reference"},{"location":"lesson/contributing_to_obo_ontologies/#the-class-description-view","text":"We have seen how to add sub/superclasses and annotate the class hierarchy. Another way to do the same thing is via the Class description view. When an OWL class is selected in the entities view, the right-hand side of the tab shows the class description panel. If we select the 'vertebral column disease' class, we see in the class description view that this class is a \"SubClass Of\" (= has a SuperClass) the 'musculoskeletal system disease' class. Using the (+) button beside \"SubClass Of\" we could add another superclass to the 'skeletal system disease' class. Note the Anonymous Ancestors. These are superclasses that are inherited from the parents. If you hover over the Subclass Of (Anonymous Ancestor) you can see the parent that the class inherited the superclass from.","title":"The Class description view"},{"location":"lesson/contributing_to_obo_ontologies/#revising-a-superclass","text":"If you want to revise the superclass, click the 'o' symbol next to the superclass and replace the text. Try to revise 'musculoskeletal system disease' to 'disease by anatomical system'. If you want to delete a superclass, click the 'x' button next to the superclass. Delete the 'disease by anatomical system' superclass. Close this window without saving. Save your work.","title":"Revising a superclass:"},{"location":"lesson/contributing_to_obo_ontologies/#make-a-pull-request","text":"Click: Create Pull Request in GitHub Desktop This will automatically open GitHub Desktop Click the green button 'Create pull request' You may now add comments to your pull request. The CL editors team will review your PR and either ask for changes or merge it. The changes will be available in the next release.","title":"Make a Pull Request"},{"location":"lesson/contributing_to_obo_ontologies/#ontology-design-patterns","text":"Dead Simple Ontology Design Patterns (DOSDPs) are specifications, written in yaml format, that specify how ontology terms should be created (see article here ). They can be used to: - generate documentation - generate new terms - retrofit existing ontology terms DOSDPs have some key features: - Description : that describes the purpose of the patterns - Examples : Provides examples of terms that use the Patterns - Declared classes : these are the classes that are used in the pattern. Any subclass of the declared class can be used in this pattern. - Declared relationships : the relationships used in the logical axioms - vars : the variable classes that are used in the pattern. Any subclass of the 'var' can be used in this pattern. - Pattern for class name, annotations, text definition and equivalentTo (logical definition) : Ontology classes are used as 'fillers' to create new classes that conform to the specific patterns. Examples of design patterns are available here: - uPheno pattern library - Mondo Disease Ontology pattern library","title":"Ontology design patterns"},{"location":"lesson/contributing_to_obo_ontologies/#use-templates-robot-dosdp","text":"under development","title":"Use templates: ROBOT, DOSDP"},{"location":"lesson/contributing_to_obo_ontologies/#basics-of-owl","text":"Clone the Ontologies 101 repository, then open the folder BDK14_exercises from your file system Open basic-subclass/chromosome-parts.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Subclass Hierarchy (review) - make sure to look at the \"detailed instructions\" for adding annotations here, as it will go over adding annotations on annotation assertions Disjointness Object Properties - note that you will rarely, if ever, be making object properties, as most of the properties you'll ever need are defined in the Relation Ontology OWL Class Restrictions Open basic-restriction/er-sec-complex.owl in Prot\u00e9g\u00e9, then do the following exercise: Basic Restrictions Open basic-dl-query/cc.owl in Prot\u00e9g\u00e9, then do the following exercises: DL Query Tab - note that owl:Nothing is defined as the very bottom node of an ontology, therefore the DL query results will show owl:Nothing as a subclass. This is expected and does not mean there is a problem with your ontology! It's only bad when something is a subclass of owl:Nothing and therefore unsatisfiable (more on that below). Basic DL Queries Open basic-classification/ubiq-ligase-complex.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Classification Read (I can't get no) satisfiability (~10 minutes) Optional : Open a new ontology in Prot\u00e9g\u00e9. Try creating an inconsistent ontology using the classes and instances in the first Pets example (hint: you'll also need to create the \"eats\" object property)... what happens when you run the reasoner?","title":"Basics of OWL"},{"location":"lesson/contributing_to_obo_ontologies/#logic-and-debugging","text":"Below are exercises to demonstrate how to: - Add equivalent axioms (logical definitions) to ontology terms - Run the reasoner and view the inferred hierarchy - Debugging and viewing explanations","title":"Logic and debugging"},{"location":"lesson/contributing_to_obo_ontologies/#practice-adding-logic-and-debugging","text":"These instructions will use the Mondo disease ontology as an example.","title":"Practice adding logic and debugging"},{"location":"lesson/contributing_to_obo_ontologies/#practice-part-1","text":"","title":"Practice (Part 1)"},{"location":"lesson/contributing_to_obo_ontologies/#add-new-terms-with-an-equivalance-axiom-to-mondo","text":"","title":"Add New Terms with an Equivalance Axiom to Mondo:"},{"location":"lesson/contributing_to_obo_ontologies/#creating-a-new-class_1","text":"New classes are created in the Class hierarchy panel on the left. There are three buttons at the top of the class hierarchy view. These allow you to add a subclass (L-shaped icon), add a sibling class (c-shaped icon), or delete a selected class (x'd circle).","title":"Creating a new class"},{"location":"lesson/contributing_to_obo_ontologies/#practice-adding-a-new-term","text":"","title":"Practice adding a new term:"},{"location":"lesson/contributing_to_obo_ontologies/#add-the-new-term-mycotoxin-allergy","text":"Navigate to the Mondo repo in GitHub Desktop, create a branch, and open mondo-edit.obo in Protege. Search for the parent term 'allergic disease' (see search guide if you are unsure how to do this). When you are clicked on the term in the Class hierarchy pane, click the add subclass button to add a child class to 'allergic disease' A dialog will popup. Name this new subclass: mycotoxin allergy. Click \"OK\" to add the class.","title":"Add the new term 'mycotoxin allergy'"},{"location":"lesson/contributing_to_obo_ontologies/#add-annotations-and-a-logical-axiom","text":"Equivalence axioms in Mondo are added according to Dead Simple Ontology Design Patterns (DOSDPs). You can view all of the design patterns in Mondo by going to code/src/patterns/dosdp-patterns/ For this class, we want to follow the design pattern for allergy . Review this pattern before proceeding. Based on the pattern specifications, add a text definition to this term. Add the database cross reference to this term: MONDO:patterns/allergy Add a synonym that is consistent with this pattern. Add the equivalence axiom according to the pattern specifications. Run the reasoner View the inferred hierarchy. You should see a child of owl:Nothing (and you should see this in the Description pane as well.) Click on the ? button next to the owl:Nothing in the Description view The explanation tells you the reason why this is unsatisfiable Mycotoxin is a child of a 'specifically dependent continuant' and and 'independent continuant' is disjoint with 'specifically dependent continuant'. This logical axiom uses the relation 'realized in response to stimulus' Click on this relation in Protege (you can click on the relation name in the equivalence axiom, or you can click on the Object properties tab and search for this relation. There are domain and range restrictions on this property, where the range has to be a material enity. Chebi mycotoxin is actually a role, so it is not a material entity. Go back to the Classes or Entities pane and remove the equivalence axiom and run the reasoner again. You should now see no unsatisfiable classes.","title":"Add annotations and a logical axiom"},{"location":"lesson/contributing_to_obo_ontologies/#practice-part-2","text":"","title":"Practice (Part 2)"},{"location":"lesson/contributing_to_obo_ontologies/#add-a-new-term-with-an-equivalance-axiom-to-mondo","text":"","title":"Add a New Term with an Equivalance Axiom to Mondo:"},{"location":"lesson/contributing_to_obo_ontologies/#add-annotations-and-logical-definition-equivalence-axiom-to-mondo0004549-cork-handlers-disease","text":"Create a new branch (named cork-handler) and open mondo-edit.obo in Protege. Search for the term cork-handlers' disease For this class, we want to follow the design pattern for specific_infectious_disease_by_agent with a slight modification, this will be modeled after MONDO:0002266 'malt worker's lung' . Add the synonym: cork worker's lung Add the equivalence axiom according to the pattern specifications. 'extrinsic allergic alveolitis' and 'infectious disease' and ('disease has infectious agent' some Penicillium glabrum) Run the reasoner View the inferred hierarchy. Confirm that there are no unsatisifiable classes and everything looks okay. Save your work and create a pull request and reference issue #1639.","title":"Add annotations and logical definition (equivalence axiom) to MONDO:0004549 'cork-handlers' disease'"},{"location":"lesson/contributing_to_obo_ontologies/#add-annotations-and-logical-definition-equivalence-axiom-to","text":"Create a new branch (named farmer-lung) and open mondo-edit.obo in Protege. Search for the term 'farmer's lung disease' Similar to above, we want to add a logical axiom. In this case though, Farmer's lung can be caused by more than one infectious agent, so we'll add this as a subclassOf axiom: 'disease has infectious agent' some Penicillium glabrum Run the reasoner View the inferred hierarchy. Confirm that there are no unsatisifiable classes and everything looks okay. Does the new inferred class make sense? Save your work and create a pull request and reference issue #1639 .","title":"Add annotations and logical definition (equivalence axiom) to"},{"location":"lesson/contributing_to_obo_ontologies/#great-further-reading","text":"Chris Mungall on how to write great textual definitions","title":"Great further reading"},{"location":"lesson/developing_an_obo_ontology/","text":"Developing an OBO Reference Ontology \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Review tutorial on Ontology Development Automation Preparation \u00b6 TBD What is delivered as part of the course \u00b6 Description: Leading a new or existing OBO project; maybe reference ontology develoment? Learning objectives \u00b6 detailed knowledge of OBO principles and best practises use OBO Dashboard use OBO Registry use PURL system Tutorials \u00b6 Week 8: Ontology Development \u00b6 First Instructor: Nico Matentzoglu, Becky Jackson Description \u00b6 By the end of this session, you should be able to: Merge ontology modules & imports with robot merge Create a classified version of an ontology with robot reason Add metadata to an ontology with robot annotate Create a simple release workflow using ROBOT commands in a Makefile Create a new ontology with ODK Preparation \u00b6 Please complete the following and then read the section below: - ROBOT Mini-Tutorial, part 2 - Software Carpentry: Automation and Make What is an ontology release? \u00b6 Like software, official OBO Foundry ontologies have versioned releases . This is important because OBO Foundry ontologies are expected to be shared and reused. Since ontologies are bound to change over time as more terms are added and refined, other developers need stable versions to point to so that there are no surprises. OBO Foundry ontologies use GitHub releases to maintain these stable copies of older versions. Generally, OBO Foundry ontologies maintain an \"edit\" version of their file that changes without notice and should not be used by external ontology developers because of this. The edit file is used to create releases on a (hopefully) regular basis. The released version of an OBO Foundry ontology generally a merged and reasoned version of the edit file. This means that all modules and imports are combined into one file, and that file has the inferred class hierarchy actually asserted. It also often has some extra metadata, including a version IRI . OBO Foundry defines the requirements for version IRIs here . The release workflow process should be stable and can be written as a series of steps, e.g.: 1. Update modules from templates 2. Merge ontology modules & the main edit file into one 3. Assert the inferred class hierarchy 4. Add a version IRI & other important metadata This series of steps can be turned into ROBOT commands: 1. robot template 2. robot merge 3. robot reason 4. robot annotate Since we can turn these steps into a series of commands, we can create a Makefile that stores these as \"recipes\" for our ontology release! Review the ROBOT commands we've learned so far (Becky; review; 30 minutes) Week 5: report and query Week 6: convert , extract , and template New: merge , reason , annotate , and diff Chaining ROBOT commands Specifying custom prefixes Introduction to Makefiles & workflows (Becky; review; 30 minutes) Review Software Carpentry course content How can the ROBOT commands be combined to create an ontology release? Practice writing recipes using ROBOT commands Using the ODK to bootstrap a new ontology (Nico; 45 minutes) Introduction to the OBO Foundry Registry (if time) Additional materials and resources \u00b6 Contributors \u00b6 Nico Matentzoglu","title":"Developing an OBO Ontology"},{"location":"lesson/developing_an_obo_ontology/#developing-an-obo-reference-ontology","text":"","title":"Developing an OBO Reference Ontology"},{"location":"lesson/developing_an_obo_ontology/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/developing_an_obo_ontology/#prerequisites","text":"Review tutorial on Ontology Development Automation","title":"Prerequisites"},{"location":"lesson/developing_an_obo_ontology/#preparation","text":"TBD","title":"Preparation"},{"location":"lesson/developing_an_obo_ontology/#what-is-delivered-as-part-of-the-course","text":"Description: Leading a new or existing OBO project; maybe reference ontology develoment?","title":"What is delivered as part of the course"},{"location":"lesson/developing_an_obo_ontology/#learning-objectives","text":"detailed knowledge of OBO principles and best practises use OBO Dashboard use OBO Registry use PURL system","title":"Learning objectives"},{"location":"lesson/developing_an_obo_ontology/#tutorials","text":"","title":"Tutorials"},{"location":"lesson/developing_an_obo_ontology/#week-8-ontology-development","text":"First Instructor: Nico Matentzoglu, Becky Jackson","title":"Week 8: Ontology Development"},{"location":"lesson/developing_an_obo_ontology/#description","text":"By the end of this session, you should be able to: Merge ontology modules & imports with robot merge Create a classified version of an ontology with robot reason Add metadata to an ontology with robot annotate Create a simple release workflow using ROBOT commands in a Makefile Create a new ontology with ODK","title":"Description"},{"location":"lesson/developing_an_obo_ontology/#preparation_1","text":"Please complete the following and then read the section below: - ROBOT Mini-Tutorial, part 2 - Software Carpentry: Automation and Make","title":"Preparation"},{"location":"lesson/developing_an_obo_ontology/#what-is-an-ontology-release","text":"Like software, official OBO Foundry ontologies have versioned releases . This is important because OBO Foundry ontologies are expected to be shared and reused. Since ontologies are bound to change over time as more terms are added and refined, other developers need stable versions to point to so that there are no surprises. OBO Foundry ontologies use GitHub releases to maintain these stable copies of older versions. Generally, OBO Foundry ontologies maintain an \"edit\" version of their file that changes without notice and should not be used by external ontology developers because of this. The edit file is used to create releases on a (hopefully) regular basis. The released version of an OBO Foundry ontology generally a merged and reasoned version of the edit file. This means that all modules and imports are combined into one file, and that file has the inferred class hierarchy actually asserted. It also often has some extra metadata, including a version IRI . OBO Foundry defines the requirements for version IRIs here . The release workflow process should be stable and can be written as a series of steps, e.g.: 1. Update modules from templates 2. Merge ontology modules & the main edit file into one 3. Assert the inferred class hierarchy 4. Add a version IRI & other important metadata This series of steps can be turned into ROBOT commands: 1. robot template 2. robot merge 3. robot reason 4. robot annotate Since we can turn these steps into a series of commands, we can create a Makefile that stores these as \"recipes\" for our ontology release! Review the ROBOT commands we've learned so far (Becky; review; 30 minutes) Week 5: report and query Week 6: convert , extract , and template New: merge , reason , annotate , and diff Chaining ROBOT commands Specifying custom prefixes Introduction to Makefiles & workflows (Becky; review; 30 minutes) Review Software Carpentry course content How can the ROBOT commands be combined to create an ontology release? Practice writing recipes using ROBOT commands Using the ODK to bootstrap a new ontology (Nico; 45 minutes) Introduction to the OBO Foundry Registry (if time)","title":"What is an ontology release?"},{"location":"lesson/developing_an_obo_ontology/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/developing_an_obo_ontology/#contributors","text":"Nico Matentzoglu","title":"Contributors"},{"location":"lesson/developing_application_ontologies/","text":"Developing an Application Ontology \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Review tutorials on Ontology Theory and Ontology Contribution Preparation \u00b6 TBD What is delivered as part of the course \u00b6 Description: Combining ontology subsets for use in a project. Learning objectives \u00b6 manage GitHub manage ontology imports use ROBOT extract: MIREOT, SLME use ROBOT report pruning trees Tutorials \u00b6 in person or video (link videos here as they become available) Additional materials and resources \u00b6 Contributors \u00b6 Nico Matentzoglu","title":"Developing an Application Ontology"},{"location":"lesson/developing_application_ontologies/#developing-an-application-ontology","text":"","title":"Developing an Application Ontology"},{"location":"lesson/developing_application_ontologies/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/developing_application_ontologies/#prerequisites","text":"Review tutorials on Ontology Theory and Ontology Contribution","title":"Prerequisites"},{"location":"lesson/developing_application_ontologies/#preparation","text":"TBD","title":"Preparation"},{"location":"lesson/developing_application_ontologies/#what-is-delivered-as-part-of-the-course","text":"Description: Combining ontology subsets for use in a project.","title":"What is delivered as part of the course"},{"location":"lesson/developing_application_ontologies/#learning-objectives","text":"manage GitHub manage ontology imports use ROBOT extract: MIREOT, SLME use ROBOT report pruning trees","title":"Learning objectives"},{"location":"lesson/developing_application_ontologies/#tutorials","text":"in person or video (link videos here as they become available)","title":"Tutorials"},{"location":"lesson/developing_application_ontologies/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/developing_application_ontologies/#contributors","text":"Nico Matentzoglu","title":"Contributors"},{"location":"lesson/modelling_with_object_properties/","text":"Modeling with Object Properties \u00b6 In this lesson, we will give an intuition of how to work with object properties in OBO ontologies, also referred to as \"relations\". We will cover, in particular, the following subjects: What is the role of object properties in OBO ontologies, and how should we model them? What is the relation ontology (RO), and how do we add object properties to it? Preparation \u00b6 We have worked with the University of Manchester to incorporate the Family History Knowledge Base Tutorial fully into OBO Academy. This is it: OBOAcademy: Family History - Modelling with Object Properties . In contrast to the Pizza tutorial, the Family history tutorial focuses on modelling with individuals. Chapters 4, 5, 8 and 9 are full of object property modelling, and are not only great to get a basic understanding of using them in your ontology, but also give good hints at where OWL and object properties fall short. We refer to the FHKB in the following and expect you to have completed at least chapter 5 before reading on. The Role of Object Properties in the OBO-sphere \u00b6 To remind ourselves, there are three different types of relations in OWL: Data properties (DatatypeProperty) connect your classes and individuals to data values, such as strings or numbers. In OBO, these are the least frequently used kinds of properties, used for example by CIDO and ONS. For some example usage, run the following query in the ontobee OLS endpoint: http://www.ontobee.org/sparql prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix owl: <http://www.w3.org/2002/07/owl#> SELECT distinct * WHERE { GRAPH ?graph_uri { ?dp rdf:type owl:DatatypeProperty . ?sub ?dp ?obj } } Note that many uses of data properties across OBO are a bit questionable, for example, you do never want to attach a modification dates or similar to your classes using data properties, as these fall under OWL semantics . This means that logically, if a superclass has a relation using a DatatypeProperty, then this relation _holds for all subclasses of that class as well. Annotation properties are similar to data properties, but they are outside of OWL semantics , i.e. OWL reasoners and reasoning do not care, in fact ignore, anything related to annotation properties. This makes them suitable for attaching metadata like labels etc to our classes and properties. We sometimes use annotation properties even to describe relationships between classes if we want reasoners to ignore them. The most typical example is IAO:replaced_by, which connects an obsolete term with its replacement. Widely used annotation properties in the OBO-sphere are standardised in the OBO Metadata Ontology (OMO) . The main type of relation we use in OBO Foundry are object properties . Object properties relate two individuals or classes with each other, for example: OWLObjectPropertyAssertion(:part_of, :heart, :cardiovascular_system) In the same way as annotation properties are maintained in OMO (see above), object properties are maintained in the Relation Ontology (RO) . Object properties are of central importance to all ontological modelling in the OBO sphere, and understanding their semantics is critical for any put the most trivial ontologies. We assume the reader to have completed the Family History Tutorial mentioned above . Object property semantics in OBO \u00b6 In our experience, these are the most widely used characteristics we specify about object properties (OP): Sub-property: if an OP is a sub-property of another parent OP, it inherits all its semantic characteristics. Most importantly: if OP1 is a sub-property of OP2, then, if (a)--[OP1]-->(b), we infer that (a)--[OP2]-->(b). Domain: if OP has a domain C, it means that every time (a)--[OP]-->(b), (a) must be a C. For example, ecologically co-occurs with in RO has the domain 'organism or virus or viroid' , which means that whenever anything ecologically co-occurs with something else, it will be inferred to be a 'organism or virus or viroid' . Range: if OP has a range C, it means that every time (a)--[OP]-->(b), (b) must be a C. For example produced by has the domain material entity . Note that in ontologies, ranges are slightly less powerful then domains: If we have a class Moderna Vaccine which is SubClass of 'produced by' some 'Moderna' we get that Moderna Vaccine is a material entity due to the domain constraint, but NOT that Moderna is a material entity due to the range constraint (explanation to this is a bit complicated, sorry). Transitivity: if an OP is transitive, it means that if (a)--[OP]-->(b)--[OP]-->(c), (a)--[OP]-->(c). For example, if the eye is part of the head, which is part of the body, we can infer that the eye must be part of the body. Property chains: Similar to transitive properties, property chains allow us to bridge across multiple properties. The FHKB tutorial above is all about amazing property chains so you should have a deep understanding of these if you followed the tutorial. Other characteristics like functionality and symmetry are used across OBO ontologies, but not nearly to the same extend as the 5 described above. The Relation Ontology (RO) \u00b6 The Relation Ontology serves two main purposes in the OBO world: As a place to standardise object properties. The idea is this: many ontologies are modelling mereological relations, such as partonomies, which requires relationships such as \"part of\" and \"has part\". To ensure that ontologies are interoperable, we need to make sure that all ontologies use the same \"part of\" relationship. Historically this is not always been true, and still is not. At the time of this writing, running: prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix owl: <http://www.w3.org/2002/07/owl#> SELECT distinct ?graph_uri ?s WHERE { GRAPH ?graph_uri { ?s rdf:type owl:ObjectProperty ; rdfs:label \"part of\" . } } On the OntoBee SPARQL endpoint still reveals a number of ontologies using non-standard part-of relations. In our experience, most of these are accidental due to past format conversions, but not all. This problem was much worse before RO came along, and our goal is to unify the representation of key properties like \"part of\" across all OBO ontologies. The OBO Dashboard checks for object properties that are not aligned with RO. As a place to encode and negotiate object property semantics. Object properties (OP) can have domains and ranges, can have characteristics such as functionality and transitivity, see above . Arguing the exact semantics of an OP can be a difficult and lengthy collaborative process, esp. since OP semantics can have a huge impact on ontology reasoning. Detailed RO documentation (modelling patterns and practices) can be found in here . The process of how relationships are added to RO is discussed in the next section. Adding relationships to RO \u00b6 To add a relationship we usually follow the following process. For details, please refer to the RO documentation . Check whether the OP is already in RO . Search for synonyms - often the relationship you are looking exist, but under a different name. If you cant find the exact OP, see whether you can find similar OPs - this may help you also identify suitable parent OPs. Make an RO issue . Take care to not only describe the name of your relationship, but also intended application areas with examples, a good definition , potential parent relationships, domains and ranges. The more detail you provide, the easier it will be for the community to review your request. Make a pull request . This involves the same steps as usual . If you are unsure what annotations need to be added and how to reflect the intended semantics, it may be useful to look at past pull requests . Join our quarterly RO calls and check out the RO documentation .","title":"Modelling with Object Properties"},{"location":"lesson/modelling_with_object_properties/#modeling-with-object-properties","text":"In this lesson, we will give an intuition of how to work with object properties in OBO ontologies, also referred to as \"relations\". We will cover, in particular, the following subjects: What is the role of object properties in OBO ontologies, and how should we model them? What is the relation ontology (RO), and how do we add object properties to it?","title":"Modeling with Object Properties"},{"location":"lesson/modelling_with_object_properties/#preparation","text":"We have worked with the University of Manchester to incorporate the Family History Knowledge Base Tutorial fully into OBO Academy. This is it: OBOAcademy: Family History - Modelling with Object Properties . In contrast to the Pizza tutorial, the Family history tutorial focuses on modelling with individuals. Chapters 4, 5, 8 and 9 are full of object property modelling, and are not only great to get a basic understanding of using them in your ontology, but also give good hints at where OWL and object properties fall short. We refer to the FHKB in the following and expect you to have completed at least chapter 5 before reading on.","title":"Preparation"},{"location":"lesson/modelling_with_object_properties/#the-role-of-object-properties-in-the-obo-sphere","text":"To remind ourselves, there are three different types of relations in OWL: Data properties (DatatypeProperty) connect your classes and individuals to data values, such as strings or numbers. In OBO, these are the least frequently used kinds of properties, used for example by CIDO and ONS. For some example usage, run the following query in the ontobee OLS endpoint: http://www.ontobee.org/sparql prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix owl: <http://www.w3.org/2002/07/owl#> SELECT distinct * WHERE { GRAPH ?graph_uri { ?dp rdf:type owl:DatatypeProperty . ?sub ?dp ?obj } } Note that many uses of data properties across OBO are a bit questionable, for example, you do never want to attach a modification dates or similar to your classes using data properties, as these fall under OWL semantics . This means that logically, if a superclass has a relation using a DatatypeProperty, then this relation _holds for all subclasses of that class as well. Annotation properties are similar to data properties, but they are outside of OWL semantics , i.e. OWL reasoners and reasoning do not care, in fact ignore, anything related to annotation properties. This makes them suitable for attaching metadata like labels etc to our classes and properties. We sometimes use annotation properties even to describe relationships between classes if we want reasoners to ignore them. The most typical example is IAO:replaced_by, which connects an obsolete term with its replacement. Widely used annotation properties in the OBO-sphere are standardised in the OBO Metadata Ontology (OMO) . The main type of relation we use in OBO Foundry are object properties . Object properties relate two individuals or classes with each other, for example: OWLObjectPropertyAssertion(:part_of, :heart, :cardiovascular_system) In the same way as annotation properties are maintained in OMO (see above), object properties are maintained in the Relation Ontology (RO) . Object properties are of central importance to all ontological modelling in the OBO sphere, and understanding their semantics is critical for any put the most trivial ontologies. We assume the reader to have completed the Family History Tutorial mentioned above .","title":"The Role of Object Properties in the OBO-sphere"},{"location":"lesson/modelling_with_object_properties/#object-property-semantics-in-obo","text":"In our experience, these are the most widely used characteristics we specify about object properties (OP): Sub-property: if an OP is a sub-property of another parent OP, it inherits all its semantic characteristics. Most importantly: if OP1 is a sub-property of OP2, then, if (a)--[OP1]-->(b), we infer that (a)--[OP2]-->(b). Domain: if OP has a domain C, it means that every time (a)--[OP]-->(b), (a) must be a C. For example, ecologically co-occurs with in RO has the domain 'organism or virus or viroid' , which means that whenever anything ecologically co-occurs with something else, it will be inferred to be a 'organism or virus or viroid' . Range: if OP has a range C, it means that every time (a)--[OP]-->(b), (b) must be a C. For example produced by has the domain material entity . Note that in ontologies, ranges are slightly less powerful then domains: If we have a class Moderna Vaccine which is SubClass of 'produced by' some 'Moderna' we get that Moderna Vaccine is a material entity due to the domain constraint, but NOT that Moderna is a material entity due to the range constraint (explanation to this is a bit complicated, sorry). Transitivity: if an OP is transitive, it means that if (a)--[OP]-->(b)--[OP]-->(c), (a)--[OP]-->(c). For example, if the eye is part of the head, which is part of the body, we can infer that the eye must be part of the body. Property chains: Similar to transitive properties, property chains allow us to bridge across multiple properties. The FHKB tutorial above is all about amazing property chains so you should have a deep understanding of these if you followed the tutorial. Other characteristics like functionality and symmetry are used across OBO ontologies, but not nearly to the same extend as the 5 described above.","title":"Object property semantics in OBO"},{"location":"lesson/modelling_with_object_properties/#the-relation-ontology-ro","text":"The Relation Ontology serves two main purposes in the OBO world: As a place to standardise object properties. The idea is this: many ontologies are modelling mereological relations, such as partonomies, which requires relationships such as \"part of\" and \"has part\". To ensure that ontologies are interoperable, we need to make sure that all ontologies use the same \"part of\" relationship. Historically this is not always been true, and still is not. At the time of this writing, running: prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix owl: <http://www.w3.org/2002/07/owl#> SELECT distinct ?graph_uri ?s WHERE { GRAPH ?graph_uri { ?s rdf:type owl:ObjectProperty ; rdfs:label \"part of\" . } } On the OntoBee SPARQL endpoint still reveals a number of ontologies using non-standard part-of relations. In our experience, most of these are accidental due to past format conversions, but not all. This problem was much worse before RO came along, and our goal is to unify the representation of key properties like \"part of\" across all OBO ontologies. The OBO Dashboard checks for object properties that are not aligned with RO. As a place to encode and negotiate object property semantics. Object properties (OP) can have domains and ranges, can have characteristics such as functionality and transitivity, see above . Arguing the exact semantics of an OP can be a difficult and lengthy collaborative process, esp. since OP semantics can have a huge impact on ontology reasoning. Detailed RO documentation (modelling patterns and practices) can be found in here . The process of how relationships are added to RO is discussed in the next section.","title":"The Relation Ontology (RO)"},{"location":"lesson/modelling_with_object_properties/#adding-relationships-to-ro","text":"To add a relationship we usually follow the following process. For details, please refer to the RO documentation . Check whether the OP is already in RO . Search for synonyms - often the relationship you are looking exist, but under a different name. If you cant find the exact OP, see whether you can find similar OPs - this may help you also identify suitable parent OPs. Make an RO issue . Take care to not only describe the name of your relationship, but also intended application areas with examples, a good definition , potential parent relationships, domains and ranges. The more detail you provide, the easier it will be for the community to review your request. Make a pull request . This involves the same steps as usual . If you are unsure what annotations need to be added and how to reflect the intended semantics, it may be useful to look at past pull requests . Join our quarterly RO calls and check out the RO documentation .","title":"Adding relationships to RO"},{"location":"lesson/ontology_design/","text":"Ontology Design \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Participants will need to have access to the following resources and tools prior to the training: - GitHub account - register for a free GitHub account here - Protege - Install Protege 5.5, download it here - Install ELK 0.5 - Install GitHub Desktop Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop Preparation \u00b6 Review tutorial on Ontology Term Use Review tutorial on Contributing to OBO Ontologies Clone Mondo repo: Follow these instructions to clone the Mondo repo What is delivered as part of this course \u00b6 Description : This course will cover reasoning with OWL. Learning objectives \u00b6 At the end of this lesson, you should know how to do: Add existential restrictions Add defined classes Add disjoint axioms Debug unsatisfiable classes Tutorials \u00b6 OpenHPI Course Content 1. Ontologies and Logic Videos 3.0-3.10 | Duration: ~3.5 hrs 1. OWL, Rules, and Reasoning Videos 4.0-4.8 | Duration: ~2.7 hrs Additional Materials and Resources \u00b6 Ontology tutorials and Resources Monkeying around with OWL: Musings on building and using ontologies, posts by Chris Mungall Documentation on Cell Ontology relations Guidelines for writing definitions in Ontologies (paper) How to deal with unintentional equivalent classes : Cool post by Chris Mungall on how to deal with an important reasoning issue. Semantic Engineer Toolbox \u00b6 Protege ELK Protege Plugin GitHub Desktop Contributors \u00b6 Nicole Vasilevsky Nico Matentzoglu Acknowledgement \u00b6 Content was adapted from Ontology 101 Tutorial OWL class restrictions \u00b6 In OWL, we use object properties to describe binary relationships between two individuals (or instances). We can also use the properties to describe new classes (or sets of individuals) using restrictions . A restriction describes a class of individuals based on the relationships that members of the class participate in. In other words, a restriction is a kind of class, in the same way that a named class is a kind of class. For example, we can use a named class to capture all the individuals that are idiopathic diseases. But we could also describe the class of idiopathic disease as all the instances that are ' has modifier' idiopathic disease. In OWL, there are three main types of restrictions that can be placed on classes. These are quantifier restriction , cardinality restrictions , and hasValue restriction. In this tutorial will initially focus on quantifier restrictions. Quantifier restrictions are further categorized into two types, the existential and the universal restriction. Existential restrictions describe classes of individuals that participate in at least one relationship along a specified property to individuals that are members of a specified class. For example, the class of individuals that have at least one ( some ) 'has modifier' relationship to members of the idiopathic disease class. In Protege, the keyword 'some' is used to denote existential restrictions. Universal restrictions describe classes of individuals that for a given property only have relationships along this property to individuals that are members of a specified class. For example, we can say a cellular component is capable of many functions using the existential quantifier, however, OWL semantics assume that there could be more. We can use the universal quantifier to add closure to the existential. That is, we can assert that a cellular component is capable of these functions, and is only capable of those functions and no other. Another example is that the process of hair growth is found only in instances of the class Mammalia. In Protege the keyword ' only ' is used. In this tutorial, we will deal exclusively with the existential (some) quantifier. Superclass restrictions \u00b6 Strictly speaking in OWL, you don't make relationships between classes , however, using OWL restrictions we essentially achieve the same thing. We wanted to capture the knowledge that the named class ' idiopathic achalasia ' is an idiopathic disease . In OWL speak, we want to say that every instance of an ' idiopathic achalasia ' is also an instance of the class of things that have at least one 'has modifier' relationship to an idiopathic disease . In OWL, we do this by creating an existential restriction on the idiopathic achalasia class. In the Entities tab, select ' idiopathic achalasia ' in the class hierarchy and look at its current class description in the bottom right box. Note that there are two superclasses (as denoted by the SubClass Of list). ' 'gastroesophageal disease' ' and 'has modifier' some idiopathic . Run the reasoner. You should see that this class is now inferred to be an idiopathic disease because of this SubClassOf (superclass) restriction. Equivalence Axioms and Automatic classification \u00b6 This example introduces equivalence axioms or defined classes (also called logical definitions ) and automatic classification. The example involves classification of Mendelian diseases that have a monogenic (single gene) varation. These equivalence axioms are based off the Mondo Deisgn Pattern disease_series_by_gene . Constructs: - and (intersection) - equivalence (logical definitions) - existential restrictions (e.g. 'disease has basis in dysfunction of') Add an equivalence axiom to an existing Mondo term \u00b6 Create a new branch and open (or re-open) mondo-edit.obo Navigate to the class 'cardioacrofacial dysplasia 1' According to OMIM , this disease is caused by a variation in the gene PRKACA. We want to add an equivalence axiom that says every instance of this class is a type of 'cardioacrofacial dysplasia' that has dysfunction in the PRKACA gene. To do this, click the + next to Equivalent To in the lower right Description box. Add the following equivalence axiom: 'cardioacrofacial dysplasia' and ('disease has basis in dysfunction of' some PRKACA) Run the reasoner. You shouldn't see any change, but try deleting the superclass assertion to 'cardioacrofacial dysplasia' and re-running the reasoner. You should see that 'cardioacrofacial dysplasia' is an inferred superclass. Undo the last change and save your work and commmit and create a pull request. Adding classes and automatically classifying them \u00b6 For teaching purposes, let's say we need a new class that is 'fungal allergy'. Create a branch and re-open mondo-edit.obo Add a new term under owl:Thing named 'fungal allergy'. Following the design pattern allergy.yaml , add the text definition, synonym and equivalentTo axiom, using the substance ECTO_0000524 'exposure to mycotoxin'. Run the reasoner and note where the class is automatically classified. Create a pull request and note in the PR what the parent class is. Debugging automatic classifications \u00b6 On the same branch, add a new term under owl:Thing named 'oral cavity neoplasm'. Following the design pattern neoplasm_by_origin , add the term label and the equivalence axiom. Run the reasoner and note where the term in automatically classified. You should see it is under owl:Nothing. Click the ? next to owl:Nothing in the Description box to see the explanation. Can you determine why this is an unsatisfiable class? Create a pull request and add a comment explaining why this is unsatisfiable. Disjointness \u00b6 By default, OWL assumes that these classes can overlap, i.e. there are individuals who can be instances of more than one of these classes. We want to create a restriction on our ontology that states these classes are different and that no individual can be a member of more than one of these classes. We can say this in OWL by creating a disjoint classes axiom. Create a branch in the Mondo repo (name it: disjoint-[your initials]. For example: disjoint-nv) Open the mondo-edit.obo file Per this ticket , we want to assert that infectious disease and 'syndromic disease' are disjoint. To do this first search for and select the infectious disease class. In the class 'Description' view, scroll down and select the (+) button next to Disjoint With. You are presented with the now familiar window allowing you to select, or type, to choose a class. In the Expression editor, add 'syndromic disease' as disjoint with 'infectious disease'. Run the ELK reasoner. Scroll to the top of your hierarchy and note that owl:Nothing has turned red. This is because there are unsatisfiable classes. Review and fix one unsatisfiable class \u00b6 Below we'll review an example of one class and how to fix it. Next you should review and fix another one on your own and create a pull request for Nicole or Nico to review. Note, fixing these may require a bit of review and subjective decision making and the fix described below may not necessarily apply to each case. Review Bickerstaff brainstem encephalitis : To understand why this class appeared under owl:Nothing, first click the ? next to owl:Nothing in the Description box. (Note, this can take a few minutes). The explanation is displayed above - it is because this class is a descedent of Guillain-Barre syndrome , which is a child of syndromic disease . Next, we have to ask if Bickerstaff brainstem encephalitis is an appropriate child of regional variant of Guillain-Barre syndrome . Note, Mondo integrates several disease terminologies and ontologies, and brought in all the subclass hierarchies from these source ontologies. To see the source of this superclass assertion, click the @ next to the assertion. This source came from Orphanet, see below. Based on the text definition, there does not seem to be any suggestion that this disease is a type of Guillain-Barre syndrome. Assuming that this disease is not a type of Guillain-Barre syndrome, we should exclude the superclass regional variant of Guillain-Barre syndrome (see this paper and this paper . It seems a bit unclear what the relationship of BBE is to Guillain-Barre syndrome. This also brings into the question if a disease can be syndromic and an infectious disease - maybe this disjoint axiom is wrong, but let's not worry about this for the teaching purposes.) To exclude a superclass, follow the instructions here .","title":"Ontology Design"},{"location":"lesson/ontology_design/#ontology-design","text":"","title":"Ontology Design"},{"location":"lesson/ontology_design/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/ontology_design/#prerequisites","text":"Participants will need to have access to the following resources and tools prior to the training: - GitHub account - register for a free GitHub account here - Protege - Install Protege 5.5, download it here - Install ELK 0.5 - Install GitHub Desktop Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop","title":"Prerequisites"},{"location":"lesson/ontology_design/#preparation","text":"Review tutorial on Ontology Term Use Review tutorial on Contributing to OBO Ontologies Clone Mondo repo: Follow these instructions to clone the Mondo repo","title":"Preparation"},{"location":"lesson/ontology_design/#what-is-delivered-as-part-of-this-course","text":"Description : This course will cover reasoning with OWL.","title":"What is delivered as part of this course"},{"location":"lesson/ontology_design/#learning-objectives","text":"At the end of this lesson, you should know how to do: Add existential restrictions Add defined classes Add disjoint axioms Debug unsatisfiable classes","title":"Learning objectives"},{"location":"lesson/ontology_design/#tutorials","text":"OpenHPI Course Content 1. Ontologies and Logic Videos 3.0-3.10 | Duration: ~3.5 hrs 1. OWL, Rules, and Reasoning Videos 4.0-4.8 | Duration: ~2.7 hrs","title":"Tutorials"},{"location":"lesson/ontology_design/#additional-materials-and-resources","text":"Ontology tutorials and Resources Monkeying around with OWL: Musings on building and using ontologies, posts by Chris Mungall Documentation on Cell Ontology relations Guidelines for writing definitions in Ontologies (paper) How to deal with unintentional equivalent classes : Cool post by Chris Mungall on how to deal with an important reasoning issue.","title":"Additional Materials and Resources"},{"location":"lesson/ontology_design/#semantic-engineer-toolbox","text":"Protege ELK Protege Plugin GitHub Desktop","title":"Semantic Engineer Toolbox"},{"location":"lesson/ontology_design/#contributors","text":"Nicole Vasilevsky Nico Matentzoglu","title":"Contributors"},{"location":"lesson/ontology_design/#acknowledgement","text":"Content was adapted from Ontology 101 Tutorial","title":"Acknowledgement"},{"location":"lesson/ontology_design/#owl-class-restrictions","text":"In OWL, we use object properties to describe binary relationships between two individuals (or instances). We can also use the properties to describe new classes (or sets of individuals) using restrictions . A restriction describes a class of individuals based on the relationships that members of the class participate in. In other words, a restriction is a kind of class, in the same way that a named class is a kind of class. For example, we can use a named class to capture all the individuals that are idiopathic diseases. But we could also describe the class of idiopathic disease as all the instances that are ' has modifier' idiopathic disease. In OWL, there are three main types of restrictions that can be placed on classes. These are quantifier restriction , cardinality restrictions , and hasValue restriction. In this tutorial will initially focus on quantifier restrictions. Quantifier restrictions are further categorized into two types, the existential and the universal restriction. Existential restrictions describe classes of individuals that participate in at least one relationship along a specified property to individuals that are members of a specified class. For example, the class of individuals that have at least one ( some ) 'has modifier' relationship to members of the idiopathic disease class. In Protege, the keyword 'some' is used to denote existential restrictions. Universal restrictions describe classes of individuals that for a given property only have relationships along this property to individuals that are members of a specified class. For example, we can say a cellular component is capable of many functions using the existential quantifier, however, OWL semantics assume that there could be more. We can use the universal quantifier to add closure to the existential. That is, we can assert that a cellular component is capable of these functions, and is only capable of those functions and no other. Another example is that the process of hair growth is found only in instances of the class Mammalia. In Protege the keyword ' only ' is used. In this tutorial, we will deal exclusively with the existential (some) quantifier.","title":"OWL class restrictions"},{"location":"lesson/ontology_design/#superclass-restrictions","text":"Strictly speaking in OWL, you don't make relationships between classes , however, using OWL restrictions we essentially achieve the same thing. We wanted to capture the knowledge that the named class ' idiopathic achalasia ' is an idiopathic disease . In OWL speak, we want to say that every instance of an ' idiopathic achalasia ' is also an instance of the class of things that have at least one 'has modifier' relationship to an idiopathic disease . In OWL, we do this by creating an existential restriction on the idiopathic achalasia class. In the Entities tab, select ' idiopathic achalasia ' in the class hierarchy and look at its current class description in the bottom right box. Note that there are two superclasses (as denoted by the SubClass Of list). ' 'gastroesophageal disease' ' and 'has modifier' some idiopathic . Run the reasoner. You should see that this class is now inferred to be an idiopathic disease because of this SubClassOf (superclass) restriction.","title":"Superclass restrictions"},{"location":"lesson/ontology_design/#equivalence-axioms-and-automatic-classification","text":"This example introduces equivalence axioms or defined classes (also called logical definitions ) and automatic classification. The example involves classification of Mendelian diseases that have a monogenic (single gene) varation. These equivalence axioms are based off the Mondo Deisgn Pattern disease_series_by_gene . Constructs: - and (intersection) - equivalence (logical definitions) - existential restrictions (e.g. 'disease has basis in dysfunction of')","title":"Equivalence Axioms and Automatic classification"},{"location":"lesson/ontology_design/#add-an-equivalence-axiom-to-an-existing-mondo-term","text":"Create a new branch and open (or re-open) mondo-edit.obo Navigate to the class 'cardioacrofacial dysplasia 1' According to OMIM , this disease is caused by a variation in the gene PRKACA. We want to add an equivalence axiom that says every instance of this class is a type of 'cardioacrofacial dysplasia' that has dysfunction in the PRKACA gene. To do this, click the + next to Equivalent To in the lower right Description box. Add the following equivalence axiom: 'cardioacrofacial dysplasia' and ('disease has basis in dysfunction of' some PRKACA) Run the reasoner. You shouldn't see any change, but try deleting the superclass assertion to 'cardioacrofacial dysplasia' and re-running the reasoner. You should see that 'cardioacrofacial dysplasia' is an inferred superclass. Undo the last change and save your work and commmit and create a pull request.","title":"Add an equivalence axiom to an existing Mondo term"},{"location":"lesson/ontology_design/#adding-classes-and-automatically-classifying-them","text":"For teaching purposes, let's say we need a new class that is 'fungal allergy'. Create a branch and re-open mondo-edit.obo Add a new term under owl:Thing named 'fungal allergy'. Following the design pattern allergy.yaml , add the text definition, synonym and equivalentTo axiom, using the substance ECTO_0000524 'exposure to mycotoxin'. Run the reasoner and note where the class is automatically classified. Create a pull request and note in the PR what the parent class is.","title":"Adding classes and automatically classifying them"},{"location":"lesson/ontology_design/#debugging-automatic-classifications","text":"On the same branch, add a new term under owl:Thing named 'oral cavity neoplasm'. Following the design pattern neoplasm_by_origin , add the term label and the equivalence axiom. Run the reasoner and note where the term in automatically classified. You should see it is under owl:Nothing. Click the ? next to owl:Nothing in the Description box to see the explanation. Can you determine why this is an unsatisfiable class? Create a pull request and add a comment explaining why this is unsatisfiable.","title":"Debugging automatic classifications"},{"location":"lesson/ontology_design/#disjointness","text":"By default, OWL assumes that these classes can overlap, i.e. there are individuals who can be instances of more than one of these classes. We want to create a restriction on our ontology that states these classes are different and that no individual can be a member of more than one of these classes. We can say this in OWL by creating a disjoint classes axiom. Create a branch in the Mondo repo (name it: disjoint-[your initials]. For example: disjoint-nv) Open the mondo-edit.obo file Per this ticket , we want to assert that infectious disease and 'syndromic disease' are disjoint. To do this first search for and select the infectious disease class. In the class 'Description' view, scroll down and select the (+) button next to Disjoint With. You are presented with the now familiar window allowing you to select, or type, to choose a class. In the Expression editor, add 'syndromic disease' as disjoint with 'infectious disease'. Run the ELK reasoner. Scroll to the top of your hierarchy and note that owl:Nothing has turned red. This is because there are unsatisfiable classes.","title":"Disjointness"},{"location":"lesson/ontology_design/#review-and-fix-one-unsatisfiable-class","text":"Below we'll review an example of one class and how to fix it. Next you should review and fix another one on your own and create a pull request for Nicole or Nico to review. Note, fixing these may require a bit of review and subjective decision making and the fix described below may not necessarily apply to each case. Review Bickerstaff brainstem encephalitis : To understand why this class appeared under owl:Nothing, first click the ? next to owl:Nothing in the Description box. (Note, this can take a few minutes). The explanation is displayed above - it is because this class is a descedent of Guillain-Barre syndrome , which is a child of syndromic disease . Next, we have to ask if Bickerstaff brainstem encephalitis is an appropriate child of regional variant of Guillain-Barre syndrome . Note, Mondo integrates several disease terminologies and ontologies, and brought in all the subclass hierarchies from these source ontologies. To see the source of this superclass assertion, click the @ next to the assertion. This source came from Orphanet, see below. Based on the text definition, there does not seem to be any suggestion that this disease is a type of Guillain-Barre syndrome. Assuming that this disease is not a type of Guillain-Barre syndrome, we should exclude the superclass regional variant of Guillain-Barre syndrome (see this paper and this paper . It seems a bit unclear what the relationship of BBE is to Guillain-Barre syndrome. This also brings into the question if a disease can be syndromic and an infectious disease - maybe this disjoint axiom is wrong, but let's not worry about this for the teaching purposes.) To exclude a superclass, follow the instructions here .","title":"Review and fix one unsatisfiable class"},{"location":"lesson/ontology_development/","text":"Ontology Development: Release Management, Quality Control and Collaborative Methods \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 You have a GitHub account You have set up docker and installed the ODK ( how to ) Preparation \u00b6 You have prepared your ODK set-up Read ODK reference overview What is delivered as part of the course \u00b6 Learning objectives \u00b6 Tutorials \u00b6 Additional materials and resources \u00b6 Contributors \u00b6","title":"Ontology Development"},{"location":"lesson/ontology_development/#ontology-development-release-management-quality-control-and-collaborative-methods","text":"","title":"Ontology Development: Release Management, Quality Control and Collaborative Methods"},{"location":"lesson/ontology_development/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/ontology_development/#prerequisites","text":"You have a GitHub account You have set up docker and installed the ODK ( how to )","title":"Prerequisites"},{"location":"lesson/ontology_development/#preparation","text":"You have prepared your ODK set-up Read ODK reference overview","title":"Preparation"},{"location":"lesson/ontology_development/#what-is-delivered-as-part-of-the-course","text":"","title":"What is delivered as part of the course"},{"location":"lesson/ontology_development/#learning-objectives","text":"","title":"Learning objectives"},{"location":"lesson/ontology_development/#tutorials","text":"","title":"Tutorials"},{"location":"lesson/ontology_development/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/ontology_development/#contributors","text":"","title":"Contributors"},{"location":"lesson/ontology_fundamentals/","text":"Ontologies: Fundamentals \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Install Protege Preparation \u00b6 Complete OpenHPI Week 5: Ontology Engineering videos 5.1, 5.2, and 5.4 - 5.6 (~2.5 hours) We are skipping 5.3: Ontology Learning and both sections on MORE Ontology Evaluation (5.7 and 5.8) Complete part of the Ontologies 101 Tutorial (~2 hours) Clone the Ontologies 101 repository, then open the folder BDK14_exercises from your file system Open basic-subclass/chromosome-parts.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Subclass Hierarchy (review) - make sure to look at the \"detailed instructions\" for adding annotations here, as it will go over adding annotations on annotation assertions Disjointness Object Properties - note that you will rarely, if ever, be making object properties, as most of the properties you'll ever need are defined in the Relation Ontology OWL Class Restrictions Open basic-restriction/er-sec-complex.owl in Prot\u00e9g\u00e9, then do the following exercise: Basic Restrictions Open basic-dl-query/cc.owl in Prot\u00e9g\u00e9, then do the following exercises: DL Query Tab - note that owl:Nothing is defined as the very bottom node of an ontology, therefore the DL query results will show owl:Nothing as a subclass. This is expected and does not mean there is a problem with your ontology! It's only bad when something is a subclass of owl:Nothing and therefore unsatisfiable (more on that below). Basic DL Queries Open basic-classification/ubiq-ligase-complex.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Classification Read (I can't get no) satisfiability (~10 minutes) Optional : Open a new ontology in Prot\u00e9g\u00e9. Try creating an inconsistent ontology using the classes and instances in the first Pets example (hint: you'll also need to create the \"eats\" object property)... what happens when you run the reasoner? Don't spend too much time on this if you get stuck, we'll look at an example of an inconsistent ontology in our session. What is delivered as part of the course \u00b6 Description: Learn the fundamentals of ontologies. Learning objectives \u00b6 OpenHPI course review: questions? (~15 minutes) OWL ontology serializations (\"formats\") (~15 minutes) Converting between serializations with robot convert (Review; ~15 minutes) Creating modules from existing ontologies (~30 minutes) What is a module? How do we use the modules in our ontologies? Extraction methods: MIREOT vs. SLME Creating a module to import with robot extract (Review; ~15 minutes) Ontology design patterns (~15 minutes) Real world example: Ontology for Biomedical Investigations (OBI) Using design patterns in robot template (Review; ~15 minutes) Including your modules in your ontology as imports Tutorials \u00b6 Additional materials and resources \u00b6 Contributors \u00b6 add name/ORCID here","title":"Ontology Fundamentals"},{"location":"lesson/ontology_fundamentals/#ontologies-fundamentals","text":"","title":"Ontologies: Fundamentals"},{"location":"lesson/ontology_fundamentals/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/ontology_fundamentals/#prerequisites","text":"Install Protege","title":"Prerequisites"},{"location":"lesson/ontology_fundamentals/#preparation","text":"Complete OpenHPI Week 5: Ontology Engineering videos 5.1, 5.2, and 5.4 - 5.6 (~2.5 hours) We are skipping 5.3: Ontology Learning and both sections on MORE Ontology Evaluation (5.7 and 5.8) Complete part of the Ontologies 101 Tutorial (~2 hours) Clone the Ontologies 101 repository, then open the folder BDK14_exercises from your file system Open basic-subclass/chromosome-parts.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Subclass Hierarchy (review) - make sure to look at the \"detailed instructions\" for adding annotations here, as it will go over adding annotations on annotation assertions Disjointness Object Properties - note that you will rarely, if ever, be making object properties, as most of the properties you'll ever need are defined in the Relation Ontology OWL Class Restrictions Open basic-restriction/er-sec-complex.owl in Prot\u00e9g\u00e9, then do the following exercise: Basic Restrictions Open basic-dl-query/cc.owl in Prot\u00e9g\u00e9, then do the following exercises: DL Query Tab - note that owl:Nothing is defined as the very bottom node of an ontology, therefore the DL query results will show owl:Nothing as a subclass. This is expected and does not mean there is a problem with your ontology! It's only bad when something is a subclass of owl:Nothing and therefore unsatisfiable (more on that below). Basic DL Queries Open basic-classification/ubiq-ligase-complex.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Classification Read (I can't get no) satisfiability (~10 minutes) Optional : Open a new ontology in Prot\u00e9g\u00e9. Try creating an inconsistent ontology using the classes and instances in the first Pets example (hint: you'll also need to create the \"eats\" object property)... what happens when you run the reasoner? Don't spend too much time on this if you get stuck, we'll look at an example of an inconsistent ontology in our session.","title":"Preparation"},{"location":"lesson/ontology_fundamentals/#what-is-delivered-as-part-of-the-course","text":"Description: Learn the fundamentals of ontologies.","title":"What is delivered as part of the course"},{"location":"lesson/ontology_fundamentals/#learning-objectives","text":"OpenHPI course review: questions? (~15 minutes) OWL ontology serializations (\"formats\") (~15 minutes) Converting between serializations with robot convert (Review; ~15 minutes) Creating modules from existing ontologies (~30 minutes) What is a module? How do we use the modules in our ontologies? Extraction methods: MIREOT vs. SLME Creating a module to import with robot extract (Review; ~15 minutes) Ontology design patterns (~15 minutes) Real world example: Ontology for Biomedical Investigations (OBI) Using design patterns in robot template (Review; ~15 minutes) Including your modules in your ontology as imports","title":"Learning objectives"},{"location":"lesson/ontology_fundamentals/#tutorials","text":"","title":"Tutorials"},{"location":"lesson/ontology_fundamentals/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/ontology_fundamentals/#contributors","text":"add name/ORCID here","title":"Contributors"},{"location":"lesson/ontology_pipelines/","text":"Ontology Pipelines with ROBOT and SPARQL \u00b6 Warning \u00b6 These materials are under construction and may be incomplete. Prerequisites \u00b6 Install ROBOT so you can use it outside of Docker (scroll down to the end of the ROBOT page to find the Windows instructions) Optional Install ODK . The ODK includes ROBOT. In the more advanced parts of the course, you will need the ODK installed for some of the other dependencies it includes, and for Windows users it is often easier to follow the tutorials from inside the docker container rather than the Windows CMD. Familiarise yourself with the ROBOT documentation , to the point that you are aware of the various commands that exist. Tutorials \u00b6 Complete the ROBOT Mini-Tutorial 1 to learn your first ROBOT commands: convert , extract and template Complete the ROBOT Mini-Tutorial 2 to learn about annotate , merge , reason and diff Complete Running Basic SPARQL Queries tutorial (~45 minutes - 1 hour) What is delivered as part of the course \u00b6 Description: There are two basic ways to edit an ontology: (1) manually, using tools such as Protege, or (2) using computational tools such as ROBOT. Both have their advantages and disadvantages: manual curation is often more practical when the required ontology change follows a non-standard pattern, such as adding a textual definition or a synonym, while automated approaches are usually much more scalable (ensure that all axioms in the ontology are consistent, or that imported terms from external ontologies are up-to-date or that all labels start with a lower-case letter). Here, we will do a first dive into the \"computational tools\" side of the edit process. We strongly believe that the modern ontology curator should have a basic set of computational tools in their Semantic Engineering toolbox, and many of the lessons in this course should apply to this role of the modern ontology curator . ROBOT is one of the most important tools in the Semantic Engineering Toolbox. For a bit more background on the tool, please refer to the paper . We also recommend to get a basic familiarity with SPARQL, the query language of the semantic web, that can be a powerful combination with ROBOT to perform changes and quality control checks on your ontology. Additional materials and resources \u00b6 Contributors \u00b6 Becky Jackson Nico Matentzoglu","title":"Ontology Pipelines with ROBOT"},{"location":"lesson/ontology_pipelines/#ontology-pipelines-with-robot-and-sparql","text":"","title":"Ontology Pipelines with ROBOT and SPARQL"},{"location":"lesson/ontology_pipelines/#warning","text":"These materials are under construction and may be incomplete.","title":"Warning"},{"location":"lesson/ontology_pipelines/#prerequisites","text":"Install ROBOT so you can use it outside of Docker (scroll down to the end of the ROBOT page to find the Windows instructions) Optional Install ODK . The ODK includes ROBOT. In the more advanced parts of the course, you will need the ODK installed for some of the other dependencies it includes, and for Windows users it is often easier to follow the tutorials from inside the docker container rather than the Windows CMD. Familiarise yourself with the ROBOT documentation , to the point that you are aware of the various commands that exist.","title":"Prerequisites"},{"location":"lesson/ontology_pipelines/#tutorials","text":"Complete the ROBOT Mini-Tutorial 1 to learn your first ROBOT commands: convert , extract and template Complete the ROBOT Mini-Tutorial 2 to learn about annotate , merge , reason and diff Complete Running Basic SPARQL Queries tutorial (~45 minutes - 1 hour)","title":"Tutorials"},{"location":"lesson/ontology_pipelines/#what-is-delivered-as-part-of-the-course","text":"Description: There are two basic ways to edit an ontology: (1) manually, using tools such as Protege, or (2) using computational tools such as ROBOT. Both have their advantages and disadvantages: manual curation is often more practical when the required ontology change follows a non-standard pattern, such as adding a textual definition or a synonym, while automated approaches are usually much more scalable (ensure that all axioms in the ontology are consistent, or that imported terms from external ontologies are up-to-date or that all labels start with a lower-case letter). Here, we will do a first dive into the \"computational tools\" side of the edit process. We strongly believe that the modern ontology curator should have a basic set of computational tools in their Semantic Engineering toolbox, and many of the lessons in this course should apply to this role of the modern ontology curator . ROBOT is one of the most important tools in the Semantic Engineering Toolbox. For a bit more background on the tool, please refer to the paper . We also recommend to get a basic familiarity with SPARQL, the query language of the semantic web, that can be a powerful combination with ROBOT to perform changes and quality control checks on your ontology.","title":"What is delivered as part of the course"},{"location":"lesson/ontology_pipelines/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/ontology_pipelines/#contributors","text":"Becky Jackson Nico Matentzoglu","title":"Contributors"},{"location":"lesson/ontology_term_use/","text":"Using Ontologies and Ontology Terms \u00b6 Warning \u00b6 These materials are under construction and may be incomplete. Prerequisites \u00b6 Sign up for a free GitHub account Preparation \u00b6 None What is delivered as part of the course \u00b6 Description: Using ontology terms for annotations and structuring data. Learning objectives \u00b6 Explain why ontologies are useful Find good ontologies: ontology repositories, OBO Find terms using ontology browsers Assess ontologies for use: license, quality Map local terminology to ontology terms Identify missing terms Make term requests to existing ontologies Understand the differences between IRIs, CURIEs, and labels Tutorials \u00b6 None Additional materials and resources \u00b6 How select and request terms from ontologies - Blog post by Chris Mungall Guidelines for writing definitions in Ontologies (paper) OntoTips - A guide by Chris Mungall covering various aspects of ontology engineering. Contributors \u00b6 Nicole Vasilevsky 1. Why ontologies are useful \u00b6 Ontologies provide a logical classification of information in a particular domain or subject area. Ontologies can be used for data annotations, for structuring disparate data types, classifying information, for inferencing and reasoning across data and computational analyses. Difference between a terminology and an ontology \u00b6 Terminology \u00b6 A terminology is a collection of terms; a term can have a definition and synonyms. Ontology \u00b6 An ontology contains a formal classification of terminology in a domain that provides textual and machine readable definitions, and defines the relationships between terms. An ontology is a terminology, but a terminology is not (necessarily) an ontology. 2. Finding good ontologies \u00b6 Numerous ontologies exist. Some recommended sources to find community developed, high quality and frequently used ontologies are listed below. OBO Foundry . Read more below The Ontology Lookup Service (OLS) . The OLS contains over 200 ontologies. BioPortal . BioPortal aggregates almost 900 biomedical ontologies, and provides a search interface to look up terms. It is a popular repository for ontologies, but as only a fraction of the ontologies are reviewed by the OBO Foundry, you should carefully review any ontologies found on BioPortal before committing to use them. Ontobee . Ontobee indexes all 200+ OBO Foundry ontologies and is the default browser for OBO: For example, when you click http://purl.obolibrary.org/obo/IAO_0000112, you will be redirected to the a page in the Ontobee browser that describes the annotation property example of usage . 3. Ontology repositories \u00b6 OBO Foundry \u00b6 The OBO Foundry is a community of ontology developers that are committed to developing a library of ontologies that are open, interoperable ontologies, logically well-formed and scientifically accurate. OBO Foundry participants follow and contribute to the development of an evolving set of principles including open use, collaborative development, non-overlapping and strictly-scoped content, and common syntax and relations, based on ontology models that work well, such as the Gene Ontology (GO) . The OBO Foundry is overseen by an Operations Committee with Editorial, Technical and Outreach working groups. Find terms using ontology browsers \u00b6 Various ontology browsers are available, we recommend using one of the ontology browsers listed below. Find terms: Ontology Lookup Service BioPortal Ontobee 4. Assessing ontologies for use \u00b6 Some considerations for determining which ontologies to use include the license and quality of the ontology. License \u00b6 Licenses define how an ontology can legally be used or reused. One requirement for OBO Foundry Ontologies is that they are open, meaning that the ontologies are openly and freely available for use with acknowledgement and without alteration. OBO ontologies are required to be released under a Creative Commons CC-BY license version 3.0 or later , OR released into the public domain under CC0 . The license should be clearly stated in the ontology file. Quality \u00b6 Some criteria that can be applied to determine the quality of an ontology include: Is there an ontology tracker to report issues? All open ontologies should have some form of an issue tracker to report bugs, make new term requests or request other changes to the ontology. Many ontologies use GitHub to track their issues. Is it currently active? Are there a large number of open tickets on the ontology tracker that have not been commented on or otherwise addressed? Are the tickets very old, have been sitting for years? Commmunity involvement On the issue tracker, is there evidence of community involvement, such as issues and comments from outside community members? Scientifically sound Does the ontology accurately represent the domain in a scientifically sound way? How to determine which is the right ontology to use? \u00b6 There are multiple ontologies that exist, start by selecting the appropriate ontology, then search and restrict your search to that ontology. Recommend using ontologies that are open and interoperable. Focusing on OBO foundry ontologies are a good place to start Make informed decision about which ontology to use Maybe the ontology you want to use does not have the term you want, so make a term request to that ontology 5. Mapping local terminology to ontology terms \u00b6 Data can mapped to ontology terms manually, using spreadsheets, or via curation tools such as: Zooma BioPortal Annotator Canto - a web-based literature curation tools Textpresso - designed for C. elegans curation OntoBrowser - an online collaborative curation tool 6. Identifying missing terms \u00b6 The figure below by Chris Mungall on his blog post on How to select and request terms from ontologies describes a workflow on searching for identifying missing terms from an ontology. 7. Making term requests to existing ontologies \u00b6 Making a new term request to Mondo \u00b6 Go to Mondo GitHub tracker : Select New issue Pick appropriate template Fill in the information that is requested on the template below each header Please include: A definition in the proper format Sources/cross references for synonyms Your ORCID or the URL for your ClinGen working group Add any additional comments at the end Nicole will automatically be tagged Please email Nicole or comment on the ticket (Nicole will be emailed) if you have any additional questions or need the ticket is high priority See video Best practices guidelines \u00b6 Note : We appreciate your contributions to extending and improving Mondo. Following best guidelines is appreciated by the curators and developers, and assists them in addressing your issue more quickly. However, we understand if you are not always able to follow these best practices. General Recommendations: \u00b6 New term requests should not match existing terms or synonyms Write a concise definition in the definition field. More info about writing definitions is here Synonyms - please provide a synonym scope and source/cross-reference Check OMIM for children classes (specific to new gene-related terms) Synonym scopes: \u00b6 Exact - an exact match Narrow - more specific term Broad - more general term Related - a word of phrase has been used synonymously with the primary term name in the literature, but the usage is not strictly correct Formatting: \u00b6 Preferred term labels should be lowercase (unless it is a proper name or abbreviation) Write the request below the prompts on the template so the Markdown formatting displays properly Synonyms should be lowercase (with exceptions above) Definition source - if from PubMed, please use the format PMID:XXXXXX (no space) Include the Mondo ID and label for the parent term List the children terms with Mondo ID and label in a bulleted list Examples \u00b6 Tickets that followed best practices: \u00b6 https://github.com/monarch-initiative/mondo/issues/2541 https://github.com/monarch-initiative/mondo/issues/1719 Note: while this ticket generally follows best practices, one thing that can be improved is defining the synonym scope. Generally, when the synonym scope is not explicity mentioned, it is assumed it is an exact synonym . https://github.com/monarch-initiative/mondo/issues/1188 https://github.com/monarch-initiative/mondo/issues/2945 Tickets that did not follow best practices: \u00b6 https://github.com/monarch-initiative/mondo/issues/1837 https://github.com/monarch-initiative/mondo/issues/276 Submitting other issues to Mondo \u00b6 Users may want to request other types of changes to Mondo (or any other ontology) beyond just adding a new term. The Mondo curation team created many issue templates for users, for specific types of requests. If none of the issue templates fit your issue, you can scroll to the bottom and click Open a blank issue 8.Differences between IRIs, CURIEs, and labels \u00b6 URI \u00b6 A uniform resource identifier (URI) is a string of characters used to identify a name or a resource. URL \u00b6 A URL is a URI that, in addition to identifying a network-homed resource, specifies the means of acting upon or obtaining the representation. A URL such as this one: https://github.com/obophenotype/uberon/blob/master/uberon_edit.obo has three main parts: 1. Protocol, e.g. https 2. Host, e.g. github.com 3. Path, e.g. /obophenotype/uberon/blob/master/uberon_edit.obo The protocol tells you how to get the resource. Common protocols for web pages are http (HyperText Transfer Protocol) and https (HTTP Secure). The host is the name of the server to contact (the where), which can be a numeric IP address, but is more often a domain name. The path is the name of the resource on that server (the what), here the Uberon anatomy ontology file. IRI \u00b6 A Internationalized Resource Identifiers (IRI) is an internet protocol standard that allows permitted characters from a wide range of scripts. While URIs are limited to a subset of the ASCII character set, IRIs may contain characters from the Universal Character Set (Unicode/ISO 10646), including Chinese or Japanese kanji, Korean, Cyrillic characters, and so forth. It is defined by RFC 3987 . More information is available here . CURIEs \u00b6 A Compact URI (CURIE) consists of a prefix and a suffix, where the prefix stands in place of a longer base IRI. By converting the prefix and appending the suffix we get back to full IRI. For example, if we define the obo prefix to stand in place of the IRI as: http://purl.obolibrary.org/obo/, then the CURIE obo:UBERON_0002280 can be expanded to http://purl.obolibrary.org/obo/UBERON_0002280, which is the UBERON Anatomy term for \u2018otolith\u2019. Any file that contains CURIEs need to define the prefixes in the file header. Label \u00b6 A label is the textual, human readable name that is given to a term, class property or instance in an ontology.","title":"Ontology Term Use"},{"location":"lesson/ontology_term_use/#using-ontologies-and-ontology-terms","text":"","title":"Using Ontologies and Ontology Terms"},{"location":"lesson/ontology_term_use/#warning","text":"These materials are under construction and may be incomplete.","title":"Warning"},{"location":"lesson/ontology_term_use/#prerequisites","text":"Sign up for a free GitHub account","title":"Prerequisites"},{"location":"lesson/ontology_term_use/#preparation","text":"None","title":"Preparation"},{"location":"lesson/ontology_term_use/#what-is-delivered-as-part-of-the-course","text":"Description: Using ontology terms for annotations and structuring data.","title":"What is delivered as part of the course"},{"location":"lesson/ontology_term_use/#learning-objectives","text":"Explain why ontologies are useful Find good ontologies: ontology repositories, OBO Find terms using ontology browsers Assess ontologies for use: license, quality Map local terminology to ontology terms Identify missing terms Make term requests to existing ontologies Understand the differences between IRIs, CURIEs, and labels","title":"Learning objectives"},{"location":"lesson/ontology_term_use/#tutorials","text":"None","title":"Tutorials"},{"location":"lesson/ontology_term_use/#additional-materials-and-resources","text":"How select and request terms from ontologies - Blog post by Chris Mungall Guidelines for writing definitions in Ontologies (paper) OntoTips - A guide by Chris Mungall covering various aspects of ontology engineering.","title":"Additional materials and resources"},{"location":"lesson/ontology_term_use/#contributors","text":"Nicole Vasilevsky","title":"Contributors"},{"location":"lesson/ontology_term_use/#1-why-ontologies-are-useful","text":"Ontologies provide a logical classification of information in a particular domain or subject area. Ontologies can be used for data annotations, for structuring disparate data types, classifying information, for inferencing and reasoning across data and computational analyses.","title":"1. Why ontologies are useful"},{"location":"lesson/ontology_term_use/#difference-between-a-terminology-and-an-ontology","text":"","title":"Difference between a terminology and an ontology"},{"location":"lesson/ontology_term_use/#terminology","text":"A terminology is a collection of terms; a term can have a definition and synonyms.","title":"Terminology"},{"location":"lesson/ontology_term_use/#ontology","text":"An ontology contains a formal classification of terminology in a domain that provides textual and machine readable definitions, and defines the relationships between terms. An ontology is a terminology, but a terminology is not (necessarily) an ontology.","title":"Ontology"},{"location":"lesson/ontology_term_use/#2-finding-good-ontologies","text":"Numerous ontologies exist. Some recommended sources to find community developed, high quality and frequently used ontologies are listed below. OBO Foundry . Read more below The Ontology Lookup Service (OLS) . The OLS contains over 200 ontologies. BioPortal . BioPortal aggregates almost 900 biomedical ontologies, and provides a search interface to look up terms. It is a popular repository for ontologies, but as only a fraction of the ontologies are reviewed by the OBO Foundry, you should carefully review any ontologies found on BioPortal before committing to use them. Ontobee . Ontobee indexes all 200+ OBO Foundry ontologies and is the default browser for OBO: For example, when you click http://purl.obolibrary.org/obo/IAO_0000112, you will be redirected to the a page in the Ontobee browser that describes the annotation property example of usage .","title":"2. Finding good ontologies"},{"location":"lesson/ontology_term_use/#3-ontology-repositories","text":"","title":"3. Ontology repositories"},{"location":"lesson/ontology_term_use/#obo-foundry","text":"The OBO Foundry is a community of ontology developers that are committed to developing a library of ontologies that are open, interoperable ontologies, logically well-formed and scientifically accurate. OBO Foundry participants follow and contribute to the development of an evolving set of principles including open use, collaborative development, non-overlapping and strictly-scoped content, and common syntax and relations, based on ontology models that work well, such as the Gene Ontology (GO) . The OBO Foundry is overseen by an Operations Committee with Editorial, Technical and Outreach working groups.","title":"OBO Foundry"},{"location":"lesson/ontology_term_use/#find-terms-using-ontology-browsers","text":"Various ontology browsers are available, we recommend using one of the ontology browsers listed below. Find terms: Ontology Lookup Service BioPortal Ontobee","title":"Find terms using ontology browsers"},{"location":"lesson/ontology_term_use/#4-assessing-ontologies-for-use","text":"Some considerations for determining which ontologies to use include the license and quality of the ontology.","title":"4. Assessing ontologies for use"},{"location":"lesson/ontology_term_use/#license","text":"Licenses define how an ontology can legally be used or reused. One requirement for OBO Foundry Ontologies is that they are open, meaning that the ontologies are openly and freely available for use with acknowledgement and without alteration. OBO ontologies are required to be released under a Creative Commons CC-BY license version 3.0 or later , OR released into the public domain under CC0 . The license should be clearly stated in the ontology file.","title":"License"},{"location":"lesson/ontology_term_use/#quality","text":"Some criteria that can be applied to determine the quality of an ontology include: Is there an ontology tracker to report issues? All open ontologies should have some form of an issue tracker to report bugs, make new term requests or request other changes to the ontology. Many ontologies use GitHub to track their issues. Is it currently active? Are there a large number of open tickets on the ontology tracker that have not been commented on or otherwise addressed? Are the tickets very old, have been sitting for years? Commmunity involvement On the issue tracker, is there evidence of community involvement, such as issues and comments from outside community members? Scientifically sound Does the ontology accurately represent the domain in a scientifically sound way?","title":"Quality"},{"location":"lesson/ontology_term_use/#how-to-determine-which-is-the-right-ontology-to-use","text":"There are multiple ontologies that exist, start by selecting the appropriate ontology, then search and restrict your search to that ontology. Recommend using ontologies that are open and interoperable. Focusing on OBO foundry ontologies are a good place to start Make informed decision about which ontology to use Maybe the ontology you want to use does not have the term you want, so make a term request to that ontology","title":"How to determine which is the right ontology to use?"},{"location":"lesson/ontology_term_use/#5-mapping-local-terminology-to-ontology-terms","text":"Data can mapped to ontology terms manually, using spreadsheets, or via curation tools such as: Zooma BioPortal Annotator Canto - a web-based literature curation tools Textpresso - designed for C. elegans curation OntoBrowser - an online collaborative curation tool","title":"5. Mapping local terminology to ontology terms"},{"location":"lesson/ontology_term_use/#6-identifying-missing-terms","text":"The figure below by Chris Mungall on his blog post on How to select and request terms from ontologies describes a workflow on searching for identifying missing terms from an ontology.","title":"6. Identifying missing terms"},{"location":"lesson/ontology_term_use/#7-making-term-requests-to-existing-ontologies","text":"","title":"7. Making term requests to existing ontologies"},{"location":"lesson/ontology_term_use/#making-a-new-term-request-to-mondo","text":"Go to Mondo GitHub tracker : Select New issue Pick appropriate template Fill in the information that is requested on the template below each header Please include: A definition in the proper format Sources/cross references for synonyms Your ORCID or the URL for your ClinGen working group Add any additional comments at the end Nicole will automatically be tagged Please email Nicole or comment on the ticket (Nicole will be emailed) if you have any additional questions or need the ticket is high priority See video","title":"Making a new term request to Mondo"},{"location":"lesson/ontology_term_use/#best-practices-guidelines","text":"Note : We appreciate your contributions to extending and improving Mondo. Following best guidelines is appreciated by the curators and developers, and assists them in addressing your issue more quickly. However, we understand if you are not always able to follow these best practices.","title":"Best practices guidelines"},{"location":"lesson/ontology_term_use/#general-recommendations","text":"New term requests should not match existing terms or synonyms Write a concise definition in the definition field. More info about writing definitions is here Synonyms - please provide a synonym scope and source/cross-reference Check OMIM for children classes (specific to new gene-related terms)","title":"General Recommendations:"},{"location":"lesson/ontology_term_use/#synonym-scopes","text":"Exact - an exact match Narrow - more specific term Broad - more general term Related - a word of phrase has been used synonymously with the primary term name in the literature, but the usage is not strictly correct","title":"Synonym scopes:"},{"location":"lesson/ontology_term_use/#formatting","text":"Preferred term labels should be lowercase (unless it is a proper name or abbreviation) Write the request below the prompts on the template so the Markdown formatting displays properly Synonyms should be lowercase (with exceptions above) Definition source - if from PubMed, please use the format PMID:XXXXXX (no space) Include the Mondo ID and label for the parent term List the children terms with Mondo ID and label in a bulleted list","title":"Formatting:"},{"location":"lesson/ontology_term_use/#examples","text":"","title":"Examples"},{"location":"lesson/ontology_term_use/#tickets-that-followed-best-practices","text":"https://github.com/monarch-initiative/mondo/issues/2541 https://github.com/monarch-initiative/mondo/issues/1719 Note: while this ticket generally follows best practices, one thing that can be improved is defining the synonym scope. Generally, when the synonym scope is not explicity mentioned, it is assumed it is an exact synonym . https://github.com/monarch-initiative/mondo/issues/1188 https://github.com/monarch-initiative/mondo/issues/2945","title":"Tickets that followed best practices:"},{"location":"lesson/ontology_term_use/#tickets-that-did-not-follow-best-practices","text":"https://github.com/monarch-initiative/mondo/issues/1837 https://github.com/monarch-initiative/mondo/issues/276","title":"Tickets that did not follow best practices:"},{"location":"lesson/ontology_term_use/#submitting-other-issues-to-mondo","text":"Users may want to request other types of changes to Mondo (or any other ontology) beyond just adding a new term. The Mondo curation team created many issue templates for users, for specific types of requests. If none of the issue templates fit your issue, you can scroll to the bottom and click Open a blank issue","title":"Submitting other issues to Mondo"},{"location":"lesson/ontology_term_use/#8differences-between-iris-curies-and-labels","text":"","title":"8.Differences between IRIs, CURIEs, and labels"},{"location":"lesson/ontology_term_use/#uri","text":"A uniform resource identifier (URI) is a string of characters used to identify a name or a resource.","title":"URI"},{"location":"lesson/ontology_term_use/#url","text":"A URL is a URI that, in addition to identifying a network-homed resource, specifies the means of acting upon or obtaining the representation. A URL such as this one: https://github.com/obophenotype/uberon/blob/master/uberon_edit.obo has three main parts: 1. Protocol, e.g. https 2. Host, e.g. github.com 3. Path, e.g. /obophenotype/uberon/blob/master/uberon_edit.obo The protocol tells you how to get the resource. Common protocols for web pages are http (HyperText Transfer Protocol) and https (HTTP Secure). The host is the name of the server to contact (the where), which can be a numeric IP address, but is more often a domain name. The path is the name of the resource on that server (the what), here the Uberon anatomy ontology file.","title":"URL"},{"location":"lesson/ontology_term_use/#iri","text":"A Internationalized Resource Identifiers (IRI) is an internet protocol standard that allows permitted characters from a wide range of scripts. While URIs are limited to a subset of the ASCII character set, IRIs may contain characters from the Universal Character Set (Unicode/ISO 10646), including Chinese or Japanese kanji, Korean, Cyrillic characters, and so forth. It is defined by RFC 3987 . More information is available here .","title":"IRI"},{"location":"lesson/ontology_term_use/#curies","text":"A Compact URI (CURIE) consists of a prefix and a suffix, where the prefix stands in place of a longer base IRI. By converting the prefix and appending the suffix we get back to full IRI. For example, if we define the obo prefix to stand in place of the IRI as: http://purl.obolibrary.org/obo/, then the CURIE obo:UBERON_0002280 can be expanded to http://purl.obolibrary.org/obo/UBERON_0002280, which is the UBERON Anatomy term for \u2018otolith\u2019. Any file that contains CURIEs need to define the prefixes in the file header.","title":"CURIEs"},{"location":"lesson/ontology_term_use/#label","text":"A label is the textual, human readable name that is given to a term, class property or instance in an ontology.","title":"Label"},{"location":"lesson/rdf/","text":"Introduction to RDF \u00b6 First Instructor: James Overton Second Instructor: Becky Jackson Warning \u00b6 These materials are under construction and incomplete. Description \u00b6 Modelling and querying data with RDF triples, and working with RDF using tables Topics \u00b6 RDF modelling RDFS SPARQL OpenHPI Course Content \u00b6 OpenHPI Linked Data Engineering (2016) Lesson 2 RDF Lesson 4 SPARQL Software Carpentry Lessons \u00b6 Using Databases and SQL New Material \u00b6 Tables and Triples Optional material \u00b6 Linking data Semantic Engineer Toolbox \u00b6","title":"RDF Fundamentals"},{"location":"lesson/rdf/#introduction-to-rdf","text":"First Instructor: James Overton Second Instructor: Becky Jackson","title":"Introduction to RDF"},{"location":"lesson/rdf/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/rdf/#description","text":"Modelling and querying data with RDF triples, and working with RDF using tables","title":"Description"},{"location":"lesson/rdf/#topics","text":"RDF modelling RDFS SPARQL","title":"Topics"},{"location":"lesson/rdf/#openhpi-course-content","text":"OpenHPI Linked Data Engineering (2016) Lesson 2 RDF Lesson 4 SPARQL","title":"OpenHPI Course Content"},{"location":"lesson/rdf/#software-carpentry-lessons","text":"Using Databases and SQL","title":"Software Carpentry Lessons"},{"location":"lesson/rdf/#new-material","text":"Tables and Triples","title":"New Material"},{"location":"lesson/rdf/#optional-material","text":"Linking data","title":"Optional material"},{"location":"lesson/rdf/#semantic-engineer-toolbox","text":"","title":"Semantic Engineer Toolbox"},{"location":"lesson/semantic_database_fundamentals/","text":"Semantic Databases: Fundamentals \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Review tutorial on Application Ontology Development Preparation \u00b6 TBD What is delivered as part of the course \u00b6 Description: Using ontology terms in a database. Learning objectives \u00b6 advanced term mapping ontology terms in SQL terminology table JOINs, constraints convert tables to triples triplestores knowledge graphs Tutorials \u00b6 in person or video (link videos here as they become available) Additional materials and resources \u00b6 Contributors \u00b6 Nico Matentzoglu","title":"Semantic Database Fundamentals"},{"location":"lesson/semantic_database_fundamentals/#semantic-databases-fundamentals","text":"","title":"Semantic Databases: Fundamentals"},{"location":"lesson/semantic_database_fundamentals/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/semantic_database_fundamentals/#prerequisites","text":"Review tutorial on Application Ontology Development","title":"Prerequisites"},{"location":"lesson/semantic_database_fundamentals/#preparation","text":"TBD","title":"Preparation"},{"location":"lesson/semantic_database_fundamentals/#what-is-delivered-as-part-of-the-course","text":"Description: Using ontology terms in a database.","title":"What is delivered as part of the course"},{"location":"lesson/semantic_database_fundamentals/#learning-objectives","text":"advanced term mapping ontology terms in SQL terminology table JOINs, constraints convert tables to triples triplestores knowledge graphs","title":"Learning objectives"},{"location":"lesson/semantic_database_fundamentals/#tutorials","text":"in person or video (link videos here as they become available)","title":"Tutorials"},{"location":"lesson/semantic_database_fundamentals/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/semantic_database_fundamentals/#contributors","text":"Nico Matentzoglu","title":"Contributors"},{"location":"lesson/templates_for_obo/","text":"Templating systems for OBO ontologies: a deep dive \u00b6 Ontologies are notoriously hard to edit. This makes it a very high burden to edit ontologies for anyone but a select few. However, many of the contents of ontologies are actually best edited by domain experts with often little or known ontological training - editing labels and synonyms, curating definitions, adding references to publications and many more. Furthermore, if we simply remove the burden of writing OWL axioms, editors with very little ontology training can actually curate even logical content: for example, if we want to describe that a class is restricted to a certain taxon (also known as taxon-restriction), the editor is often capable to select the appropriate taxon for a term (say, a \"mouse heart\" is restricted to the taxon of Mus musculus ), but maybe they would not know how to \"add that restriction to the ontology\". Tables are great (for a deep dive into tables and triples see here ). Scientists in particular love tables , and, even more importantly, can be trained easily to edit data in spreadsheet tools, such as Google Sheets or Microsoft Excel. Ontology templating systems, such as DOSDP templates , ROBOT templates and Reasonable Ontology Templates (OTTR) allow separating the raw data in the ontology (labels, synonyms, related ontological entities, descriptions, cross-references and other metadata) from the OWL language patterns that are used to manifest them in the ontology. There are three main ingredients to a templating system: A way to capture the data. In all the systems we care about, these are tables, usually manifested as spreadsheets in Excel or Google Sheets. A way to capture the template. In ROBOT templates the templates are captured in a header row of the same table that captures the data, in DOSDP templates the templates are captured in a separate YAML file and in OTTR typically the templates are serialised as and RDF-graph in a format like RDF/XML or Turtle. A toolkit that can combine the data and the template to generate OWL axioms and annotations. ROBOT templates can be compiled to OWL using ROBOT , DOSDP templates can be compiled using DOSDP tools and OTTR templates using Lutra . In OBO we are currently mostly concerned with ROBOT templates and DOSDP templates. Before moving on, we recommend to complete a basic tutorial in both: ROBOT template tutorial DOSDP template tutorial ROBOT template vs DOSDP template \u00b6 Ontologies, especially in the biomedical domain, are complex and, while growing in size, increasingly hard to manage for their curators. In this section, we will look at some of the key differences of two popular templating systems in the OBO domain: Dead Simple Ontology Design Patterns (DOSDPs) and ROBOT templates. We will not cover the rationale for templates in general in much depth (the interested reader should check ontology design patterns and Reasonable Ontology Templates (OTTR): Motivation and Overview , which pertains to a different system, but applies none-the-less in general), and focus on making it easier for developers to pick the right templating approach for their particular use case. We will first discuss in detail representational differences, before we go through the functional ones and delineate use cases. Structural differences, formats and tools \u00b6 DOSDP templates: structure and format \u00b6 DOSDP separates data and templates into two files: a yaml file which defines the template, and a TSV file which holds the data. Lets look at s example. The template: abnormalAnatomicalEntity pattern_name: abnormalAnatomicalEntity pattern_iri: http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml description: \"Any unspecified abnormality of an anatomical entity.\" contributors: - https://orcid.org/0000-0002-9900-7880 - https://orcid.org/0000-0001-9076-6015 - https://orcid.org/0000-0003-4148-4606 - https://orcid.org/0000-0002-3528-5267 classes: quality: PATO:0000001 abnormal: PATO:0000460 anatomical entity: UBERON:0001062 relations: inheres_in_part_of: RO:0002314 has_modifier: RO:0002573 has_part: BFO:0000051 annotationProperties: exact_synonym: oio:hasExactSynonym vars: anatomical_entity: \"'anatomical entity'\" name: text: \"abnormal %s\" vars: - anatomical_entity annotations: - annotationProperty: exact_synonym text: \"abnormality of %s\" vars: - anatomical_entity def: text: \"Abnormality of %s.\" vars: - anatomical_entity equivalentTo: text: \"'has_part' some ('quality' and ('inheres_in_part_of' some %s) and ('has_modifier' some 'abnormal'))\" vars: - anatomical_entity The data: abnormalAnatomicalEntity.tsv defined_class defined_class_label anatomical_entity anatomical_entity_label HP:0040286 Abnormal axial muscle morphology UBERON:0003897 axial muscle HP:0011297 Abnormal digit morphology UBERON:0002544 digit ROBOT templates: structure and format \u00b6 ROBOT encodes both the template and the data in the same TSV; after the table header, the second row basically encodes the entire template logic, and the data follows in table row 3. ID Label EQ Anatomy Label ID LABEL EC 'has_part' some ('quality' and ('inheres_in_part_of' some %) and ('has_modifier' some 'abnormal')) HP:0040286 Abnormal axial muscle morphology UBERON:0003897 axial muscle HP:0011297 Abnormal digit morphology UBERON:0002544 digit Note that for the Anatomy Label we deliberately left the second row empty, which instructs the ROBOT template tool to completely ignore this column . A discussion on the main differences \u00b6 Ontology Engineering perspective \u00b6 From an ontology engineering perspective, the essence of the difference between DOSDP and ROBOT templates could be captured as follows: DOSDP templates are more about generating annotations and axioms, while ROBOT templates are more about curating annotations and axioms. Curating annotations and axioms means that an editor, or ontology curator, manually enters the labels, synonyms, definitions and so forth into the spreadsheet. Generating axioms in the sense of this section means that we try to automatically generate labels, synonyms, definitions and so forth based on the related logical entities in the patterns. E.g., using the example template above, the label \"abnormal kidney\" would automatically be generated when the Uberon term for kidney is supplied. While both ROBOT and DOSDP can be used for \"curation\" of annotation of axioms, DOSDP seeks to apply generation rules to automatically generate synonyms, labels, definitions and so forth while for ROBOT template seeks to collect manually curated information in an easy-to-use table which is then compiled into OWL. In other words: the average DOSDP user will not write their own labels, definitions and synonyms - they will want those to be generated automatically from a set of simple rules; the average ROBOT template user will not want automatically generated definitions, labels and synonyms - they will want to capture their own. Sharing and Re-use \u00b6 However, there is another dimension in which both approaches differ widely: sharing and re-use. DOSDPs by far the most important feature is that it allows a community of developers to rally around a modelling problem, debate and establish consensus; for example, a pattern can be used to say: this is how we model abnormal anatomical entities. Consensus can be made explicit by \"signing off\" on the pattern (e.g. by adding your ORCId to the list of contributors), and due to the template/data separation, the template can be simply imported using its IRI (for example http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml) and re-used by everyone. Furthermore, additional metadata fields including textual descriptions, and more recently \"examples\", make DOSDP template files comparatively easy to understand, even by a less technically inclined editor. ROBOT templates on the other hand do not lend themselves to community debates in the same way; first of all, they are typically supplied including all data merged in; secondly, they do not provide additional metadata fields that could, for example, conveniently be used to represent a sign off (you could, of course, add the ORCId's into a non-functional column, or as a pipe-separated string into a cell in the first or second row; but its obvious that this would be quite clunky) or a textual description. A yaml file is much easier for a human to read and understand then the header of a TSV file, especially when the template becomes quite large. However, there is a flipside to the strict separation of data and templates. One is that DOSDP templates are really hard to change. Once, for example, a particular variable name was chosen, renaming the variable will require an excessive community-wide action to rename columns in all associated spreadsheets - which requires them all to be known beforehand (which is not always the case). You don't have such a problem with ROBOT templates; if you change a column name, or a template string, everything will continue to work without any additional coordination. Summary \u00b6 Both ROBOT templates and DOSDP templates are widely used. The author of this page uses both in most of the projects he is involved in, because of their different strengths and capabilities. You can use the following rules of thumb to inform your choice: Consider ROBOT templates if your emphasis is on manually curating labels, definitions, synonyms and axioms or other annotations managing your templates in the spreadsheet itself is a concern for you (this is often the case, for example, when turning an existing data table into a ROBOT template ad hoc) Consider DOSDP templates if your emphasis is on re-use, community-wide implementation of the same templates and community discussion, you should consider DOSDP templates automatically generating labels, definitions, synonyms from rules in the pattern. Detour: Concerns with Managing Tables \u00b6 There is a nice debate going on which questions the use of tables in ontology curation altogether. There are many nuances in this debate, but I want to stylise it here as two schools of thoughts (there are probably hundreds in between, but this makes it easier to follow): The one school (let's call them Tablosceptics) claims that using tables introduces a certain degree of fragility into the development process due to a number of factors, including: losing the immediateness of QC feedback; Table-based development, so the Tablosceptics, encourages lazy editing (adding stuff to a template and then not reviewing the consequence properly, which we will discuss in more depth later). losing track of the ID space (in a multi-table world, it becomes increasingly hard to manage IDs, making sure they are not double used etc) and 3) encouraging bad design (relying more on assertion than inference). They prefer to use tools like Protege that show the curator immediately the consequences of their actions, like reasoning errors (unintended equivalent classes, unsatisfiable classes and other unintended inferences). The Tablophile school of thought responds to these accusations in essence with \"tools\"; they say that tables are essentially a convenient matrix to input the data (which in turns opens ontology curation to a much wider range of people), and it is up to the tools to ensure that QC is run, hierarchies are being presented for review and weird ID space clashes are flagged up. Furthermore, they say, having a controlled input matrix will actually decrease the number of faulty annotations or axioms (which is evidenced by the large number of wrongful annotation assertions across OBO foundry ontologies I see every day as part of my work). At first sight, both template systems are affected equally by the war of the Tablosceptics and the Tablophile. Indeed, in my on practice, the ID space issue is really problematic when we manage 100s and more templates, and so far, I have not seen a nice and clear solution that ensures that no ID used twice unless it is so intended and respects ID spaces which are often semi-formally assigned to individual curators of an ontology. Generally in this course we do not want to take a 100% stance. The author of this page believes that the advantage of using tables and involving many more people in the development process outweighs any concerns, but tooling is required that can provide more immediate feedback when such tables such as the ones presented here are curated at scale.","title":"Templates for OBO ontologies"},{"location":"lesson/templates_for_obo/#templating-systems-for-obo-ontologies-a-deep-dive","text":"Ontologies are notoriously hard to edit. This makes it a very high burden to edit ontologies for anyone but a select few. However, many of the contents of ontologies are actually best edited by domain experts with often little or known ontological training - editing labels and synonyms, curating definitions, adding references to publications and many more. Furthermore, if we simply remove the burden of writing OWL axioms, editors with very little ontology training can actually curate even logical content: for example, if we want to describe that a class is restricted to a certain taxon (also known as taxon-restriction), the editor is often capable to select the appropriate taxon for a term (say, a \"mouse heart\" is restricted to the taxon of Mus musculus ), but maybe they would not know how to \"add that restriction to the ontology\". Tables are great (for a deep dive into tables and triples see here ). Scientists in particular love tables , and, even more importantly, can be trained easily to edit data in spreadsheet tools, such as Google Sheets or Microsoft Excel. Ontology templating systems, such as DOSDP templates , ROBOT templates and Reasonable Ontology Templates (OTTR) allow separating the raw data in the ontology (labels, synonyms, related ontological entities, descriptions, cross-references and other metadata) from the OWL language patterns that are used to manifest them in the ontology. There are three main ingredients to a templating system: A way to capture the data. In all the systems we care about, these are tables, usually manifested as spreadsheets in Excel or Google Sheets. A way to capture the template. In ROBOT templates the templates are captured in a header row of the same table that captures the data, in DOSDP templates the templates are captured in a separate YAML file and in OTTR typically the templates are serialised as and RDF-graph in a format like RDF/XML or Turtle. A toolkit that can combine the data and the template to generate OWL axioms and annotations. ROBOT templates can be compiled to OWL using ROBOT , DOSDP templates can be compiled using DOSDP tools and OTTR templates using Lutra . In OBO we are currently mostly concerned with ROBOT templates and DOSDP templates. Before moving on, we recommend to complete a basic tutorial in both: ROBOT template tutorial DOSDP template tutorial","title":"Templating systems for OBO ontologies: a deep dive"},{"location":"lesson/templates_for_obo/#robot-template-vs-dosdp-template","text":"Ontologies, especially in the biomedical domain, are complex and, while growing in size, increasingly hard to manage for their curators. In this section, we will look at some of the key differences of two popular templating systems in the OBO domain: Dead Simple Ontology Design Patterns (DOSDPs) and ROBOT templates. We will not cover the rationale for templates in general in much depth (the interested reader should check ontology design patterns and Reasonable Ontology Templates (OTTR): Motivation and Overview , which pertains to a different system, but applies none-the-less in general), and focus on making it easier for developers to pick the right templating approach for their particular use case. We will first discuss in detail representational differences, before we go through the functional ones and delineate use cases.","title":"ROBOT template vs DOSDP template"},{"location":"lesson/templates_for_obo/#structural-differences-formats-and-tools","text":"","title":"Structural differences, formats and tools"},{"location":"lesson/templates_for_obo/#dosdp-templates-structure-and-format","text":"DOSDP separates data and templates into two files: a yaml file which defines the template, and a TSV file which holds the data. Lets look at s example. The template: abnormalAnatomicalEntity pattern_name: abnormalAnatomicalEntity pattern_iri: http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml description: \"Any unspecified abnormality of an anatomical entity.\" contributors: - https://orcid.org/0000-0002-9900-7880 - https://orcid.org/0000-0001-9076-6015 - https://orcid.org/0000-0003-4148-4606 - https://orcid.org/0000-0002-3528-5267 classes: quality: PATO:0000001 abnormal: PATO:0000460 anatomical entity: UBERON:0001062 relations: inheres_in_part_of: RO:0002314 has_modifier: RO:0002573 has_part: BFO:0000051 annotationProperties: exact_synonym: oio:hasExactSynonym vars: anatomical_entity: \"'anatomical entity'\" name: text: \"abnormal %s\" vars: - anatomical_entity annotations: - annotationProperty: exact_synonym text: \"abnormality of %s\" vars: - anatomical_entity def: text: \"Abnormality of %s.\" vars: - anatomical_entity equivalentTo: text: \"'has_part' some ('quality' and ('inheres_in_part_of' some %s) and ('has_modifier' some 'abnormal'))\" vars: - anatomical_entity The data: abnormalAnatomicalEntity.tsv defined_class defined_class_label anatomical_entity anatomical_entity_label HP:0040286 Abnormal axial muscle morphology UBERON:0003897 axial muscle HP:0011297 Abnormal digit morphology UBERON:0002544 digit","title":"DOSDP templates: structure and format"},{"location":"lesson/templates_for_obo/#robot-templates-structure-and-format","text":"ROBOT encodes both the template and the data in the same TSV; after the table header, the second row basically encodes the entire template logic, and the data follows in table row 3. ID Label EQ Anatomy Label ID LABEL EC 'has_part' some ('quality' and ('inheres_in_part_of' some %) and ('has_modifier' some 'abnormal')) HP:0040286 Abnormal axial muscle morphology UBERON:0003897 axial muscle HP:0011297 Abnormal digit morphology UBERON:0002544 digit Note that for the Anatomy Label we deliberately left the second row empty, which instructs the ROBOT template tool to completely ignore this column .","title":"ROBOT templates: structure and format"},{"location":"lesson/templates_for_obo/#a-discussion-on-the-main-differences","text":"","title":"A discussion on the main differences"},{"location":"lesson/templates_for_obo/#ontology-engineering-perspective","text":"From an ontology engineering perspective, the essence of the difference between DOSDP and ROBOT templates could be captured as follows: DOSDP templates are more about generating annotations and axioms, while ROBOT templates are more about curating annotations and axioms. Curating annotations and axioms means that an editor, or ontology curator, manually enters the labels, synonyms, definitions and so forth into the spreadsheet. Generating axioms in the sense of this section means that we try to automatically generate labels, synonyms, definitions and so forth based on the related logical entities in the patterns. E.g., using the example template above, the label \"abnormal kidney\" would automatically be generated when the Uberon term for kidney is supplied. While both ROBOT and DOSDP can be used for \"curation\" of annotation of axioms, DOSDP seeks to apply generation rules to automatically generate synonyms, labels, definitions and so forth while for ROBOT template seeks to collect manually curated information in an easy-to-use table which is then compiled into OWL. In other words: the average DOSDP user will not write their own labels, definitions and synonyms - they will want those to be generated automatically from a set of simple rules; the average ROBOT template user will not want automatically generated definitions, labels and synonyms - they will want to capture their own.","title":"Ontology Engineering perspective"},{"location":"lesson/templates_for_obo/#sharing-and-re-use","text":"However, there is another dimension in which both approaches differ widely: sharing and re-use. DOSDPs by far the most important feature is that it allows a community of developers to rally around a modelling problem, debate and establish consensus; for example, a pattern can be used to say: this is how we model abnormal anatomical entities. Consensus can be made explicit by \"signing off\" on the pattern (e.g. by adding your ORCId to the list of contributors), and due to the template/data separation, the template can be simply imported using its IRI (for example http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml) and re-used by everyone. Furthermore, additional metadata fields including textual descriptions, and more recently \"examples\", make DOSDP template files comparatively easy to understand, even by a less technically inclined editor. ROBOT templates on the other hand do not lend themselves to community debates in the same way; first of all, they are typically supplied including all data merged in; secondly, they do not provide additional metadata fields that could, for example, conveniently be used to represent a sign off (you could, of course, add the ORCId's into a non-functional column, or as a pipe-separated string into a cell in the first or second row; but its obvious that this would be quite clunky) or a textual description. A yaml file is much easier for a human to read and understand then the header of a TSV file, especially when the template becomes quite large. However, there is a flipside to the strict separation of data and templates. One is that DOSDP templates are really hard to change. Once, for example, a particular variable name was chosen, renaming the variable will require an excessive community-wide action to rename columns in all associated spreadsheets - which requires them all to be known beforehand (which is not always the case). You don't have such a problem with ROBOT templates; if you change a column name, or a template string, everything will continue to work without any additional coordination.","title":"Sharing and Re-use"},{"location":"lesson/templates_for_obo/#summary","text":"Both ROBOT templates and DOSDP templates are widely used. The author of this page uses both in most of the projects he is involved in, because of their different strengths and capabilities. You can use the following rules of thumb to inform your choice: Consider ROBOT templates if your emphasis is on manually curating labels, definitions, synonyms and axioms or other annotations managing your templates in the spreadsheet itself is a concern for you (this is often the case, for example, when turning an existing data table into a ROBOT template ad hoc) Consider DOSDP templates if your emphasis is on re-use, community-wide implementation of the same templates and community discussion, you should consider DOSDP templates automatically generating labels, definitions, synonyms from rules in the pattern.","title":"Summary"},{"location":"lesson/templates_for_obo/#detour-concerns-with-managing-tables","text":"There is a nice debate going on which questions the use of tables in ontology curation altogether. There are many nuances in this debate, but I want to stylise it here as two schools of thoughts (there are probably hundreds in between, but this makes it easier to follow): The one school (let's call them Tablosceptics) claims that using tables introduces a certain degree of fragility into the development process due to a number of factors, including: losing the immediateness of QC feedback; Table-based development, so the Tablosceptics, encourages lazy editing (adding stuff to a template and then not reviewing the consequence properly, which we will discuss in more depth later). losing track of the ID space (in a multi-table world, it becomes increasingly hard to manage IDs, making sure they are not double used etc) and 3) encouraging bad design (relying more on assertion than inference). They prefer to use tools like Protege that show the curator immediately the consequences of their actions, like reasoning errors (unintended equivalent classes, unsatisfiable classes and other unintended inferences). The Tablophile school of thought responds to these accusations in essence with \"tools\"; they say that tables are essentially a convenient matrix to input the data (which in turns opens ontology curation to a much wider range of people), and it is up to the tools to ensure that QC is run, hierarchies are being presented for review and weird ID space clashes are flagged up. Furthermore, they say, having a controlled input matrix will actually decrease the number of faulty annotations or axioms (which is evidenced by the large number of wrongful annotation assertions across OBO foundry ontologies I see every day as part of my work). At first sight, both template systems are affected equally by the war of the Tablosceptics and the Tablophile. Indeed, in my on practice, the ID space issue is really problematic when we manage 100s and more templates, and so far, I have not seen a nice and clear solution that ensures that no ID used twice unless it is so intended and respects ID spaces which are often semi-formally assigned to individual curators of an ontology. Generally in this course we do not want to take a 100% stance. The author of this page believes that the advantage of using tables and involving many more people in the development process outweighs any concerns, but tooling is required that can provide more immediate feedback when such tables such as the ones presented here are curated at scale.","title":"Detour: Concerns with Managing Tables"},{"location":"lesson/troublehooting-robot/","text":"Lessons learned from troubleshooting ROBOT \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Review tutorial on Ontology pipelines with ROBOT and SPARQL Review tutorial on Ontology Pipelines with ROBOT 2 Learning objectives \u00b6 Learn common mistakes when using ROBOT and how to troubleshoot and fix them Lessons learned \u00b6 Protege only allows one comment on a class. If you are adding new comments to terms via ROBOT, you will get an error if a comment already exists on a term. If you run ROBOT and get an error, it may create a blank file. You need to discard the changes and/or open a new branch. The error with the optional \u201cnull\u201d is when the mondo-edit file is empty. Optional.get() cannot be called on an absent value Use the -vvv option to show the stack trace. Use the --help option to see usage information make: *** [mondo.Makefile:454: merge_template] Error 1 Open questions \u00b6 How do we create a new ontology from scratch using ROBOT? Contributors \u00b6 Sabrina Toro ( ORCID ) Nicole Vasilevsky ( ORCID )","title":"Lessons learned from troubleshooting ROBOT"},{"location":"lesson/troublehooting-robot/#lessons-learned-from-troubleshooting-robot","text":"","title":"Lessons learned from troubleshooting ROBOT"},{"location":"lesson/troublehooting-robot/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/troublehooting-robot/#prerequisites","text":"Review tutorial on Ontology pipelines with ROBOT and SPARQL Review tutorial on Ontology Pipelines with ROBOT 2","title":"Prerequisites"},{"location":"lesson/troublehooting-robot/#learning-objectives","text":"Learn common mistakes when using ROBOT and how to troubleshoot and fix them","title":"Learning objectives"},{"location":"lesson/troublehooting-robot/#lessons-learned","text":"Protege only allows one comment on a class. If you are adding new comments to terms via ROBOT, you will get an error if a comment already exists on a term. If you run ROBOT and get an error, it may create a blank file. You need to discard the changes and/or open a new branch. The error with the optional \u201cnull\u201d is when the mondo-edit file is empty. Optional.get() cannot be called on an absent value Use the -vvv option to show the stack trace. Use the --help option to see usage information make: *** [mondo.Makefile:454: merge_template] Error 1","title":"Lessons learned"},{"location":"lesson/troublehooting-robot/#open-questions","text":"How do we create a new ontology from scratch using ROBOT?","title":"Open questions"},{"location":"lesson/troublehooting-robot/#contributors","text":"Sabrina Toro ( ORCID ) Nicole Vasilevsky ( ORCID )","title":"Contributors"},{"location":"lesson/using_disease_and_phenotype_ontologies/","text":"Finding and using Disease and Phenotype Ontologies \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 None Preparation \u00b6 Review tutorial on Ontology Term Use What is delivered as part of the course \u00b6 Description: An introduction to the landscape of disease and phenotype terminologies and ontologies, and how they can be used to add value to your analysis. Learning objectives \u00b6 Become aware of the major disease and phenotype ontologies that are available Be able to decide which phenotype or disease ontology to use for different use cases Understand how to leverage disease and phenotype ontologies for advanced data analytics Have a basic understanding of how to integrate other data Tutorials \u00b6 Video from Disease and Phenotypes c-path lesson 2021-06-16 Additional materials and resources \u00b6 Contributors \u00b6 Nicole Vasilevsky Nico Matentzoglu Major disease and phenotype ontologies that are available \u00b6 A landscape analysis of major disease and phenotype ontologies that are currently available is here . Decide which phenotype or disease ontology to use for different use cases \u00b6 Different ontologies are build for different purposes and were created for various reasons. For example, some ontologies are built for text mining purposes, some are built for annotating data and downstream computational analysis. The unified phenotype ontology (uPheno) aggregates species-specific phenotype ontologies into a unified resource. Several species-specific phenotype ontologies exist, such as the Human Phenotype Ontology , Mammalian Phenotype Ontology (http://www.informatics.jax.org/searches/MP_form.shtml) and many more. Similarly to the phenotype ontologies, there are many disease ontologies that exist that are specific to certain areas of diseases, such as infectious diseases (e.g. Infectious Disease Ontology ), cancer (e.g. National Cancer Institute Thesaurus ), rare diseases (e.g. Orphanet ), etc. In addition, there are several more general disease ontologies, such as the Mondo Disease Ontology , the Human Disease Ontology (DO) , SNOMED , etc. Different disease ontologies may be built for different purposes; for example, ontologies like Mondo and DO are intended to be used for classifying data, and downstream computational analyses. Some terminologies are used for indexing purposes, such as International classification of Diseases (ICD). ICD-11 is intended for indexing medical encounters for the purposes of billing and coding. Some of the disease ontologies listed on the landscape contain terms that define diseases, such as Ontology for General Medical Sciences (OGMS) are upper level ontologies and are intended for integration with other ontologies. When deciding on which phenotype or disease ontology to use, some things to consider: Do you need a more specific ontology, such as a species-specific ontology, or do you need a more general ontology that is cross-species or covers more aspects of diseases? Is the ontology open and free to use? Does the description of the ontology describe it's intended use? For example, some ontologies are built for text mining purposes, some are built for annotating data and downstream computational analysis. Is the ontology actively maintained? Does the ontology contain the terms you need? If not, is there a mechanism to request changes and new terms and are the ontology developers responsive to change requests on their tracker? Is the ontology widely used by the community? You can check things like active contributors on GitHub, usages described on the OBO Foundry page (for example http://obofoundry.org/ontology/mondo.html), published papers and citations. Understand how to leverage disease and phenotype ontologies for advanced data analytics \u00b6 How to integrate other data \u00b6","title":"Disease and Phenotype Ontologies"},{"location":"lesson/using_disease_and_phenotype_ontologies/#finding-and-using-disease-and-phenotype-ontologies","text":"","title":"Finding and using Disease and Phenotype Ontologies"},{"location":"lesson/using_disease_and_phenotype_ontologies/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/using_disease_and_phenotype_ontologies/#prerequisites","text":"None","title":"Prerequisites"},{"location":"lesson/using_disease_and_phenotype_ontologies/#preparation","text":"Review tutorial on Ontology Term Use","title":"Preparation"},{"location":"lesson/using_disease_and_phenotype_ontologies/#what-is-delivered-as-part-of-the-course","text":"Description: An introduction to the landscape of disease and phenotype terminologies and ontologies, and how they can be used to add value to your analysis.","title":"What is delivered as part of the course"},{"location":"lesson/using_disease_and_phenotype_ontologies/#learning-objectives","text":"Become aware of the major disease and phenotype ontologies that are available Be able to decide which phenotype or disease ontology to use for different use cases Understand how to leverage disease and phenotype ontologies for advanced data analytics Have a basic understanding of how to integrate other data","title":"Learning objectives"},{"location":"lesson/using_disease_and_phenotype_ontologies/#tutorials","text":"Video from Disease and Phenotypes c-path lesson 2021-06-16","title":"Tutorials"},{"location":"lesson/using_disease_and_phenotype_ontologies/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/using_disease_and_phenotype_ontologies/#contributors","text":"Nicole Vasilevsky Nico Matentzoglu","title":"Contributors"},{"location":"lesson/using_disease_and_phenotype_ontologies/#major-disease-and-phenotype-ontologies-that-are-available","text":"A landscape analysis of major disease and phenotype ontologies that are currently available is here .","title":"Major disease and phenotype ontologies that are available"},{"location":"lesson/using_disease_and_phenotype_ontologies/#decide-which-phenotype-or-disease-ontology-to-use-for-different-use-cases","text":"Different ontologies are build for different purposes and were created for various reasons. For example, some ontologies are built for text mining purposes, some are built for annotating data and downstream computational analysis. The unified phenotype ontology (uPheno) aggregates species-specific phenotype ontologies into a unified resource. Several species-specific phenotype ontologies exist, such as the Human Phenotype Ontology , Mammalian Phenotype Ontology (http://www.informatics.jax.org/searches/MP_form.shtml) and many more. Similarly to the phenotype ontologies, there are many disease ontologies that exist that are specific to certain areas of diseases, such as infectious diseases (e.g. Infectious Disease Ontology ), cancer (e.g. National Cancer Institute Thesaurus ), rare diseases (e.g. Orphanet ), etc. In addition, there are several more general disease ontologies, such as the Mondo Disease Ontology , the Human Disease Ontology (DO) , SNOMED , etc. Different disease ontologies may be built for different purposes; for example, ontologies like Mondo and DO are intended to be used for classifying data, and downstream computational analyses. Some terminologies are used for indexing purposes, such as International classification of Diseases (ICD). ICD-11 is intended for indexing medical encounters for the purposes of billing and coding. Some of the disease ontologies listed on the landscape contain terms that define diseases, such as Ontology for General Medical Sciences (OGMS) are upper level ontologies and are intended for integration with other ontologies. When deciding on which phenotype or disease ontology to use, some things to consider: Do you need a more specific ontology, such as a species-specific ontology, or do you need a more general ontology that is cross-species or covers more aspects of diseases? Is the ontology open and free to use? Does the description of the ontology describe it's intended use? For example, some ontologies are built for text mining purposes, some are built for annotating data and downstream computational analysis. Is the ontology actively maintained? Does the ontology contain the terms you need? If not, is there a mechanism to request changes and new terms and are the ontology developers responsive to change requests on their tracker? Is the ontology widely used by the community? You can check things like active contributors on GitHub, usages described on the OBO Foundry page (for example http://obofoundry.org/ontology/mondo.html), published papers and citations.","title":"Decide which phenotype or disease ontology to use for different use cases"},{"location":"lesson/using_disease_and_phenotype_ontologies/#understand-how-to-leverage-disease-and-phenotype-ontologies-for-advanced-data-analytics","text":"","title":"Understand how to leverage disease and phenotype ontologies for advanced data analytics"},{"location":"lesson/using_disease_and_phenotype_ontologies/#how-to-integrate-other-data","text":"","title":"How to integrate other data"},{"location":"reference/git_faq/","text":"Git FAQs \u00b6 This page aims to consolidate some tips and tricks that ontology editors have found useful in using git . It is not meant to be a tutorial of git , but rather as a page with tips that could help in certain specialised situations. Reverting Commits \u00b6 Reverting particular files back to master version \u00b6 If you want to revert only certain files (eg import files), you can do it using Terminal. For this example, we will use uberon_import.owl as the file we want reverted back to the version in master branch, however, this can be done on any file. Assuming your directory is set to src/ontology , in terminal use: git checkout master -- imports/uberon_import.owl . Commit the change to the branch as normal. Reverting particular files back to a previous version \u00b6 If you want to revert a file back to a previous version instead of master, you can use the commit ID. To do this, in Terminal use: git log to list out the previous commits and copy the commit code of the commit you would like to revert to (example: see yellow string of text in screenshot below). Press q on your keyboard to quit git log (or down arrow to scroll down to continue to find the commit ID you want to revert if it is further down). In terminal use: git checkout ff18c9482035062bbbbb27aaeb50e658298fb635 -- imports/uberon_import.owl using whichever commit code you want instead of the commit code in this example. commit the change to the branch as normal.","title":"Git FAQ"},{"location":"reference/git_faq/#git-faqs","text":"This page aims to consolidate some tips and tricks that ontology editors have found useful in using git . It is not meant to be a tutorial of git , but rather as a page with tips that could help in certain specialised situations.","title":"Git FAQs"},{"location":"reference/git_faq/#reverting-commits","text":"","title":"Reverting Commits"},{"location":"reference/git_faq/#reverting-particular-files-back-to-master-version","text":"If you want to revert only certain files (eg import files), you can do it using Terminal. For this example, we will use uberon_import.owl as the file we want reverted back to the version in master branch, however, this can be done on any file. Assuming your directory is set to src/ontology , in terminal use: git checkout master -- imports/uberon_import.owl . Commit the change to the branch as normal.","title":"Reverting particular files back to master version"},{"location":"reference/git_faq/#reverting-particular-files-back-to-a-previous-version","text":"If you want to revert a file back to a previous version instead of master, you can use the commit ID. To do this, in Terminal use: git log to list out the previous commits and copy the commit code of the commit you would like to revert to (example: see yellow string of text in screenshot below). Press q on your keyboard to quit git log (or down arrow to scroll down to continue to find the commit ID you want to revert if it is further down). In terminal use: git checkout ff18c9482035062bbbbb27aaeb50e658298fb635 -- imports/uberon_import.owl using whichever commit code you want instead of the commit code in this example. commit the change to the branch as normal.","title":"Reverting particular files back to a previous version"},{"location":"reference/github_desktop/","text":"GitHub Desktop \u00b6 For most of our training activities, we recommend using GitHub Desktop . It provides a very convenient way to push and pull changes, and inspect the \"diff\". It is, however, not mandatory if you are already familiar with other git workflows (such as command line, or Sourcetree).","title":"GitHub Desktop"},{"location":"reference/github_desktop/#github-desktop","text":"For most of our training activities, we recommend using GitHub Desktop . It provides a very convenient way to push and pull changes, and inspect the \"diff\". It is, however, not mandatory if you are already familiar with other git workflows (such as command line, or Sourcetree).","title":"GitHub Desktop"},{"location":"reference/glossary/","text":"Glossary for concepts in and around OBO \u00b6 Tools \u00b6 Term Definition Type Docs Ontology Development Kit (ODK) A toolkit and docker image for managing ontology releases. Tool docs ROBOT A toolkit for transforming and interacting with ontologies. Tool docs rdflib A python library to interact with RDF data Library docs OWL API A java-based API to interact with OWL ontologies Library docs Protege A typical ontology development tool used by ontology developers in the OBO-sphere Tool docs ROBOT templates A templating system based on tables, where the templates are integrated in the same table as the data Standard docs Dead Simple Ontology Design Patterns (DOSDP) A templating system for ontologies with well-documented patterns and templates. Standard docs DOSDP tools DOSDP is the open source reference implementation of the DOSDP templating language. Tool docs Reasonable Ontology Templates (OTTR) A system for composable ontology templates and documentation Standard docs Lutra Lutra is the open source reference implementation of the OTTR templating language. Tool docs","title":"Glossary"},{"location":"reference/glossary/#glossary-for-concepts-in-and-around-obo","text":"","title":"Glossary for concepts in and around OBO"},{"location":"reference/glossary/#tools","text":"Term Definition Type Docs Ontology Development Kit (ODK) A toolkit and docker image for managing ontology releases. Tool docs ROBOT A toolkit for transforming and interacting with ontologies. Tool docs rdflib A python library to interact with RDF data Library docs OWL API A java-based API to interact with OWL ontologies Library docs Protege A typical ontology development tool used by ontology developers in the OBO-sphere Tool docs ROBOT templates A templating system based on tables, where the templates are integrated in the same table as the data Standard docs Dead Simple Ontology Design Patterns (DOSDP) A templating system for ontologies with well-documented patterns and templates. Standard docs DOSDP tools DOSDP is the open source reference implementation of the DOSDP templating language. Tool docs Reasonable Ontology Templates (OTTR) A system for composable ontology templates and documentation Standard docs Lutra Lutra is the open source reference implementation of the OTTR templating language. Tool docs","title":"Tools"},{"location":"reference/mungall_blog_radar/","text":"Monkeying around with OWL \u00b6 An index page to find some of our favourite articles on Chris' blog. These are not all articles, but I selection we found useful during our every work. Ontology development and modelling \u00b6 OntoTips Series . Must read series for the beginning ontology developer. Warning about complex modelling . Chris is generally big on Occam's Razor solutions: given two solutions that solve a use case, the simpler is better. OntoTip: Don\u2019t over-specify OWL definitions . From the above OntoTip series. How to deal with unintentional equivalent classes Ontology curation \u00b6 OntoTip: Write simple, concise, clear, operational textual definitions . One of our favourite blog posts of Chris. Must read!","title":"Monkeying around with OWL"},{"location":"reference/mungall_blog_radar/#monkeying-around-with-owl","text":"An index page to find some of our favourite articles on Chris' blog. These are not all articles, but I selection we found useful during our every work.","title":"Monkeying around with OWL"},{"location":"reference/mungall_blog_radar/#ontology-development-and-modelling","text":"OntoTips Series . Must read series for the beginning ontology developer. Warning about complex modelling . Chris is generally big on Occam's Razor solutions: given two solutions that solve a use case, the simpler is better. OntoTip: Don\u2019t over-specify OWL definitions . From the above OntoTip series. How to deal with unintentional equivalent classes","title":"Ontology development and modelling"},{"location":"reference/mungall_blog_radar/#ontology-curation","text":"OntoTip: Write simple, concise, clear, operational textual definitions . One of our favourite blog posts of Chris. Must read!","title":"Ontology curation"},{"location":"reference/odk/","text":"Ontology Development Kit (ODK) Reference \u00b6 The ODK is essentially two things: A toolbox. All frequently used tools for managing the ontology life cycle are bundled together into a Docker image: ROBOT, owltools, fastobo-validator, dosdp-tools, riot and many, many more. A system, you could have say \"methodology\" for managing the ontology life cycle from continious integration and quality control to imports and release management. The Toolbox \u00b6 The ODK bundles a lot of tools together, such as ROBOT, owltools, fastobo-validator and dosdp-tools. To get a better idea, its best to simply read the Dockerfile specifications of the ODK image: ODK Lite Image . This contains the most essentials tools related to ODK development. Most of the day to day activities of ontology developers with ROBOT are well covered by odklite . ODK Full Image . Extends the ODK Lite image with a further round of powerful tools. It contains for example Apache Jena, the OBO Dashboard, the Konclude reasoner and a large array of command line tools. The system for ontology life cycle management \u00b6 One of the tools in the toolbox, the \"seed my repo\" function, allows us to generate a complete GitHub repository with everything needed to manage an OBO ontology according to OBO best practices. The two central components are A Makefile that encodes the rules by which ontology release files should be derived from the source of truth (the edit file). A support for CI such as GitHub actions or Travis for running continuous integration checks.","title":"Ontology Development Kit (ODK)"},{"location":"reference/odk/#ontology-development-kit-odk-reference","text":"The ODK is essentially two things: A toolbox. All frequently used tools for managing the ontology life cycle are bundled together into a Docker image: ROBOT, owltools, fastobo-validator, dosdp-tools, riot and many, many more. A system, you could have say \"methodology\" for managing the ontology life cycle from continious integration and quality control to imports and release management.","title":"Ontology Development Kit (ODK) Reference"},{"location":"reference/odk/#the-toolbox","text":"The ODK bundles a lot of tools together, such as ROBOT, owltools, fastobo-validator and dosdp-tools. To get a better idea, its best to simply read the Dockerfile specifications of the ODK image: ODK Lite Image . This contains the most essentials tools related to ODK development. Most of the day to day activities of ontology developers with ROBOT are well covered by odklite . ODK Full Image . Extends the ODK Lite image with a further round of powerful tools. It contains for example Apache Jena, the OBO Dashboard, the Konclude reasoner and a large array of command line tools.","title":"The Toolbox"},{"location":"reference/odk/#the-system-for-ontology-life-cycle-management","text":"One of the tools in the toolbox, the \"seed my repo\" function, allows us to generate a complete GitHub repository with everything needed to manage an OBO ontology according to OBO best practices. The two central components are A Makefile that encodes the rules by which ontology release files should be derived from the source of truth (the edit file). A support for CI such as GitHub actions or Travis for running continuous integration checks.","title":"The system for ontology life cycle management"},{"location":"reference/ontology-curator/","text":"A Day in the Life of an Ontology Curator \u00b6 Review issues on the issue tracker . Tickets by organized by assigning labels (such as new term requests ) and milestones Can also sort tickets on Project boards Can search on labels, milestones , assignee , etc. In Mondo, we set priorities based on user requests, size of the ticket (ie amount of work required), if it is blocking something else, etc. Edits to the Mondo ontology are made on Branches and via Pull Requests on the mondo-edit.obo file. Example: work on an open ticket to add a new term using Protege. Detailed instructions on how to add a new term are here . Example: User request to add 50+ subtyps of acute myeloid leukemia. We used a ROBOT template . Do you want to contribute? See tickets labeled good first issue . Documentation and more instructions are available in the Mondo editors guide .","title":"A day in the life of an ontology curator"},{"location":"reference/ontology-curator/#a-day-in-the-life-of-an-ontology-curator","text":"Review issues on the issue tracker . Tickets by organized by assigning labels (such as new term requests ) and milestones Can also sort tickets on Project boards Can search on labels, milestones , assignee , etc. In Mondo, we set priorities based on user requests, size of the ticket (ie amount of work required), if it is blocking something else, etc. Edits to the Mondo ontology are made on Branches and via Pull Requests on the mondo-edit.obo file. Example: work on an open ticket to add a new term using Protege. Detailed instructions on how to add a new term are here . Example: User request to add 50+ subtyps of acute myeloid leukemia. We used a ROBOT template . Do you want to contribute? See tickets labeled good first issue . Documentation and more instructions are available in the Mondo editors guide .","title":"A Day in the Life of an Ontology Curator"},{"location":"reference/other_resources/","text":"Other Resources \u00b6 Here's a collection of links about the Open Biological and Biomedical Ontologies (OBO), and related topics. If you're completely new to OBO, I suggest starting with Ontologies 101 : Unit 1: Controlled Vocabularies, Ontologies, and Data Linking (PowerPoint Slides) Unit 2: An Introduction to OWL (Powerpoint Slides) Unit 3: Ontology Community (Powerpoint Slides) BDK14 Ontologies 101 repository If you're new to scientific computing more generally, then I strongly recommend Software Carpentry , which provides a set of very pragmatic introductions to the Unix command line, git, Python, Make, and other tools widely used by OBO developers. Open Biological and Biomedical Ontologies \u00b6 OBO is a community of people collaborating on open source ontologies for science. We have a set of shared principles and best practises to help people and data work together effectively. OBO Foundry Homepage The OBO Foundry: coordinated evolution of ontologies to support biomedical data integration (journal article) OBO Discuss mailing list Services \u00b6 Here is a very incomplete list of some excellent services to help you find an use OBO terms and ontologies. EMBL-EBI OLS: Ontology Lookup Service is an excellent ontology browser and search service OxO shows mappings between ontologies and terms Zooma for mapping free text to ontology terms Onto-Animals Ontobee is an ontology browser Ontofox is an ontology extraction tool Bioportal provides ontology browsing, search, mapping, etc. Tools \u00b6 This is the suite of open source software that most OBO developers use. OBO Tools mailing list GitHub is where most OBO projects are hosted and what we use to manage code, issues, etc. GitHub tutorial Prot\u00e9g\u00e9 is a graphical user interface for editing OWL ontologies. (Java) ROBOT is a command-line tool for automating ontology tasks. (Java) ROBOT tutorial ROBOT: A Tool for Automating Ontology Workflows (journal article) ENVO ROBOT Template and Merge Workflow DOS-DP is a command-line tool for working with ontology design patterns. (Python) ODK: Ontology Development Kit is a collection of tools for building and maintaining an OBO project. (Docker) OBO Tools and Workflows (Google Slides) A good overview of technical and advanced topics of OBO practises, including the Ontology Development Kit. OWLAPI is a Java library for working with ontologies, and is the foundation for Prot\u00e9g\u00e9 and ROBOT. OBO PURL System is used to redirect OBO terms from their IRIs to the right resource String of PURLs \u2013 frugal migration and maintenance of persistent identifiers (journal article) Technical \u00b6 This section is for technical reference, not beginners. OBO projects use Semantic Web and Linked Data technologies: W3C Semantic Web overview Search for W3C data standards W3C Data on the Web Best Practices These standards form layers: IRI: Internationalized Resource Identifiers are a superset of the familiar URLs used to locate resources on the web. Every ontology term has a globally unique IRI. RDF: Resource Description Format is a standard for combining IRIs into subject-predicate-object \"triples\" that make a statement about some thing. Sets of triples form a graph (i.e. network), and graphs can easily be merged to form larger graphs. SPARQL is the language for querying RDF graphs. RDF 1.1 Primer SPARQL 1.1 Overview RDFS: RDF Schema 1.1 extends RDF with classes, hierarchies, and other features. XSD: W3C XML Schema Definition Language (XSD) 1.1 Part 2: Datatypes is the common standard for datatypes in RDF OWL: Web Ontology Language extends RDF and RDFS to provide more powerful logic OWL 2 Web Ontology Language Primer (Second Edition) Other useful resources on technical topics: Monkeying around with OWL Chris Mungall's blog, mostly on technical topics for ontologies.","title":"Other useful resources"},{"location":"reference/other_resources/#other-resources","text":"Here's a collection of links about the Open Biological and Biomedical Ontologies (OBO), and related topics. If you're completely new to OBO, I suggest starting with Ontologies 101 : Unit 1: Controlled Vocabularies, Ontologies, and Data Linking (PowerPoint Slides) Unit 2: An Introduction to OWL (Powerpoint Slides) Unit 3: Ontology Community (Powerpoint Slides) BDK14 Ontologies 101 repository If you're new to scientific computing more generally, then I strongly recommend Software Carpentry , which provides a set of very pragmatic introductions to the Unix command line, git, Python, Make, and other tools widely used by OBO developers.","title":"Other Resources"},{"location":"reference/other_resources/#open-biological-and-biomedical-ontologies","text":"OBO is a community of people collaborating on open source ontologies for science. We have a set of shared principles and best practises to help people and data work together effectively. OBO Foundry Homepage The OBO Foundry: coordinated evolution of ontologies to support biomedical data integration (journal article) OBO Discuss mailing list","title":"Open Biological and Biomedical Ontologies"},{"location":"reference/other_resources/#services","text":"Here is a very incomplete list of some excellent services to help you find an use OBO terms and ontologies. EMBL-EBI OLS: Ontology Lookup Service is an excellent ontology browser and search service OxO shows mappings between ontologies and terms Zooma for mapping free text to ontology terms Onto-Animals Ontobee is an ontology browser Ontofox is an ontology extraction tool Bioportal provides ontology browsing, search, mapping, etc.","title":"Services"},{"location":"reference/other_resources/#tools","text":"This is the suite of open source software that most OBO developers use. OBO Tools mailing list GitHub is where most OBO projects are hosted and what we use to manage code, issues, etc. GitHub tutorial Prot\u00e9g\u00e9 is a graphical user interface for editing OWL ontologies. (Java) ROBOT is a command-line tool for automating ontology tasks. (Java) ROBOT tutorial ROBOT: A Tool for Automating Ontology Workflows (journal article) ENVO ROBOT Template and Merge Workflow DOS-DP is a command-line tool for working with ontology design patterns. (Python) ODK: Ontology Development Kit is a collection of tools for building and maintaining an OBO project. (Docker) OBO Tools and Workflows (Google Slides) A good overview of technical and advanced topics of OBO practises, including the Ontology Development Kit. OWLAPI is a Java library for working with ontologies, and is the foundation for Prot\u00e9g\u00e9 and ROBOT. OBO PURL System is used to redirect OBO terms from their IRIs to the right resource String of PURLs \u2013 frugal migration and maintenance of persistent identifiers (journal article)","title":"Tools"},{"location":"reference/other_resources/#technical","text":"This section is for technical reference, not beginners. OBO projects use Semantic Web and Linked Data technologies: W3C Semantic Web overview Search for W3C data standards W3C Data on the Web Best Practices These standards form layers: IRI: Internationalized Resource Identifiers are a superset of the familiar URLs used to locate resources on the web. Every ontology term has a globally unique IRI. RDF: Resource Description Format is a standard for combining IRIs into subject-predicate-object \"triples\" that make a statement about some thing. Sets of triples form a graph (i.e. network), and graphs can easily be merged to form larger graphs. SPARQL is the language for querying RDF graphs. RDF 1.1 Primer SPARQL 1.1 Overview RDFS: RDF Schema 1.1 extends RDF with classes, hierarchies, and other features. XSD: W3C XML Schema Definition Language (XSD) 1.1 Part 2: Datatypes is the common standard for datatypes in RDF OWL: Web Ontology Language extends RDF and RDFS to provide more powerful logic OWL 2 Web Ontology Language Primer (Second Edition) Other useful resources on technical topics: Monkeying around with OWL Chris Mungall's blog, mostly on technical topics for ontologies.","title":"Technical"},{"location":"reference/owl_formats_variants/","text":"OWL, OBO, JSON? Base, simple, full, basic? What should you use, and why? \u00b6 Ontologies come in different serialisations, formalisms, and variants For example, their are a full 9 (!) different release files associated with an ontology released using the default settings of the Ontology Development Kit , which causes a lot of confusion for current and prospective users. Note: In the OBO Foundry pages, \"variant\" is currently referred to as \"products\", i.e. the way we use \"variant\" here is synonymous with with notion of \"product\". Overview of the relevant concepts \u00b6 A formalism or formal language can be used to describe entities and their relationships in an ontology. The most important formalisms we have are: Web Ontology Language (OWL): OWL is by far the dominant formalism in the biomedical domain due to its inference capabilities. RDF(S): Is a generally weaker language than OWL, but widely used by triple stores and other SPARQL engines. RDF(S) is lacking some of the strong logical guarantees that come with OWL and should only be used in scenarios where scalability (computation time) is the primary concern. OBO : OBO used to be the dominant language in the biomedical domain before the advent of OWL. I also used to have its own specific semantics associated with it. OBO semantics have since been mapped into OWL semantics, so that for all practical purposes, we consider \"OBO\" now a dialect of OWL, which means that when you hear 'OBO format' today, we are generally referring to the serialisation (see below), NOT the formalism . Note that when we say OBO ontologies we mean literally Open Biomedical and Biological Ontologies , and NOT Ontology in OBO format . Some people like to also list SHACL and Shex as ontology languages and formalism. Formalisms define syntax (e.g. grammar rules) and semantics (what does what expression mean?). The analogue in the real world would be natural languages, like English or Greek. A format , or serialisation of a language is used to write down statements of a formal language in some way. Formats are not formalisms - they simply enable statements in a formalism to be expressed in some (usually textual) way. The most common formats in our domains are: RDF/XML . This is the default serialisation language of the OWL flavours of OBO ontologies. It is a pretty ugly format, really hard to understand by most users but it has one advantage - it can be understood widely by RDF-focused tools like rdflib , OWL-focused tools like those based on the OWL API OWL Functional Syntax : This is very common syntax for editing ontologies in OWL, because they look nice in diff tools such as git diff , i.e changes to ontologies in functional syntax are much easier to be reviewed. RDF/XML is not suitable for manual review, due to its verbosity and complexity. OWL Manchester Syntax : This is the default language for OWL tutorials and for writing class expressions in editors such as Protege OBO Format : The most easy to read of all the serialisations. In many ontologies such as Mondo and Uberon, we still use OBO as the editors format (as opposed to OWL Functional Syntax, which is more wide-spread). OBO format looks clear and beautiful in diffs such as git diffs, and therefore still continues to be wide-spread. OBO Format does not cover all of owl, and should only be used in conjunction with ontologies that stay within the limit of the OBO format specification . OBO Graphs JSON : A simple JSON serialisation of ontologies. This format roughly reflects the capabilities of the OBO format , but is intended for consumption by tools. Again, it does not cover all of OWL, but it does cover the parts that are relevant in 99% of the use cases. The real-world analogue of serialisation or format is the script, i.e. Latin or Cyrillic script (not quite clean analogue). A variant is a version of the ontology exported for a specific purpose. The most important variants are: Edit : The variant of the ontology that is edited by ontology curators. Its sole purpose is to be used by ontology editors, and should not be used by any other application . In a ODK-style repository, the edit file is typically located hidden from view, e.g. src/ontology/cl-edit.owl . Full : The ontology with all its imports merged in, and classified using a reasoner, see docs . The Full variant should be used by users that require the use of reasoners and a guarantee that all the inferences work as intended by the ontology developers. This is the default variant of most OBO ontologies. Base : The axioms belonging to the ontology, excluding any axioms from imported ontologies, see docs . Base variants are used by ontology repository developers to combine the latest versions of all ontologies in a way that avoids problems due to conflicting versions. Base files should not be used by users that want to use the ontology in downstream tools, such as annotation tools or scientific databases, as they are incomplete, i.e. not fully classified. Simple : A version of the ontology that only contains only a subset of the ontology (only the direct relations, see docs ). The simple variant should be used by most users that build tools that use the ontology, especially when serialised as OBO graphs json. This variant should probably be avoided by power-users working with reasoners, as many of the axioms that drive reasoning are missing. Basic : A variant of Simple , in that it is reduced to only a specific set of relations, such as subClassOf and partOf . Some users that require the ontology to correspond to acyclic graphs, or deliberately want to focus only on a set of core relations, will want to use this variant, see docs ). The formal definition of the basic variant can be found here . Other variants : Some variants are still used, like \"non-classified\", see docs ), but should be avoided. Others like base-plus , a variant that corresponds to base + the inferred axioms, are still under development, and will be explained here when they are fully developed. Best practices \u00b6 Tool developers developing tools that use the ontology (and do not need reasoners), such as database curation tools, web-browsers and similar, should typically use OBO graphs JSON and avoid using OBO format or any of the OWL focussed serialisations (Functional, Manchester or RDF/XML). OWL-focussed serialisations contain a huge deal of axiomatic content that make no sense to most users, and can lead to a variety of mistakes. We have seen it many times that software developers try to interpret OWL axioms somehow to extract relations . Do not do that! Work with the ontologies to ensure they provide the relationships you need in the appropriate form. Tool developers building tools to work with ontologies should typically ensure that they can read and write RDF/XML - this is the most widely understood serialisation. Work with ontologies means here 'enable operations that change the content of the ontology'. Tool developers building infrastructure to query across ontologies should consider using base variants - these ensure that you can always use the latest version of each ontology and avoid most of the common version clashes. It is important that such users are keenly aware of the role of OWL reasoning in such a process. Many users of ontologies think they need the reasoner actually don't . Make sure you consult with an expert before building a system that relies on OWL reasoners to deliver user facing services. As an ontology developer, it is great practice to provide the above variants in the common serialisations. The Ontology Development Kit provides defaults for all of these. As an ontology developer, you should avoid publishing your ontology with owl:imports statements - these are easily ignored by your users and make the intended \"content\" of the ontology quite none-transparent.","title":"OWL, OBO, JSON? Base, simple, full, basic? What should you use, and why?"},{"location":"reference/owl_formats_variants/#owl-obo-json-base-simple-full-basic-what-should-you-use-and-why","text":"Ontologies come in different serialisations, formalisms, and variants For example, their are a full 9 (!) different release files associated with an ontology released using the default settings of the Ontology Development Kit , which causes a lot of confusion for current and prospective users. Note: In the OBO Foundry pages, \"variant\" is currently referred to as \"products\", i.e. the way we use \"variant\" here is synonymous with with notion of \"product\".","title":"OWL, OBO, JSON? Base, simple, full, basic? What should you use, and why?"},{"location":"reference/owl_formats_variants/#overview-of-the-relevant-concepts","text":"A formalism or formal language can be used to describe entities and their relationships in an ontology. The most important formalisms we have are: Web Ontology Language (OWL): OWL is by far the dominant formalism in the biomedical domain due to its inference capabilities. RDF(S): Is a generally weaker language than OWL, but widely used by triple stores and other SPARQL engines. RDF(S) is lacking some of the strong logical guarantees that come with OWL and should only be used in scenarios where scalability (computation time) is the primary concern. OBO : OBO used to be the dominant language in the biomedical domain before the advent of OWL. I also used to have its own specific semantics associated with it. OBO semantics have since been mapped into OWL semantics, so that for all practical purposes, we consider \"OBO\" now a dialect of OWL, which means that when you hear 'OBO format' today, we are generally referring to the serialisation (see below), NOT the formalism . Note that when we say OBO ontologies we mean literally Open Biomedical and Biological Ontologies , and NOT Ontology in OBO format . Some people like to also list SHACL and Shex as ontology languages and formalism. Formalisms define syntax (e.g. grammar rules) and semantics (what does what expression mean?). The analogue in the real world would be natural languages, like English or Greek. A format , or serialisation of a language is used to write down statements of a formal language in some way. Formats are not formalisms - they simply enable statements in a formalism to be expressed in some (usually textual) way. The most common formats in our domains are: RDF/XML . This is the default serialisation language of the OWL flavours of OBO ontologies. It is a pretty ugly format, really hard to understand by most users but it has one advantage - it can be understood widely by RDF-focused tools like rdflib , OWL-focused tools like those based on the OWL API OWL Functional Syntax : This is very common syntax for editing ontologies in OWL, because they look nice in diff tools such as git diff , i.e changes to ontologies in functional syntax are much easier to be reviewed. RDF/XML is not suitable for manual review, due to its verbosity and complexity. OWL Manchester Syntax : This is the default language for OWL tutorials and for writing class expressions in editors such as Protege OBO Format : The most easy to read of all the serialisations. In many ontologies such as Mondo and Uberon, we still use OBO as the editors format (as opposed to OWL Functional Syntax, which is more wide-spread). OBO format looks clear and beautiful in diffs such as git diffs, and therefore still continues to be wide-spread. OBO Format does not cover all of owl, and should only be used in conjunction with ontologies that stay within the limit of the OBO format specification . OBO Graphs JSON : A simple JSON serialisation of ontologies. This format roughly reflects the capabilities of the OBO format , but is intended for consumption by tools. Again, it does not cover all of OWL, but it does cover the parts that are relevant in 99% of the use cases. The real-world analogue of serialisation or format is the script, i.e. Latin or Cyrillic script (not quite clean analogue). A variant is a version of the ontology exported for a specific purpose. The most important variants are: Edit : The variant of the ontology that is edited by ontology curators. Its sole purpose is to be used by ontology editors, and should not be used by any other application . In a ODK-style repository, the edit file is typically located hidden from view, e.g. src/ontology/cl-edit.owl . Full : The ontology with all its imports merged in, and classified using a reasoner, see docs . The Full variant should be used by users that require the use of reasoners and a guarantee that all the inferences work as intended by the ontology developers. This is the default variant of most OBO ontologies. Base : The axioms belonging to the ontology, excluding any axioms from imported ontologies, see docs . Base variants are used by ontology repository developers to combine the latest versions of all ontologies in a way that avoids problems due to conflicting versions. Base files should not be used by users that want to use the ontology in downstream tools, such as annotation tools or scientific databases, as they are incomplete, i.e. not fully classified. Simple : A version of the ontology that only contains only a subset of the ontology (only the direct relations, see docs ). The simple variant should be used by most users that build tools that use the ontology, especially when serialised as OBO graphs json. This variant should probably be avoided by power-users working with reasoners, as many of the axioms that drive reasoning are missing. Basic : A variant of Simple , in that it is reduced to only a specific set of relations, such as subClassOf and partOf . Some users that require the ontology to correspond to acyclic graphs, or deliberately want to focus only on a set of core relations, will want to use this variant, see docs ). The formal definition of the basic variant can be found here . Other variants : Some variants are still used, like \"non-classified\", see docs ), but should be avoided. Others like base-plus , a variant that corresponds to base + the inferred axioms, are still under development, and will be explained here when they are fully developed.","title":"Overview of the relevant concepts"},{"location":"reference/owl_formats_variants/#best-practices","text":"Tool developers developing tools that use the ontology (and do not need reasoners), such as database curation tools, web-browsers and similar, should typically use OBO graphs JSON and avoid using OBO format or any of the OWL focussed serialisations (Functional, Manchester or RDF/XML). OWL-focussed serialisations contain a huge deal of axiomatic content that make no sense to most users, and can lead to a variety of mistakes. We have seen it many times that software developers try to interpret OWL axioms somehow to extract relations . Do not do that! Work with the ontologies to ensure they provide the relationships you need in the appropriate form. Tool developers building tools to work with ontologies should typically ensure that they can read and write RDF/XML - this is the most widely understood serialisation. Work with ontologies means here 'enable operations that change the content of the ontology'. Tool developers building infrastructure to query across ontologies should consider using base variants - these ensure that you can always use the latest version of each ontology and avoid most of the common version clashes. It is important that such users are keenly aware of the role of OWL reasoning in such a process. Many users of ontologies think they need the reasoner actually don't . Make sure you consult with an expert before building a system that relies on OWL reasoners to deliver user facing services. As an ontology developer, it is great practice to provide the above variants in the common serialisations. The Ontology Development Kit provides defaults for all of these. As an ontology developer, you should avoid publishing your ontology with owl:imports statements - these are easily ignored by your users and make the intended \"content\" of the ontology quite none-transparent.","title":"Best practices"},{"location":"reference/reasoning/","text":"Why do we need reasoning? \u00b6 A quick personal perspective up-front. When I was finishing my undergrad, I barely had heard the term Semantic Web. What I had heard vaguely intrigued me, so I decided that for my final project, I would try to combine something Semantic Web related with my other major, Law and build a tool that could automatically infer the applicability of a law (written in OWL) given a legal case. Super naively, I just went went ahead, read a few papers about legal ontologies, build a simple one, loaded it into my application and somehow got it to work, with reasoning and all, without even having heard of Description Logic. In my PhD, I worked on actual reasoning algorithms, which meant, no more avoiding logic. But - I did not get it. Up until this point in my life, I could just study harder and harder, and in the end I was confident with what I learned, but First Order Logic, in particular model theory and proofs, caused me anxiety until the date of my viva. In the end, a very basic understanding of model theory and Tableau did help me with charactering the algorithms I was working with (I was studying the effect of modularity, cutting out logically connected subsets of an ontology, on reasoning performance) but I can confidently say today: I never really, like deeply, understood logical proofs. I still cant read them - and I have a PhD in Reasoning (albeit from an empirical angle). If you followed the Open HPI courses on logic, and you are anything like me, your head will hurt and you will want to hide under your blankets. Most students feel like that. For a complete education in Semantic Web technologies, going through this part once is essential: it tells you something about how difficult some stuff is under the hood, and how much work has been done to make something like OWL work for knowledge representation. You should have gained some appreciation of the domain, which is no less complex than Machine Learning or Stochastic Processes. But, in my experience, some of the most effective ontology engineers barely understand reasoning - definitely have no idea how it works - and still do amazing work. In that spirit, I would like to invite you at this stage to put logic and reasoning behind you (unless it made you curious of course) - you won't need to know much of that for being an effective Semantic Engineer. In the following, I will summarise some of the key take-aways that I find useful to keep in mind. Semantics define how to interpret an ontology . For example, in OWL, the statement Human SubClassOf: Mammal means that all instances of the Human class, like me, are also instances of the Mammal class. Or, in other words, from the statements: Human SubClassOf: Mammal Nico type: Human Semantics allow as to deduce that Nico:Mammal . What are semantics practically ? Show me your semantics? Look at something like the OWL semantics . In there, you will find language statements (syntax) like X SubClassOf: Y and a bunch of formulae from model theory that describe how to interpret it - no easy read, and not really important for you now. - OWL has a number of profiles , basically sub-languages where you can say less things. Why would we want to restrict our \"expressivity\"? Because their is a trade-off. An important slide I remember from when I learned about ontology languages was the triangle of complexity (here only paraphrased from memory): When expressivity goes up, cognitive complexity and computational complexity go up. When we want to decrease cognitive complexity (make it easier to build ontologies), expressivity goes down. When we want reasoners to be faster at making inferences (computational complexity), we need to decrease expressivity. So we need to find a way to balance. What are the most important practical applications of reasoning? There are many, and there will be many opinions, but in the OBO world, by far (95%) of all uses of reasoners pertain to the following: Classification. Most, if not all, of our ontologies are conceptually hierarchies of classes we use reasoners to automatically infer hierarchies. Look for example at the Xenopus Phenotype Ontology - the class hierarchy is entirely build with a reasoner - no Human intervention! Debugging. There are two major threats to ontologies. In the worst case, they can be inconsistent - which means, totally broken. A slightly less bad, but still undesirable situation is that some of the classes in your ontologies break (in parlance, become unsatisfiable). This happens when you say some contradictory things about them. Reasoners help you find these unsatisfiable classes, and there is a special reasoning algorithm that can generate an explanation for you - to help fixing your problem. So in general, what is reasoning? There are probably a dozen or more official characterisations in the scientific literature, but from the perspective of biomedical ontologies, the question can be roughly split like this: How can we capture what we know? This is the (research-) area of knowledge representation, logical formalisms, such as First Order Logic, Description Logic, etc. It is concerned with how we write down what we now: All cars have four wheels If you are a human, you are also a mammal If you are a bird, you can fly (unless you are a penguin) How can we uncover implicit knowledge efficiently? This is the area of reasoning, and while being closely related to the formalisms above, it makes sense to think of them in a distinct manner, as the problems are very different in practice. It can typically be grouped into the following two categories: deductive reasoning infers by Lets think about a naive approach: using a fact-, or data-, base.","title":"Reasoning"},{"location":"reference/reasoning/#why-do-we-need-reasoning","text":"A quick personal perspective up-front. When I was finishing my undergrad, I barely had heard the term Semantic Web. What I had heard vaguely intrigued me, so I decided that for my final project, I would try to combine something Semantic Web related with my other major, Law and build a tool that could automatically infer the applicability of a law (written in OWL) given a legal case. Super naively, I just went went ahead, read a few papers about legal ontologies, build a simple one, loaded it into my application and somehow got it to work, with reasoning and all, without even having heard of Description Logic. In my PhD, I worked on actual reasoning algorithms, which meant, no more avoiding logic. But - I did not get it. Up until this point in my life, I could just study harder and harder, and in the end I was confident with what I learned, but First Order Logic, in particular model theory and proofs, caused me anxiety until the date of my viva. In the end, a very basic understanding of model theory and Tableau did help me with charactering the algorithms I was working with (I was studying the effect of modularity, cutting out logically connected subsets of an ontology, on reasoning performance) but I can confidently say today: I never really, like deeply, understood logical proofs. I still cant read them - and I have a PhD in Reasoning (albeit from an empirical angle). If you followed the Open HPI courses on logic, and you are anything like me, your head will hurt and you will want to hide under your blankets. Most students feel like that. For a complete education in Semantic Web technologies, going through this part once is essential: it tells you something about how difficult some stuff is under the hood, and how much work has been done to make something like OWL work for knowledge representation. You should have gained some appreciation of the domain, which is no less complex than Machine Learning or Stochastic Processes. But, in my experience, some of the most effective ontology engineers barely understand reasoning - definitely have no idea how it works - and still do amazing work. In that spirit, I would like to invite you at this stage to put logic and reasoning behind you (unless it made you curious of course) - you won't need to know much of that for being an effective Semantic Engineer. In the following, I will summarise some of the key take-aways that I find useful to keep in mind. Semantics define how to interpret an ontology . For example, in OWL, the statement Human SubClassOf: Mammal means that all instances of the Human class, like me, are also instances of the Mammal class. Or, in other words, from the statements: Human SubClassOf: Mammal Nico type: Human Semantics allow as to deduce that Nico:Mammal . What are semantics practically ? Show me your semantics? Look at something like the OWL semantics . In there, you will find language statements (syntax) like X SubClassOf: Y and a bunch of formulae from model theory that describe how to interpret it - no easy read, and not really important for you now. - OWL has a number of profiles , basically sub-languages where you can say less things. Why would we want to restrict our \"expressivity\"? Because their is a trade-off. An important slide I remember from when I learned about ontology languages was the triangle of complexity (here only paraphrased from memory): When expressivity goes up, cognitive complexity and computational complexity go up. When we want to decrease cognitive complexity (make it easier to build ontologies), expressivity goes down. When we want reasoners to be faster at making inferences (computational complexity), we need to decrease expressivity. So we need to find a way to balance. What are the most important practical applications of reasoning? There are many, and there will be many opinions, but in the OBO world, by far (95%) of all uses of reasoners pertain to the following: Classification. Most, if not all, of our ontologies are conceptually hierarchies of classes we use reasoners to automatically infer hierarchies. Look for example at the Xenopus Phenotype Ontology - the class hierarchy is entirely build with a reasoner - no Human intervention! Debugging. There are two major threats to ontologies. In the worst case, they can be inconsistent - which means, totally broken. A slightly less bad, but still undesirable situation is that some of the classes in your ontologies break (in parlance, become unsatisfiable). This happens when you say some contradictory things about them. Reasoners help you find these unsatisfiable classes, and there is a special reasoning algorithm that can generate an explanation for you - to help fixing your problem. So in general, what is reasoning? There are probably a dozen or more official characterisations in the scientific literature, but from the perspective of biomedical ontologies, the question can be roughly split like this: How can we capture what we know? This is the (research-) area of knowledge representation, logical formalisms, such as First Order Logic, Description Logic, etc. It is concerned with how we write down what we now: All cars have four wheels If you are a human, you are also a mammal If you are a bird, you can fly (unless you are a penguin) How can we uncover implicit knowledge efficiently? This is the area of reasoning, and while being closely related to the formalisms above, it makes sense to think of them in a distinct manner, as the problems are very different in practice. It can typically be grouped into the following two categories: deductive reasoning infers by Lets think about a naive approach: using a fact-, or data-, base.","title":"Why do we need reasoning?"},{"location":"reference/semantic_engineering_toolbox/","text":"The Semantic OBO Engineer's Toolbox \u00b6 Essentials Prot\u00e9g\u00e9 DL Query Tab ROBOT OBO Dashboard : OBO-wide quality control monitor for OBO ontologies. Automation GNU Make Ontology Development Kit (ODK) DROID : DROID is a web-based interface for working with make , managed by git . Text editors: Kakoune text/code editor Sublime Atom SPARQL query tool: Yasgui ROBOT query SPARQL endpoints Ontobee SPARQL endpoint : Useful to run queries across all OBO Foundry ontologies Ubergraph SPARQL endpoint : Many key OBO ontologies are loaded here with lots of materialised inferences ( docs ). Templating systems DOSDP ROBOT template Ontology Mappings SSSOM and sssom-py : Toolkit and framework for managing mappings across and beyond ontologies. Where to find ontologies and terms: Term browsers and ontology repositories OLS : The boss of the current term browsers out there. While the code base is a bit dated, it still gives access to a wide range of relevant open biomedical ontology terms. Note, while being a bit painful, it is possible to set up your own OLS (for your organisation) which only contains those terms/ontologies that are relevant for your work. Ontobee : The default term browser for OBO term purls. For example, click on http://purl.obolibrary.org/obo/OBI_0000070. This will redirect you directly to Ontobee, to show you the terms location in the hierarchy. In practice, there is no particular reason why you would favour Ontobee over OLS for example - I just sometimes prefer the way Ontobee presents annotations and \"uses\" by other ontologies, so I use both. AberOWL : Another ontology repository and semantic search engine. Some ontologies such as PhenomeNet can only be found on AberOWL, however, I personally prefer OLS. identifiers.org : A centralised registry for identifiers used in the life sciences. This is one of the tools that bridge the gap between CURIEs and URLs, but it does not cover (OBO) ontologies very well, and if so, is not aware of the proper URI prefixes (see for example here , and HP term resolution that does not list the proper persistent URL of the HP identifier (http://purl.obolibrary.org/obo/HP_0000001)). Identifiers.org has mainly good coverage for databases/resources that use CURIE type identifiers. But: you can enter any ID you find in your data and it will tell you what it is associated with. OBO Foundry Ontology Library . The OBO Foundry works with other repositories and term browsers such as OLS, Ontobee and BioPortal. For example, OLS directly reads the OBO Foundry registry metadata, and automatically loads new ontologies added to the OBO Foundry Ontology Library. BioPortal CPT Story . The Current Procedural Terminology was the by far most highly accessed Terminology on Bioportal - for many years. Due to license concerns, it had to be withdrawn from the repository. This story serves a cautionary tale of using terminologies with non-open or non-transparent licensing schemes. AgroPortal : Like BioPortal, but focussed on the Agronomy domain. Linked Open Data Vocabularies (LOV) : Lists the most important vocabularies in the Linked Data space, such as Dublin Core , SKOS and Friend-of-a-Friend (FOAF). Ontology visualisation OBO Graphviz : Library to visualise ontologies in beautifully readable graphics based on Dot . Nico's top 10 tools for the Semantic OBO Engineer's Toolbox \u00b6 ROBOT Prot\u00e9g\u00e9 Term browsers ( OLS , Ontobee ) Ontology Development Kit (ODK) SPARQL (e.g. ROBOT query and Yasgui ) GNU Make Text editor workflows (i.e. Atom , Sublime, VIM): a bit of regex Basic Shell scripting and pipelining From tables to ontologies: DOSDP templates and ROBOT templates GitHub Actions Other tools in my toolbox These are a bit less essential than the above, but I consider them still tremendously useful. Cogs (experimental) for automatically synchronising your spreadsheets with Google Sheets. Basic Dockerfile development : This can help you automate processes that go beyond usual ODK day-to-day business, such as automated mapping tools, graph machine learning, NLP etc. GitHub community management and git version control: Learn how to effectively manage your contributors, issue requests and code reviews. Also get your git commands straight - these can be life savers! Basics in python scripting : This is always useful, and python is our go-to language for most of automation nowadays - this used to be Java. Most of the Java heavy lifting is done in ROBOT now! SSSOM and sssom-py : Toolkit and framework for managing mappings between ontologies. DROID : DROID is a web-based interface for working with make , managed by git . OBO Dashboard : OBO-wide quality control monitor for OBO ontologies.","title":"Semantic Engineering Toolbox"},{"location":"reference/semantic_engineering_toolbox/#the-semantic-obo-engineers-toolbox","text":"Essentials Prot\u00e9g\u00e9 DL Query Tab ROBOT OBO Dashboard : OBO-wide quality control monitor for OBO ontologies. Automation GNU Make Ontology Development Kit (ODK) DROID : DROID is a web-based interface for working with make , managed by git . Text editors: Kakoune text/code editor Sublime Atom SPARQL query tool: Yasgui ROBOT query SPARQL endpoints Ontobee SPARQL endpoint : Useful to run queries across all OBO Foundry ontologies Ubergraph SPARQL endpoint : Many key OBO ontologies are loaded here with lots of materialised inferences ( docs ). Templating systems DOSDP ROBOT template Ontology Mappings SSSOM and sssom-py : Toolkit and framework for managing mappings across and beyond ontologies. Where to find ontologies and terms: Term browsers and ontology repositories OLS : The boss of the current term browsers out there. While the code base is a bit dated, it still gives access to a wide range of relevant open biomedical ontology terms. Note, while being a bit painful, it is possible to set up your own OLS (for your organisation) which only contains those terms/ontologies that are relevant for your work. Ontobee : The default term browser for OBO term purls. For example, click on http://purl.obolibrary.org/obo/OBI_0000070. This will redirect you directly to Ontobee, to show you the terms location in the hierarchy. In practice, there is no particular reason why you would favour Ontobee over OLS for example - I just sometimes prefer the way Ontobee presents annotations and \"uses\" by other ontologies, so I use both. AberOWL : Another ontology repository and semantic search engine. Some ontologies such as PhenomeNet can only be found on AberOWL, however, I personally prefer OLS. identifiers.org : A centralised registry for identifiers used in the life sciences. This is one of the tools that bridge the gap between CURIEs and URLs, but it does not cover (OBO) ontologies very well, and if so, is not aware of the proper URI prefixes (see for example here , and HP term resolution that does not list the proper persistent URL of the HP identifier (http://purl.obolibrary.org/obo/HP_0000001)). Identifiers.org has mainly good coverage for databases/resources that use CURIE type identifiers. But: you can enter any ID you find in your data and it will tell you what it is associated with. OBO Foundry Ontology Library . The OBO Foundry works with other repositories and term browsers such as OLS, Ontobee and BioPortal. For example, OLS directly reads the OBO Foundry registry metadata, and automatically loads new ontologies added to the OBO Foundry Ontology Library. BioPortal CPT Story . The Current Procedural Terminology was the by far most highly accessed Terminology on Bioportal - for many years. Due to license concerns, it had to be withdrawn from the repository. This story serves a cautionary tale of using terminologies with non-open or non-transparent licensing schemes. AgroPortal : Like BioPortal, but focussed on the Agronomy domain. Linked Open Data Vocabularies (LOV) : Lists the most important vocabularies in the Linked Data space, such as Dublin Core , SKOS and Friend-of-a-Friend (FOAF). Ontology visualisation OBO Graphviz : Library to visualise ontologies in beautifully readable graphics based on Dot .","title":"The Semantic OBO Engineer's Toolbox"},{"location":"reference/semantic_engineering_toolbox/#nicos-top-10-tools-for-the-semantic-obo-engineers-toolbox","text":"ROBOT Prot\u00e9g\u00e9 Term browsers ( OLS , Ontobee ) Ontology Development Kit (ODK) SPARQL (e.g. ROBOT query and Yasgui ) GNU Make Text editor workflows (i.e. Atom , Sublime, VIM): a bit of regex Basic Shell scripting and pipelining From tables to ontologies: DOSDP templates and ROBOT templates GitHub Actions Other tools in my toolbox These are a bit less essential than the above, but I consider them still tremendously useful. Cogs (experimental) for automatically synchronising your spreadsheets with Google Sheets. Basic Dockerfile development : This can help you automate processes that go beyond usual ODK day-to-day business, such as automated mapping tools, graph machine learning, NLP etc. GitHub community management and git version control: Learn how to effectively manage your contributors, issue requests and code reviews. Also get your git commands straight - these can be life savers! Basics in python scripting : This is always useful, and python is our go-to language for most of automation nowadays - this used to be Java. Most of the Java heavy lifting is done in ROBOT now! SSSOM and sssom-py : Toolkit and framework for managing mappings between ontologies. DROID : DROID is a web-based interface for working with make , managed by git . OBO Dashboard : OBO-wide quality control monitor for OBO ontologies.","title":"Nico's top 10 tools for the Semantic OBO Engineer's Toolbox"},{"location":"reference/tables-and-triples/","text":"Tables and Triples \u00b6 Tables and triples seem very different. Tables are familiar and predictable. Triples are weird and floppy. SQL is normal, SPARQL is bizarre, at least at first. Tables are great, and they're the right tool for a lot of jobs, but they have their limitations. Triples shine when it comes to merging heterogeneous data. But it turns out that there's a clear path from tables to triples, which should help make RDF make more sense. Tables \u00b6 Tables are great! Here's a table! first_name last_name Luke Skywalker Leia Organa Darth Vader Han Solo You won't be surprised to find out that tables have rows and columns . Often each row corresponds to some thing that we want to talk about, such as a fictional character from Star Wars . Each column usually corresponds to some sort of property that those things might have. Then the cells contain the values of those properties for their respective row. We take some sort of complex information about the world, and we break it down along two dimensions: the things (rows) and their properties (columns). Primary Keys \u00b6 Tables are great! We can add another name to our table: first_name last_name Luke Skywalker Leia Organa Darth Vader Han Solo Anakin Skywalker Hmm. That's a perfectly good table, but it's not capturing the information that we wanted. It turns out ( Spoiler Alert! ) that Anakin Skywalker is Darth Vader! We might have thought that the rows of our table were describing individual people, but it turns out that they're just describing individual names. A person can change their name or have more than one name. We want some sort of identifier that lets us pick out the same person, and distinguish them from all the other people. Sometimes there's a \"natural key\" that we can use for this purpose: some bit of information that uniquely identifies a thing. When we don't have a natural key, we can generate an \"artificial key\". Random strings and number can be good artificial keys, but sometimes a simple incrementing integer is good enough. The main problem with artificial keys is that it's our job to maintain the link between the thing and the identifier that we gave it. We prefer natural keys because we just have to inspect that thing (in some way) to figure out what to call it. Even when it's possible, sometimes that's too much work. Maybe we could use a DNA sequence as a natural key for a person, but it probably isn't practical. We do use fingerprints and facial recognition, for similar things, though. (Do people in Star Wars even have DNA? Or just midichlorions?) Let's add a column with an artificial key to our table: sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo 3 Anakin Skywalker This is our table of names, allowing a given person to have multiple names. But what we thought we wanted was a person table with one row for each person, like this: sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo In SQL we could assert that the \"sw_id\" column of the person table is a PRIMARY KEY. This means it must be unique. (It probably shouldn't be NULL either!) The names in the person table could be the primary names that we use in our Star Wars database system, and we could have another alternative_name table: sw_id first_name last_name 3 Anakin Skywalker Holes \u00b6 Tables are great! We can add more columns to our person table: sw_id first_name last_name occupation 1 Luke Skywalker Jedi 2 Leia Organa princess 3 Darth Vader 4 Han Solo scoundrel The 2D pattern of a table is a strong one. It not only provides a \"slot\" (cell) for every combination of row and column, it also makes it very obvious when one of those slots is empty . What does it mean for a slot to be empty? It could mean many things. For example, in the previous table in the row for Darth Vader, the cell for the \"occupation\" column is empty. This could mean that: we don't know whether he has an occupation we know that he has an occupation, but we don't know which occupation it is. we might know, but we haven't bothered to write it down yet we might know, but it doesn't fit nicely into the New Republic Standard Registry of Occupations; in other words, we know what his occupation is, but including it here would violate a constraint on our database we specifically know that he doesn't have an occupation; we triple-checked we know more generally ( Spoiler Alert!! ) that he's dead, and dead people can't have an occupation. I'm sure I haven't captured all the possibilities. The point is that there's lot of possible reasons why a cell would be blank. So what can we do about it? If our table is stored in a SQL database, then we have the option of putting a NULL value in the cell. NULL is pretty strange. It isn't TRUE and it isn't FALSE. Usually NULL values are excluded from SQL query results unless you are careful to ask for them. The way that NULL works in SQL eliminates some of the possibilities above. SQL uses the \"closed-world assumption\", which is the assumption that if a statement is true then it's known to be true, and conversely that if it's not known to be true then it's false. So if Anakin's occupation is NULL in a SQL database, then as far as SQL is concerned, we must know that he doesn't have an occupation. That might not be what you were expecting! The Software Carpentry module on Missing Data has more information. Multiple Values \u00b6 Tables are great! Let's add even more information to our table: sw_id first_name last_name occupation enemy 1 Luke Skywalker Jedi 3 2 Leia Organa princess 3 3 Darth Vader 1,2,4 4 Han Solo scoundrel 3 We're trying to say that Darth Vader is the enemy of everybody else in our table. We're using the primary key of the person in the enemy column, which is good, but we've ended up with multiple values in the \"enemy\" column for Darth Vader. In any table or SQL database you could make the \"enemy\" column a string, pick a delimiter such as the comma, and concatenate your values into a comma-separated list. This works, but not very well. In some SQL databases, such as Postgres, you could given the \"enemy\" column an array type, so it can contain multiple values. You get special operators for querying inside arrays. This can work pretty well. The usual advice is to break this \"one to many\" information into a new \"enemy\" table: sw_id enemy 1 3 2 3 3 1 3 2 3 4 4 1 Then you can JOIN the person table to the enemy table as needed. Sparse Tables \u00b6 Tables are great! Let's add even more information to our table: sw_id first_name last_name occupation father lightsaber_color ship 1 Luke Skywalker Jedi 3 green 2 Leia Organa princess 3 3 Darth Vader red 4 Han Solo scoundrel Millennium Falcon A bunch of these columns only apply to a few rows. Now we've got a lot more NULLs to deal with. As the number of columns increases, this can become a problem. Property Tables \u00b6 Tables are great! If sparse tables are a problem, then let's try to apply the same solution that worked for the \"many to one\" problem in the previous section. name table: sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo 3 Anakin Skywalker occupation table: sw_id occupation 1 Jedi 2 princess 4 scoundrel enemy table: sw_id enemy 1 3 2 3 3 1 3 2 3 4 4 1 father table: sw_id father 1 3 2 3 lightsaber_color table: sw_id lightsaber_color 1 green 3 red ship table: sw_id ship 4 Millennium Falcon Hmm. Yeah, that will work. But every query we write will need some JOINs. It feels like we've lost something. Entity, Attribute, Value \u00b6 Tables are great! But there's such a thing as too many tables. We started out with a table with a bunch of rows and a bunch of columns, and ended up with a bunch of tables with a bunch of rows but just a few columns. I have a brilliant idea! Let's combine all these property tables into just one table, by adding a \"property\" column! sw_id property value 1 first_name Luke 2 first_name Leia 3 first_name Darth 4 first_name Han 5 first_name Anakin 1 last_name Skywalker 2 last_name Skywalker 3 last_name Vader 4 last_name Solo 5 last_name Skywalker 1 occupation Jedi 2 occupation princess 4 occupation scoundrel 1 enemy 3 2 enemy 3 3 enemy 1 3 enemy 2 3 enemy 4 4 enemy 1 1 father 3 2 father 3 1 lightsaber_color green 3 lightsaber_color red 4 ship Millenium Falcon It turns out that I'm not the first one to think of this idea. People call it \"Entity, Attribute, Value\" or \"EAV\". People also call it an \"anti-pattern\", in other words: a clear sign that you've made a terrible mistake. There are lots of circumstances in which one big, extremely generic table is a bad idea. First of all, you can't do very much with the datatypes for the property and value columns. They kind of have to be strings. It's potentially difficult to index. And tables like this are miserable to query, because you end up with all sorts of self-joins to handle. But there's at least one use case where it turns out to work quite well... Merging Tables \u00b6 Tables are great! Until they're not. The strong row and column structure of tables makes them great for lots of things, but not so great for merging data from different sources. Before you can merge two tables you need to know all about: how the columns are structured what the rows mean what the cells mean So you need to know the schemas of the two tables before you can start merging them together. But if you happen to have two EAV tables then, as luck would have it, they already have the same schema! You also need to know that you're talking about the same things: the rows have to be about the same things, you need to be using the same property names for the same things, and the cell values also need to line up. If only there was an open standard for specifying globally unique identifiers... Yes, you guessed it: URLs (and URNs and URIs and IRIs)! Let's assume that we use the same URLs for the same things across the two tables. Since we're a close-knit community, we've come to an agreement on a Star Wars data vocabulary. URLs are annoyingly long to use in databases, so let's use standard \"sw\" prefix to shorten them. Now we have table 1: sw_id property value sw:1 sw:first_name Luke sw:2 sw:first_name Leia sw:3 sw:first_name Darth sw:4 sw:first_name Han sw:5 sw:first_name Anakin sw:1 sw:last_name Skywalker sw:2 sw:last_name Skywalker sw:3 sw:last_name Vader sw:4 sw:last_name Solo sw:5 sw:last_name Skywalker sw:1 sw:occupation sw:Jedi sw:2 sw:occupation sw:princess sw:4 sw:occupation sw:scoundrel and table 2: sw_id property value sw:1 sw:enemy sw:3 sw:2 sw:enemy sw:3 sw:3 sw:enemy sw:1 sw:3 sw:enemy sw:2 sw:3 sw:enemy sw:4 sw:4 sw:enemy sw:1 sw:1 sw:father sw:3 sw:2 sw:father sw:3 sw:1 sw:lightsaber_color green sw:3 sw:lightsaber_color red sw:4 sw:ship Millenium Falcon To merge these two tables, we simple concatenate them. It couldn't be simpler. Wait, this looks kinda familiar... RDF \u00b6 These tables are pretty much in RDF format. You just have to squint a little! sw_id == subject property == predicate value == object Each row of the table is a subject-predicate-object triple. Our subjects, predicates, and some objects are URLs. We also have some literal objects. We could turn this table directly into Turtle format with a little SQL magic (basically just concatenating strings): SELECT \"@prefix sw: <http://example.com/sw_> .\" UNION ALL SELECT \"\" UNION ALL SELECT sw_id || \" \" || property || \" \" || IF( INSTR(value, \":\"), value, -- CURIE \"\"\"\" || value || \"\"\"\" -- literal ) || \" .\" FROM triple_table; The first few lines will look like this: @prefix sw: <http://example.com/sw_> . sw:1 sw:first_name \"Luke\" . sw:2 sw:first_name \"Leia\" . sw:3 sw:first_name \"Darth\" . sw:4 sw:first_name \"Han\" . Two things we're missing from RDF are language tagged literals and typed literals. We also haven't used any blank nodes in our triple table. These are easy enough to add. The biggest thing that's different about RDF is that it uses the \"open-world assumption\", so something may be true even though we don't have a triple asserting that it's true. The open-world assumption is a better fit than the closed-world assumption when we're integrating data on the Web. Conclusion \u00b6 Tables are great! We use them all the time, they're strong and rigid, and we're comfortable with them. RDF, on the other hand, looks strange at first. For most common data processing, RDF is too flexible. But sometimes flexiblity is the most important thing. The greatest strength of tables is their rigid structure, but that's also their greatest weakness. We saw a number of problems with tables, and how they could be overcome by breaking tables apart into smaller tables, until we got down to the most basic pattern: subject-predicate-object. Step by step, we were pushed toward RDF. Merging tables is particularly painful. When working with data on the Web, merging is one of the most common and important operations, and so it makes sense to use RDF for these tasks. If self-joins with SQL is the worst problem for EAV tables, then SPARQL solves it. These examples show that it's not really very hard to convert tables to triples. And once you've seen SPARQL, the RDF query language, you've seen one good way to convert triples to tables: SPARQL SELECT results are just tables! Since it's straightforward to convert tables to triples and back again, make sure to use the right tool for the right job. When you need to merge heterogeneous data, reach for triples. For most other data processing tasks, use tables. They're great!","title":"Tables and Triples"},{"location":"reference/tables-and-triples/#tables-and-triples","text":"Tables and triples seem very different. Tables are familiar and predictable. Triples are weird and floppy. SQL is normal, SPARQL is bizarre, at least at first. Tables are great, and they're the right tool for a lot of jobs, but they have their limitations. Triples shine when it comes to merging heterogeneous data. But it turns out that there's a clear path from tables to triples, which should help make RDF make more sense.","title":"Tables and Triples"},{"location":"reference/tables-and-triples/#tables","text":"Tables are great! Here's a table! first_name last_name Luke Skywalker Leia Organa Darth Vader Han Solo You won't be surprised to find out that tables have rows and columns . Often each row corresponds to some thing that we want to talk about, such as a fictional character from Star Wars . Each column usually corresponds to some sort of property that those things might have. Then the cells contain the values of those properties for their respective row. We take some sort of complex information about the world, and we break it down along two dimensions: the things (rows) and their properties (columns).","title":"Tables"},{"location":"reference/tables-and-triples/#primary-keys","text":"Tables are great! We can add another name to our table: first_name last_name Luke Skywalker Leia Organa Darth Vader Han Solo Anakin Skywalker Hmm. That's a perfectly good table, but it's not capturing the information that we wanted. It turns out ( Spoiler Alert! ) that Anakin Skywalker is Darth Vader! We might have thought that the rows of our table were describing individual people, but it turns out that they're just describing individual names. A person can change their name or have more than one name. We want some sort of identifier that lets us pick out the same person, and distinguish them from all the other people. Sometimes there's a \"natural key\" that we can use for this purpose: some bit of information that uniquely identifies a thing. When we don't have a natural key, we can generate an \"artificial key\". Random strings and number can be good artificial keys, but sometimes a simple incrementing integer is good enough. The main problem with artificial keys is that it's our job to maintain the link between the thing and the identifier that we gave it. We prefer natural keys because we just have to inspect that thing (in some way) to figure out what to call it. Even when it's possible, sometimes that's too much work. Maybe we could use a DNA sequence as a natural key for a person, but it probably isn't practical. We do use fingerprints and facial recognition, for similar things, though. (Do people in Star Wars even have DNA? Or just midichlorions?) Let's add a column with an artificial key to our table: sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo 3 Anakin Skywalker This is our table of names, allowing a given person to have multiple names. But what we thought we wanted was a person table with one row for each person, like this: sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo In SQL we could assert that the \"sw_id\" column of the person table is a PRIMARY KEY. This means it must be unique. (It probably shouldn't be NULL either!) The names in the person table could be the primary names that we use in our Star Wars database system, and we could have another alternative_name table: sw_id first_name last_name 3 Anakin Skywalker","title":"Primary Keys"},{"location":"reference/tables-and-triples/#holes","text":"Tables are great! We can add more columns to our person table: sw_id first_name last_name occupation 1 Luke Skywalker Jedi 2 Leia Organa princess 3 Darth Vader 4 Han Solo scoundrel The 2D pattern of a table is a strong one. It not only provides a \"slot\" (cell) for every combination of row and column, it also makes it very obvious when one of those slots is empty . What does it mean for a slot to be empty? It could mean many things. For example, in the previous table in the row for Darth Vader, the cell for the \"occupation\" column is empty. This could mean that: we don't know whether he has an occupation we know that he has an occupation, but we don't know which occupation it is. we might know, but we haven't bothered to write it down yet we might know, but it doesn't fit nicely into the New Republic Standard Registry of Occupations; in other words, we know what his occupation is, but including it here would violate a constraint on our database we specifically know that he doesn't have an occupation; we triple-checked we know more generally ( Spoiler Alert!! ) that he's dead, and dead people can't have an occupation. I'm sure I haven't captured all the possibilities. The point is that there's lot of possible reasons why a cell would be blank. So what can we do about it? If our table is stored in a SQL database, then we have the option of putting a NULL value in the cell. NULL is pretty strange. It isn't TRUE and it isn't FALSE. Usually NULL values are excluded from SQL query results unless you are careful to ask for them. The way that NULL works in SQL eliminates some of the possibilities above. SQL uses the \"closed-world assumption\", which is the assumption that if a statement is true then it's known to be true, and conversely that if it's not known to be true then it's false. So if Anakin's occupation is NULL in a SQL database, then as far as SQL is concerned, we must know that he doesn't have an occupation. That might not be what you were expecting! The Software Carpentry module on Missing Data has more information.","title":"Holes"},{"location":"reference/tables-and-triples/#multiple-values","text":"Tables are great! Let's add even more information to our table: sw_id first_name last_name occupation enemy 1 Luke Skywalker Jedi 3 2 Leia Organa princess 3 3 Darth Vader 1,2,4 4 Han Solo scoundrel 3 We're trying to say that Darth Vader is the enemy of everybody else in our table. We're using the primary key of the person in the enemy column, which is good, but we've ended up with multiple values in the \"enemy\" column for Darth Vader. In any table or SQL database you could make the \"enemy\" column a string, pick a delimiter such as the comma, and concatenate your values into a comma-separated list. This works, but not very well. In some SQL databases, such as Postgres, you could given the \"enemy\" column an array type, so it can contain multiple values. You get special operators for querying inside arrays. This can work pretty well. The usual advice is to break this \"one to many\" information into a new \"enemy\" table: sw_id enemy 1 3 2 3 3 1 3 2 3 4 4 1 Then you can JOIN the person table to the enemy table as needed.","title":"Multiple Values"},{"location":"reference/tables-and-triples/#sparse-tables","text":"Tables are great! Let's add even more information to our table: sw_id first_name last_name occupation father lightsaber_color ship 1 Luke Skywalker Jedi 3 green 2 Leia Organa princess 3 3 Darth Vader red 4 Han Solo scoundrel Millennium Falcon A bunch of these columns only apply to a few rows. Now we've got a lot more NULLs to deal with. As the number of columns increases, this can become a problem.","title":"Sparse Tables"},{"location":"reference/tables-and-triples/#property-tables","text":"Tables are great! If sparse tables are a problem, then let's try to apply the same solution that worked for the \"many to one\" problem in the previous section. name table: sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo 3 Anakin Skywalker occupation table: sw_id occupation 1 Jedi 2 princess 4 scoundrel enemy table: sw_id enemy 1 3 2 3 3 1 3 2 3 4 4 1 father table: sw_id father 1 3 2 3 lightsaber_color table: sw_id lightsaber_color 1 green 3 red ship table: sw_id ship 4 Millennium Falcon Hmm. Yeah, that will work. But every query we write will need some JOINs. It feels like we've lost something.","title":"Property Tables"},{"location":"reference/tables-and-triples/#entity-attribute-value","text":"Tables are great! But there's such a thing as too many tables. We started out with a table with a bunch of rows and a bunch of columns, and ended up with a bunch of tables with a bunch of rows but just a few columns. I have a brilliant idea! Let's combine all these property tables into just one table, by adding a \"property\" column! sw_id property value 1 first_name Luke 2 first_name Leia 3 first_name Darth 4 first_name Han 5 first_name Anakin 1 last_name Skywalker 2 last_name Skywalker 3 last_name Vader 4 last_name Solo 5 last_name Skywalker 1 occupation Jedi 2 occupation princess 4 occupation scoundrel 1 enemy 3 2 enemy 3 3 enemy 1 3 enemy 2 3 enemy 4 4 enemy 1 1 father 3 2 father 3 1 lightsaber_color green 3 lightsaber_color red 4 ship Millenium Falcon It turns out that I'm not the first one to think of this idea. People call it \"Entity, Attribute, Value\" or \"EAV\". People also call it an \"anti-pattern\", in other words: a clear sign that you've made a terrible mistake. There are lots of circumstances in which one big, extremely generic table is a bad idea. First of all, you can't do very much with the datatypes for the property and value columns. They kind of have to be strings. It's potentially difficult to index. And tables like this are miserable to query, because you end up with all sorts of self-joins to handle. But there's at least one use case where it turns out to work quite well...","title":"Entity, Attribute, Value"},{"location":"reference/tables-and-triples/#merging-tables","text":"Tables are great! Until they're not. The strong row and column structure of tables makes them great for lots of things, but not so great for merging data from different sources. Before you can merge two tables you need to know all about: how the columns are structured what the rows mean what the cells mean So you need to know the schemas of the two tables before you can start merging them together. But if you happen to have two EAV tables then, as luck would have it, they already have the same schema! You also need to know that you're talking about the same things: the rows have to be about the same things, you need to be using the same property names for the same things, and the cell values also need to line up. If only there was an open standard for specifying globally unique identifiers... Yes, you guessed it: URLs (and URNs and URIs and IRIs)! Let's assume that we use the same URLs for the same things across the two tables. Since we're a close-knit community, we've come to an agreement on a Star Wars data vocabulary. URLs are annoyingly long to use in databases, so let's use standard \"sw\" prefix to shorten them. Now we have table 1: sw_id property value sw:1 sw:first_name Luke sw:2 sw:first_name Leia sw:3 sw:first_name Darth sw:4 sw:first_name Han sw:5 sw:first_name Anakin sw:1 sw:last_name Skywalker sw:2 sw:last_name Skywalker sw:3 sw:last_name Vader sw:4 sw:last_name Solo sw:5 sw:last_name Skywalker sw:1 sw:occupation sw:Jedi sw:2 sw:occupation sw:princess sw:4 sw:occupation sw:scoundrel and table 2: sw_id property value sw:1 sw:enemy sw:3 sw:2 sw:enemy sw:3 sw:3 sw:enemy sw:1 sw:3 sw:enemy sw:2 sw:3 sw:enemy sw:4 sw:4 sw:enemy sw:1 sw:1 sw:father sw:3 sw:2 sw:father sw:3 sw:1 sw:lightsaber_color green sw:3 sw:lightsaber_color red sw:4 sw:ship Millenium Falcon To merge these two tables, we simple concatenate them. It couldn't be simpler. Wait, this looks kinda familiar...","title":"Merging Tables"},{"location":"reference/tables-and-triples/#rdf","text":"These tables are pretty much in RDF format. You just have to squint a little! sw_id == subject property == predicate value == object Each row of the table is a subject-predicate-object triple. Our subjects, predicates, and some objects are URLs. We also have some literal objects. We could turn this table directly into Turtle format with a little SQL magic (basically just concatenating strings): SELECT \"@prefix sw: <http://example.com/sw_> .\" UNION ALL SELECT \"\" UNION ALL SELECT sw_id || \" \" || property || \" \" || IF( INSTR(value, \":\"), value, -- CURIE \"\"\"\" || value || \"\"\"\" -- literal ) || \" .\" FROM triple_table; The first few lines will look like this: @prefix sw: <http://example.com/sw_> . sw:1 sw:first_name \"Luke\" . sw:2 sw:first_name \"Leia\" . sw:3 sw:first_name \"Darth\" . sw:4 sw:first_name \"Han\" . Two things we're missing from RDF are language tagged literals and typed literals. We also haven't used any blank nodes in our triple table. These are easy enough to add. The biggest thing that's different about RDF is that it uses the \"open-world assumption\", so something may be true even though we don't have a triple asserting that it's true. The open-world assumption is a better fit than the closed-world assumption when we're integrating data on the Web.","title":"RDF"},{"location":"reference/tables-and-triples/#conclusion","text":"Tables are great! We use them all the time, they're strong and rigid, and we're comfortable with them. RDF, on the other hand, looks strange at first. For most common data processing, RDF is too flexible. But sometimes flexiblity is the most important thing. The greatest strength of tables is their rigid structure, but that's also their greatest weakness. We saw a number of problems with tables, and how they could be overcome by breaking tables apart into smaller tables, until we got down to the most basic pattern: subject-predicate-object. Step by step, we were pushed toward RDF. Merging tables is particularly painful. When working with data on the Web, merging is one of the most common and important operations, and so it makes sense to use RDF for these tasks. If self-joins with SQL is the worst problem for EAV tables, then SPARQL solves it. These examples show that it's not really very hard to convert tables to triples. And once you've seen SPARQL, the RDF query language, you've seen one good way to convert triples to tables: SPARQL SELECT results are just tables! Since it's straightforward to convert tables to triples and back again, make sure to use the right tool for the right job. When you need to merge heterogeneous data, reach for triples. For most other data processing tasks, use tables. They're great!","title":"Conclusion"},{"location":"teaching/case_studies/","text":"Learning Outcomes for Critical Path Tutorial \u00b6 Understand the value of URIs as global identifiers and the potential shortcomings. Having a basic picture of the flagship efforts of the Semantic Web. Being aware of some of the central Semantic Web applications in the biomedical domain. Having a cursory understanding of how linked data can help to power your Critical Path data analysis problems. Interesting Case Studies to talk about: \u00b6 The Experimental Factor Ontology: from controlled vocabulary to integrated application ontology driving drug target identification. From barely structured data via data dictionaries to semantic data integration: International HundredK+ Cohorts Consortium (IHCC) data harmonization case study: How to get from messy, individual data dictionaries for COHORT data to an integrated resource for browsing and grouping. The EJPRD story: Registry level integration using a semantic metadata model Common data elements in rare disease registration. EFO case study \u00b6 Build controlled vocabulary Look a bit at the anatomy of a term So what happens now? The story of scientific database curation The integrator hub with the killer use case comes along Now the vocabulary is getting \u201cforced\u201d onto other databases that want to be part (and have to be part) The number of terms needed shoot up exponentially - external ontologies need two be integrated Uberon Mondo Why Mondo and not DO? Finally: better, more specialised hierarchies Its hard to re-use. (Measurement story) Output data of integrator hub can now be integrated even higher (e.g. disease to gene networks) Individual sources can also be integrated individually Stories like this happen all the time: The SCDO story First started building a vocab Then using ROBOT Then linking OBO terms Then applying for OBO membership Then using OBO purls and re-using OBO terms More to come IHCC story \u00b6 Cohort data are scattered and there is no easy way to group data across cohorts Even just finding the right cohort can be difficult Data dictionaries are often just spreadsheets on someones computer Data dictionaries do not have rich metadata (you dont know data dictionary category or value pertains to a disease) What to do: Build controlled vocabulary Map data dictionaries to a controlled vocabulary Build ontological model from controlled terms rich enough to group the data for the use cases at hand Design a process that makes the above scalable Show examples So now, we want enable the discovery of data across these cohorts. Build GECKO Assign data dictionary elements to IDs and publish as \"Linked Data\" ( browse here ) Build mapping pipeline Check example google sheet Link IDs to ontology terms These links can now be used to group the metadata for identifying cohorts EJPRD story \u00b6 Rare disease registries are scattered across the web and there is no easy way to search across all EJPRD is developing two metadata schemas: On Registry level, they are building the metadata model which is reusing some standard vocabularies such as dcat. There is not that much \"semantics\" here - it really is a metadata model On Record level, they are building the Clinical Data Elements (CDE) Semantic Model , see for example the core model . The idea is that registries publish their metadata (and eventually data) as linked data that can be easily queried using the above models. One of the most major problems is the size of the project and competing voices (\"If its not RDF its not FAIR\"), but also the sheer scale of the technical issue: many of the so called registries are essentially excel spreadsheets on an FTP server.","title":"Case studies for teaching"},{"location":"teaching/case_studies/#learning-outcomes-for-critical-path-tutorial","text":"Understand the value of URIs as global identifiers and the potential shortcomings. Having a basic picture of the flagship efforts of the Semantic Web. Being aware of some of the central Semantic Web applications in the biomedical domain. Having a cursory understanding of how linked data can help to power your Critical Path data analysis problems.","title":"Learning Outcomes for Critical Path Tutorial"},{"location":"teaching/case_studies/#interesting-case-studies-to-talk-about","text":"The Experimental Factor Ontology: from controlled vocabulary to integrated application ontology driving drug target identification. From barely structured data via data dictionaries to semantic data integration: International HundredK+ Cohorts Consortium (IHCC) data harmonization case study: How to get from messy, individual data dictionaries for COHORT data to an integrated resource for browsing and grouping. The EJPRD story: Registry level integration using a semantic metadata model Common data elements in rare disease registration.","title":"Interesting Case Studies to talk about:"},{"location":"teaching/case_studies/#efo-case-study","text":"Build controlled vocabulary Look a bit at the anatomy of a term So what happens now? The story of scientific database curation The integrator hub with the killer use case comes along Now the vocabulary is getting \u201cforced\u201d onto other databases that want to be part (and have to be part) The number of terms needed shoot up exponentially - external ontologies need two be integrated Uberon Mondo Why Mondo and not DO? Finally: better, more specialised hierarchies Its hard to re-use. (Measurement story) Output data of integrator hub can now be integrated even higher (e.g. disease to gene networks) Individual sources can also be integrated individually Stories like this happen all the time: The SCDO story First started building a vocab Then using ROBOT Then linking OBO terms Then applying for OBO membership Then using OBO purls and re-using OBO terms More to come","title":"EFO case study"},{"location":"teaching/case_studies/#ihcc-story","text":"Cohort data are scattered and there is no easy way to group data across cohorts Even just finding the right cohort can be difficult Data dictionaries are often just spreadsheets on someones computer Data dictionaries do not have rich metadata (you dont know data dictionary category or value pertains to a disease) What to do: Build controlled vocabulary Map data dictionaries to a controlled vocabulary Build ontological model from controlled terms rich enough to group the data for the use cases at hand Design a process that makes the above scalable Show examples So now, we want enable the discovery of data across these cohorts. Build GECKO Assign data dictionary elements to IDs and publish as \"Linked Data\" ( browse here ) Build mapping pipeline Check example google sheet Link IDs to ontology terms These links can now be used to group the metadata for identifying cohorts","title":"IHCC story"},{"location":"teaching/case_studies/#ejprd-story","text":"Rare disease registries are scattered across the web and there is no easy way to search across all EJPRD is developing two metadata schemas: On Registry level, they are building the metadata model which is reusing some standard vocabularies such as dcat. There is not that much \"semantics\" here - it really is a metadata model On Record level, they are building the Clinical Data Elements (CDE) Semantic Model , see for example the core model . The idea is that registries publish their metadata (and eventually data) as linked data that can be easily queried using the above models. One of the most major problems is the size of the project and competing voices (\"If its not RDF its not FAIR\"), but also the sheer scale of the technical issue: many of the so called registries are essentially excel spreadsheets on an FTP server.","title":"EJPRD story"},{"location":"tutorial/dosdp_template/","text":"Dead Simple Ontology Design Patterns (DOSDP) \u00b6 Note: This is an updated Version of Jim Balhoff's DOSDP tutorial here . The main use case for dosdp-tools (and the DOS-DP framework ) is managing a set of ontology terms, which all follow a common logical pattern, by simply collecting the unique aspect of each term as a line in a spreadsheet. For example, we may be developing an ontology of environmental exposures. We would like to have terms in our ontology which represent exposure to a variety of stressors, such as chemicals, radiation, social stresses, etc. Creating an ontology of environmental exposures \u00b6 To maximize reuse and facilitate data integration, we can build our exposure concepts by referencing terms from domain-specific ontologies, such as the Chemical Entities of Biological Interest Ontology (ChEBI) for chemicals. By modeling each exposure concept in the same way, we can use a reasoner to leverage the chemical classification provided by ChEBI to provide a classification for our exposure concepts. Since each exposure concept has a logical definition based on our data model for exposure, there is no need to manually manage the classification hierarchy. Let's say our model for exposure concepts holds that an \"exposure\" is an event with a particular input (the thing the subject is exposed to): 'exposure to X' EquivalentTo 'exposure event' and 'has input' some X If we need an ontology class to represent 'exposure to sarin' (bad news!), we can simply use the term sarin from ChEBI, and create a logical definition: 'exposure to sarin' EquivalentTo 'exposure event' and 'has input' some sarin We can go ahead and create some other concepts we need for our exposure data: 'exposure to asbestos' EquivalentTo 'exposure event' and 'has input' some asbestos 'exposure to chemical substance' EquivalentTo 'exposure event' and 'has input' some 'chemical substance' These definitions again can reference terms provided by ChEBI: asbestos and chemical substance Classifying our concepts \u00b6 Since the three concepts we've created all follow the same logical model, their hierarchical relationship can be logically determined by the relationships of the chemicals they reference. ChEBI asserts this structure for those terms: 'chemical substance' | | -------------- | | | | sarin asbestos Based on this, an OWL reasoner can automatically tell us the relationships between our exposure concepts: 'exposure to chemical substance' | | -------------------------- | | | | 'exposure to sarin' 'exposure to asbestos' To support this, we simply need to declare the ChEBI OWL file as an owl:import in our exposure ontology, and use an OWL reasoner such as ELK. Managing terms with dosdp-tools \u00b6 Creating terms by hand like we just did works fine, and relying on the reasoner for the classification will save us a lot of trouble and maintain correctness as our ontology grows. But since all the terms use the same logical pattern, it would be nice to keep this in one place; this will help make sure we always follow the pattern correctly when we create new concepts. We really only need to store the list of inputs (e.g. chemicals) in order to create all our exposure concepts. As we will see later, we may also want to manage separate sets of terms that follow other, different, patterns. To do this with dosdp-tools , we need three main files: a pattern template , a spreadsheet of pattern fillers , and a source ontology . You will also usually need a file of prefix definitions so that the tool knows how to expand your shortened identifiers into IRIs. For our chemical exposures, getting the source ontology is easy: just download chebi.owl . Note\u2014it's about 450 MB. For our pattern fillers spreadsheet , we just need to make a tab-delimited file containing the chemical stressors for which we need exposure concepts. The file needs a column for the term IRI to be used for the generated class ( this column is always called defined_class ), and also a column for the chemical to reference ( choose a label according to your data model ). It should look like this: defined_class input EXPOSO:1 CHEBI:75701 EXPOSO:2 CHEBI:46661 EXPOSO:3 CHEBI:59999 The columns should be tab-separated\u2014you can download a correctly formatted file to follow along. For now you will just maintain this file by hand, adding chemicals by looking up their ID in ChEBI, and manually choosing the next ID for your generated classes. In the future this may be simplified using the DOS-DP table editor , which is under development. The trickiest part to DOS-DP is creating your pattern template (but it's not so hard). Pattern templates are written in YAML , a simple file format based on keys and values. The keys are text labels; values can be plain values, another key-value structure, or a list. The DOS-DP schema specifies the keys and values which can be used in a pattern file. We'll use most of the common entries in this example. Read the comments (lines starting with #) for explanation of the various fields: # We can provide a name for this pattern here. pattern_name: exposure_with_input # In 'classes', we define the terms we will use in this pattern. # In the OBO community the terms often have numeric IDs, so here # we can provide human-readable names we can use further in the pattern. # The key is the name to be used; the value is the ID in prefixed form (i.e. a CURIE). classes: exposure event: ExO:0000002 Thing: owl:Thing # Use 'relations' the same way as 'classes', # but for the object properties used in the pattern. relations: has input: RO:0002233 # The 'vars' section defines the various slots that can be # filled in for this pattern. We have only one, which we call 'input'. # The value is the range, meaning the class of things that are valid # values for this pattern. By specifying owl:Thing, we're allowing any # class to be provided as a variable filler. You need a column in your # spreadsheet for each variable defined here, in addition to the `defined class` column. vars: input: 'Thing' # We can provide a template for an `rdfs:label` value to generate # for our new term. dosdp-tools will search the source ontology # to find the label for the filler term, and fill it into the # name template in place of the %s. name: text: \"exposure to %s\" vars: - input # This works the same as label generation, but instead creates # a definition annotation. def: text: \"A exposure event involving the interaction of an exposure receptor to %s. Exposure may be through a variety of means, including through the air or surrounding medium, or through ingestion.\" vars: - input # Here we can generate a logical axiom for our new concept. Create an # expression using OWL Manchester syntax. The expression can use any # of the terms defined at the beginning of the pattern. A reference # to the variable value will be inserted in place of the %s. equivalentTo: text: \"'exposure event' and 'has input' some %s\" vars: - input Download the pattern template file to follow along. Now we only need one more file before we can run dosdp-tools . A file of prefix definitions (also in YAML format) will specify how to expand the CURIEs we used in our spreadsheet and pattern files: EXPOSO: http://example.org/exposure/ Here we are specifying how to expand our EXPOSO prefix (used in our spreadsheet defined_class column). To expand the others, we'll pass a convenience option to dosdp-tools , --obo-prefixes , which will activate some predefined prefixes such as owl: , and handle any other prefixes using the standard expansion for OBO IDs: http://purl.obolibrary.org/obo/PREFIX_ . Here's a link to the prefixes file . Now we're all set to run dosdp-tools ! If you've downloaded or created all the necessary files, run this command to generate your ontology of exposures (assuming you've added the dosdp-tools to your Unix PATH): dosdp-tools generate --obo-prefixes=true --prefixes=prefixes.yaml --infile=exposure_with_input.tsv --template=exposure_with_input.yaml --ontology=chebi.owl --outfile=exposure_with_input.owl This will apply the pattern to each line in your spreadsheet, and save the result in an ontology saved at exposure_with_input.owl (it should look something like this ). If you take a look at this ontology in a text editor or in Prot\u00e9g\u00e9, you'll see that it contains three classes, each with a generated label, text definition, and equivalent class definition. You're done! Well... you're sort of done. But wouldn't it be nice if your exposure ontology included some information about the chemicals you referenced? Without this our reasoner can't classify our exposure concepts. As we said above, we could add an owl:import declaration and load all of ChEBI, but your exposure ontology has three classes and ChEBI has over 120,000 classes. Instead, we can use the ROBOT tool to extract a module of just the relevant axioms from ChEBI. Later, we will also see how to use ROBOT to merge the outputs from multiple DOS-DP patterns into one ontology. You can download ROBOT from its homepage . Extracting a module from the source ontology \u00b6 ROBOT has a few different methods for extracting a subset from an ontology. We'll use the Syntactic Locality Module Extractor (SLME) to get a set of axioms relevant to the ChEBI terms we've referenced. ROBOT will need a file containing the list of terms. We can use a Unix command to get these out of our spreadsheet file: sed '1d' exposure_with_input.tsv | cut -f 2 >inputs.txt We'll end up with a simple list: CHEBI:75701 CHEBI:46661 CHEBI:59999 Now we can use ROBOT to extract an SLME bottom module for those terms out of ChEBI: robot extract --method BOT --input chebi.owl --term-file inputs.txt --output chebi_extract.owl Our ChEBI extract only has 63 classes. Great! If you want, you can merge the ChEBI extract into your exposure ontology before releasing it to the public: robot merge --input exposure_with_input.owl --input chebi_extract.owl --output exposo.owl Now you can open exposo.owl in Prot\u00e9g\u00e9, run the reasoner, and see a correct classification for your exposure concepts! You may notice that your ontology is missing labels for ExO:0000002 ('exposure event') and RO:0002233 ('has input'). If you want, you can use ROBOT to extract that information from ExO and RO. Working with multiple patterns \u00b6 You will often want to generate ontology modules using more than one DOS-DP pattern. For example, you may want to organize environmental exposures by an additional axis of classification, such as exposure to substances with various biological roles, based on information provided by ChEBI. This requires a slightly different logical expression, so we'll make a new pattern : pattern_name: exposure_with_input_with_role classes: exposure event: ExO:0000002 Thing: owl:Thing relations: has input: RO:0002233 has role: RO:0000087 vars: input: 'Thing' name: text: \"exposure to %s\" vars: - input def: text: \"A exposure event involving the interaction of an exposure receptor to a substance with %s role. Exposure may be through a variety of means, including through the air or surrounding medium, or through ingestion.\" vars: - input equivalentTo: text: \"'exposure event' and 'has input' some ('has role' some %s)\" vars: - input Let's create an input file for this pattern, with a single filler, neurotoxin : defined_class input EXPOSO:4 CHEBI:50910 Now we can run dosdp-tools for this pattern: dosdp-tools generate --obo-prefixes --prefixes=prefixes.yaml --infile=exposure_with_input_with_role.tsv --template=exposure_with_input_with_role.yaml --ontology=chebi.owl --outfile=exposure_with_input_with_role.owl We can re-run our ChEBI module extractor, first appending the terms used for this pattern to the ones we used for the first pattern: sed '1d' exposure_with_input_with_role.tsv | cut -f 2 >>inputs.txt And then run robot extract exactly as before: robot extract --method BOT --input chebi.owl --term-file inputs.txt --output chebi_extract.owl Now we just want to merge both of our generated modules, along with our ChEBI extract: robot merge --input exposure_with_input.owl --input exposure_with_input_with_role.owl --input chebi_extract.owl --output exposo.owl If you open the new exposo.owl in Prot\u00e9g\u00e9 and run the reasoner, you'll now see 'exposure to sarin' classified under both 'exposure to chemical substance' and also 'exposure to neurotoxin'. Conclusion \u00b6 By using dosdp-tools and robot together, you can effectively develop ontologies which compose parts of ontologies from multiple domains using standard patterns. You will probably want to orchestrate the types of commands used in this tutorial within a Makefile, so that you can automate this process for easy repeatability.","title":"DOSDP Templates"},{"location":"tutorial/dosdp_template/#dead-simple-ontology-design-patterns-dosdp","text":"Note: This is an updated Version of Jim Balhoff's DOSDP tutorial here . The main use case for dosdp-tools (and the DOS-DP framework ) is managing a set of ontology terms, which all follow a common logical pattern, by simply collecting the unique aspect of each term as a line in a spreadsheet. For example, we may be developing an ontology of environmental exposures. We would like to have terms in our ontology which represent exposure to a variety of stressors, such as chemicals, radiation, social stresses, etc.","title":"Dead Simple Ontology Design Patterns (DOSDP)"},{"location":"tutorial/dosdp_template/#creating-an-ontology-of-environmental-exposures","text":"To maximize reuse and facilitate data integration, we can build our exposure concepts by referencing terms from domain-specific ontologies, such as the Chemical Entities of Biological Interest Ontology (ChEBI) for chemicals. By modeling each exposure concept in the same way, we can use a reasoner to leverage the chemical classification provided by ChEBI to provide a classification for our exposure concepts. Since each exposure concept has a logical definition based on our data model for exposure, there is no need to manually manage the classification hierarchy. Let's say our model for exposure concepts holds that an \"exposure\" is an event with a particular input (the thing the subject is exposed to): 'exposure to X' EquivalentTo 'exposure event' and 'has input' some X If we need an ontology class to represent 'exposure to sarin' (bad news!), we can simply use the term sarin from ChEBI, and create a logical definition: 'exposure to sarin' EquivalentTo 'exposure event' and 'has input' some sarin We can go ahead and create some other concepts we need for our exposure data: 'exposure to asbestos' EquivalentTo 'exposure event' and 'has input' some asbestos 'exposure to chemical substance' EquivalentTo 'exposure event' and 'has input' some 'chemical substance' These definitions again can reference terms provided by ChEBI: asbestos and chemical substance","title":"Creating an ontology of environmental exposures"},{"location":"tutorial/dosdp_template/#classifying-our-concepts","text":"Since the three concepts we've created all follow the same logical model, their hierarchical relationship can be logically determined by the relationships of the chemicals they reference. ChEBI asserts this structure for those terms: 'chemical substance' | | -------------- | | | | sarin asbestos Based on this, an OWL reasoner can automatically tell us the relationships between our exposure concepts: 'exposure to chemical substance' | | -------------------------- | | | | 'exposure to sarin' 'exposure to asbestos' To support this, we simply need to declare the ChEBI OWL file as an owl:import in our exposure ontology, and use an OWL reasoner such as ELK.","title":"Classifying our concepts"},{"location":"tutorial/dosdp_template/#managing-terms-with-dosdp-tools","text":"Creating terms by hand like we just did works fine, and relying on the reasoner for the classification will save us a lot of trouble and maintain correctness as our ontology grows. But since all the terms use the same logical pattern, it would be nice to keep this in one place; this will help make sure we always follow the pattern correctly when we create new concepts. We really only need to store the list of inputs (e.g. chemicals) in order to create all our exposure concepts. As we will see later, we may also want to manage separate sets of terms that follow other, different, patterns. To do this with dosdp-tools , we need three main files: a pattern template , a spreadsheet of pattern fillers , and a source ontology . You will also usually need a file of prefix definitions so that the tool knows how to expand your shortened identifiers into IRIs. For our chemical exposures, getting the source ontology is easy: just download chebi.owl . Note\u2014it's about 450 MB. For our pattern fillers spreadsheet , we just need to make a tab-delimited file containing the chemical stressors for which we need exposure concepts. The file needs a column for the term IRI to be used for the generated class ( this column is always called defined_class ), and also a column for the chemical to reference ( choose a label according to your data model ). It should look like this: defined_class input EXPOSO:1 CHEBI:75701 EXPOSO:2 CHEBI:46661 EXPOSO:3 CHEBI:59999 The columns should be tab-separated\u2014you can download a correctly formatted file to follow along. For now you will just maintain this file by hand, adding chemicals by looking up their ID in ChEBI, and manually choosing the next ID for your generated classes. In the future this may be simplified using the DOS-DP table editor , which is under development. The trickiest part to DOS-DP is creating your pattern template (but it's not so hard). Pattern templates are written in YAML , a simple file format based on keys and values. The keys are text labels; values can be plain values, another key-value structure, or a list. The DOS-DP schema specifies the keys and values which can be used in a pattern file. We'll use most of the common entries in this example. Read the comments (lines starting with #) for explanation of the various fields: # We can provide a name for this pattern here. pattern_name: exposure_with_input # In 'classes', we define the terms we will use in this pattern. # In the OBO community the terms often have numeric IDs, so here # we can provide human-readable names we can use further in the pattern. # The key is the name to be used; the value is the ID in prefixed form (i.e. a CURIE). classes: exposure event: ExO:0000002 Thing: owl:Thing # Use 'relations' the same way as 'classes', # but for the object properties used in the pattern. relations: has input: RO:0002233 # The 'vars' section defines the various slots that can be # filled in for this pattern. We have only one, which we call 'input'. # The value is the range, meaning the class of things that are valid # values for this pattern. By specifying owl:Thing, we're allowing any # class to be provided as a variable filler. You need a column in your # spreadsheet for each variable defined here, in addition to the `defined class` column. vars: input: 'Thing' # We can provide a template for an `rdfs:label` value to generate # for our new term. dosdp-tools will search the source ontology # to find the label for the filler term, and fill it into the # name template in place of the %s. name: text: \"exposure to %s\" vars: - input # This works the same as label generation, but instead creates # a definition annotation. def: text: \"A exposure event involving the interaction of an exposure receptor to %s. Exposure may be through a variety of means, including through the air or surrounding medium, or through ingestion.\" vars: - input # Here we can generate a logical axiom for our new concept. Create an # expression using OWL Manchester syntax. The expression can use any # of the terms defined at the beginning of the pattern. A reference # to the variable value will be inserted in place of the %s. equivalentTo: text: \"'exposure event' and 'has input' some %s\" vars: - input Download the pattern template file to follow along. Now we only need one more file before we can run dosdp-tools . A file of prefix definitions (also in YAML format) will specify how to expand the CURIEs we used in our spreadsheet and pattern files: EXPOSO: http://example.org/exposure/ Here we are specifying how to expand our EXPOSO prefix (used in our spreadsheet defined_class column). To expand the others, we'll pass a convenience option to dosdp-tools , --obo-prefixes , which will activate some predefined prefixes such as owl: , and handle any other prefixes using the standard expansion for OBO IDs: http://purl.obolibrary.org/obo/PREFIX_ . Here's a link to the prefixes file . Now we're all set to run dosdp-tools ! If you've downloaded or created all the necessary files, run this command to generate your ontology of exposures (assuming you've added the dosdp-tools to your Unix PATH): dosdp-tools generate --obo-prefixes=true --prefixes=prefixes.yaml --infile=exposure_with_input.tsv --template=exposure_with_input.yaml --ontology=chebi.owl --outfile=exposure_with_input.owl This will apply the pattern to each line in your spreadsheet, and save the result in an ontology saved at exposure_with_input.owl (it should look something like this ). If you take a look at this ontology in a text editor or in Prot\u00e9g\u00e9, you'll see that it contains three classes, each with a generated label, text definition, and equivalent class definition. You're done! Well... you're sort of done. But wouldn't it be nice if your exposure ontology included some information about the chemicals you referenced? Without this our reasoner can't classify our exposure concepts. As we said above, we could add an owl:import declaration and load all of ChEBI, but your exposure ontology has three classes and ChEBI has over 120,000 classes. Instead, we can use the ROBOT tool to extract a module of just the relevant axioms from ChEBI. Later, we will also see how to use ROBOT to merge the outputs from multiple DOS-DP patterns into one ontology. You can download ROBOT from its homepage .","title":"Managing terms with dosdp-tools"},{"location":"tutorial/dosdp_template/#extracting-a-module-from-the-source-ontology","text":"ROBOT has a few different methods for extracting a subset from an ontology. We'll use the Syntactic Locality Module Extractor (SLME) to get a set of axioms relevant to the ChEBI terms we've referenced. ROBOT will need a file containing the list of terms. We can use a Unix command to get these out of our spreadsheet file: sed '1d' exposure_with_input.tsv | cut -f 2 >inputs.txt We'll end up with a simple list: CHEBI:75701 CHEBI:46661 CHEBI:59999 Now we can use ROBOT to extract an SLME bottom module for those terms out of ChEBI: robot extract --method BOT --input chebi.owl --term-file inputs.txt --output chebi_extract.owl Our ChEBI extract only has 63 classes. Great! If you want, you can merge the ChEBI extract into your exposure ontology before releasing it to the public: robot merge --input exposure_with_input.owl --input chebi_extract.owl --output exposo.owl Now you can open exposo.owl in Prot\u00e9g\u00e9, run the reasoner, and see a correct classification for your exposure concepts! You may notice that your ontology is missing labels for ExO:0000002 ('exposure event') and RO:0002233 ('has input'). If you want, you can use ROBOT to extract that information from ExO and RO.","title":"Extracting a module from the source ontology"},{"location":"tutorial/dosdp_template/#working-with-multiple-patterns","text":"You will often want to generate ontology modules using more than one DOS-DP pattern. For example, you may want to organize environmental exposures by an additional axis of classification, such as exposure to substances with various biological roles, based on information provided by ChEBI. This requires a slightly different logical expression, so we'll make a new pattern : pattern_name: exposure_with_input_with_role classes: exposure event: ExO:0000002 Thing: owl:Thing relations: has input: RO:0002233 has role: RO:0000087 vars: input: 'Thing' name: text: \"exposure to %s\" vars: - input def: text: \"A exposure event involving the interaction of an exposure receptor to a substance with %s role. Exposure may be through a variety of means, including through the air or surrounding medium, or through ingestion.\" vars: - input equivalentTo: text: \"'exposure event' and 'has input' some ('has role' some %s)\" vars: - input Let's create an input file for this pattern, with a single filler, neurotoxin : defined_class input EXPOSO:4 CHEBI:50910 Now we can run dosdp-tools for this pattern: dosdp-tools generate --obo-prefixes --prefixes=prefixes.yaml --infile=exposure_with_input_with_role.tsv --template=exposure_with_input_with_role.yaml --ontology=chebi.owl --outfile=exposure_with_input_with_role.owl We can re-run our ChEBI module extractor, first appending the terms used for this pattern to the ones we used for the first pattern: sed '1d' exposure_with_input_with_role.tsv | cut -f 2 >>inputs.txt And then run robot extract exactly as before: robot extract --method BOT --input chebi.owl --term-file inputs.txt --output chebi_extract.owl Now we just want to merge both of our generated modules, along with our ChEBI extract: robot merge --input exposure_with_input.owl --input exposure_with_input_with_role.owl --input chebi_extract.owl --output exposo.owl If you open the new exposo.owl in Prot\u00e9g\u00e9 and run the reasoner, you'll now see 'exposure to sarin' classified under both 'exposure to chemical substance' and also 'exposure to neurotoxin'.","title":"Working with multiple patterns"},{"location":"tutorial/dosdp_template/#conclusion","text":"By using dosdp-tools and robot together, you can effectively develop ontologies which compose parts of ontologies from multiple domains using standard patterns. You will probably want to orchestrate the types of commands used in this tutorial within a Makefile, so that you can automate this process for easy repeatability.","title":"Conclusion"},{"location":"tutorial/fhkb/","text":"Manchester Family History Advanced OWL \u00b6 This is a fork of the infamous Manchester Family History Advanced OWL Tutorial version 1.1, located at http://owl.cs.manchester.ac.uk/publications/talks-and-tutorials/fhkbtutorial/ The translation to markdown is not without issue, but we are making a start to making the tutorial a bit more accessible. This reproduction is done with kind permission by Robert Stevens. Original credits (Version 1.1, see pdf ): \u00b6 Authors: Robert Stevens Margaret Stevens Nicolas Matentzoglu Simon Jupp Bio-Health Informatics Group School of Computer Science University of Manchester Oxford Road Manchester United Kingdom M13 9PL robert.stevens@manchester.ac.uk Contributors \u00b6 v 1.0 Robert Stevens, Margaret Stevens, Nicolas Matentzoglu and Simon Jupp v 1.1 Robert Stevens, Nicolas Matentzoglu v 2.0 (Web version) Robert Stevens, Nicolas Matentzoglu, Shawn Tan The University of Manchester Copyright\u00a9 The University of Manchester November 25, 2015 Acknowledgements \u00b6 This tutorial was realised as part of the Semantic Web Authoring Tool (SWAT) project (see http://www.swatproject.org), which is supported by the UK Engineering and Physical Sciences Research Council (EPSRC) grant EP/G032459/1, to the University of Manchester, the University of Sussex and the Open University. Dedication \u00b6 The Stevens family\u2014all my ancestors were necessary for this to happen. Also, for my Mum who gathered all the information. Contents \u00b6 Preamble 0.1 Licencing 0.2 Reporting Errors 0.3 Acknowledgements 1. Introduction 1.1 Learning Outcomes 1.2 Why Family History? 1.3 How to use this Tutorial 1.4 FHKB Resources 1.5 Conventions used in this Tutorial 2. Adding some Individuals to the FHKB 2.1 A World of Objects 2.2 Asserting Parentage Facts 2.3 Summary 3. Ancestors and Descendants 3.1 Ancestors and Descendants 3.2 Grandparents and Great Grandparents 3.3 Summary 4. Modelling the Person Class 4.1 The Class of Person 4.2 Describing Sex in the FHKB 4.3 Defining Man and Woman 4.4 Describing Parentage in the FHKB 4.5 Who has a father? 4.6 Filling in Domains and Ranges for the FHKB Properties 4.7 Inconsistencies 4.8 Adding Some Defined Classes for Ancestors and so on 4.9 Summary 5. Siblings in the FHKB 5.1 Blood relations 5.2 Siblings: Option One 5.2.1 Brothers and Sisters 5.3 Siblings: Option two 5.3.1 Which Modelling Option to Choose for Siblings? 5.4 Half-Siblings 5.5 Aunts and Uncles 5.6 Summary 6. Individuals in Class Expressions 6.1 Richard and Robert\u2019s Parents and Ancestors 6.2 Closing Down What we Know About Parents and Siblings 6.3 Summary 7. Data Properties in the FHKB 7.1 Adding Some Data Properties for Event Years 7.1.1 Counting Numbers of Children 7.2 The Open World Assumption 7.3 Adding Given and Family Names 7.4 Summary 8. Cousins in the FHKB 8.1 Introducing Cousins 8.2 First Cousins 8.3 Other Degrees and Removes of Cousin 8.4 Doing First Cousins Properly 8.5 Summary 9. Marriage in the FHKB 9.1 Marriage 9.1.1 Spouses 9.2 In-Laws 9.3 Brothers and Sisters In-Law 9.4 Aunts and Uncles in-Law 9.5 Summary 10. Extending the TBox 10.1 Adding Defined Classes 10.2 Summary 11. Final remarks A FHKB Family Data Preamble \u00b6 0.1 Licencing \u00b6 The \u2018Manchester Family History Advanced OWL Tutorial\u2019 by Robert Stevens, Margaret Stevens, Nicolas Matentzoglu, Simon Jupp is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. 0.2 Reporting Errors \u00b6 This manual will almost certainly contain errors, defects and infelicities. Do report them to robert.stevens@manchester.ac.uk supplying chapter, section and some actual context in the form of words will help in fixing any of these issues. 0.3 Acknowledgements \u00b6 As well as the author list, many people have contributed to this work. Any contribution, such as reporting bugs etc., is rewarded by an acknowledgement of contribution (in alphabetical order) when the authors get around to adding them: Graham Goff; Matthew Horridge; Jared Leo; Fennie Liang; Phil Lord; Fiona McNeill; Eleni Mikroyannidi; George Moulton; Bijan Parsia; Alan Rector; Uli Sattler; Dmitry Tsarkov; Danielle Welter. Chapter 1 \u00b6 Introduction \u00b6 This tutorial introduces the tutee to many of the more advanced features of the Web Ontology Language (OWL). The topic of family history is used to take the tutee through various modelling issues and, in doing so, using many features of OWL 2 to build a Family History Knowledge Base (FHKB). The exercises are designed to maximise inference about family history through the use of an automated reasoner on an OWL knowledge base (KB) containing many members of the Stevens family. The aim, therefore, is to enable people to learn advanced features of OWL 2 in a setting that involves both classes and individuals, while attempting to maximise the use of inference within the FHKB. 1.1 Learning Outcomes \u00b6 By doing this tutorial, a tutee should be able to: Know about the separation of entities into TBox and ABox; Use classes and individuals in modelling; Write fancy class expressions; Assert facts about individuals; Use the effects of property hierarchies, property characteristics, domain/range constraints to drive inference; Use constraints and role chains on inferences about individuals; Understand and manage the consequences of the open world assumption in the TBox and ABox; Use nominals in class expressions; Appreciate some limits of OWL 2. 1.2 Why Family History? \u00b6 Building an FHKB enables us to meet our learning outcomes through a topic that is accessible to virtually everyone. Family history or genealogy is a good topic for a general tutorial on OWL 2 as it enables us to touch many features of the language and, importantly, it is a field that everyone knows. All people have a family and therefore a family history \u2013 even if they do not know their particular family history. A small caveat was put on the topic being accessible to everyone as some cultures differ, for instance, in the description of cousins and labels given to different siblings. Nevertheless, family history remains a topic that everyone can talk about. Family history is a good topic for an OWL ontology as it obviously involves both individuals \u2013 the people involved \u2013 and classes of individuals \u2013 people, men and women, cousins, etc. Also, it is an area rich in inference; from only knowing parentage and sex of an individual, it is possible to work out all family relationships \u2013 for example, sharing parents implies a sibling relationship; one\u2019s parent\u2019s brothers are one\u2019s uncles; one\u2019s parent\u2019s parents are one\u2019s grandparents. So, we should be able to construct an ontology that allows us to both express family history, but also to infer family relationships between people from knowing relatively little about them. As we will learn through the tutorial, OWL 2 cannot actually do all that is needed to create a FHKB. This is unfortunate, but we use it to our advantage to illustrate some of the limitations of OWL 2. We know that rule based systems can do family history with ease, but that is not the point here; we are not advocating OWL DL as an appropriate mechanism for doing family history, but we do use it as a good educational example. We make the following assumptions about what people know: We assume that people know OWL to the level that is known at the end of the Pizza tutorial . Some ground will be covered again, but a lot of basic OWL is assumed. We assume people know how to use Prot\u00e9g\u00e9 or their OWL environment of choice. We do not give \u2018click by click\u2019 instructions. At some places, some guidance is given, but this is not to be relied upon as Prot\u00e9g\u00e9 changes and we will not keep up to date. We make some simplifying assumptions in this tutorial: We take a conventional western view of family history. This appears to have most effects on naming of sibling and cousin relationships. We take a straight-forward view on the sex of people; this is explored further in Chapter 4; A \u2018conventional\u2019 view of marriage is taken; this is explored further in Chapter 9. We make no special treatment of time or dates; we are only interested in years and we do not do anything fancy; this is explored more in Chapter 7. We assume the ancestors of people go back for ever; obviously this is not true, eventually one would get back to a primordial soup and one\u2019s ancestors are not humans (members of the classPerson), but we don\u2019t bother with such niceties. At the end of the tutorial, you should be able to produce a property hierarchy and a TBox or class hierarchy such as shown in Figure 1.1; all supported by use of the automated reasoner and a lot of OWL 2\u2019s features. Figure 1.1: A part of the class and property hierarchy of the final FHKB. 1.3 How to use this Tutorial \u00b6 Here are some tips on using this manual to the best advantage: Start at the beginning and work towards the end. You can just read the tutorial, but building the FHKB will help you learn much more and much more easily Use the reasoner in each task; a lot of the FHKB tutorial is about using the reasoner and not doing so will detract from the learning outcomes. 1.4 FHKB Resources \u00b6 The following resources are available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial: A full version of the Stevens FHKB. Some links to papers about the FHKB. Some slides about the FHKB tutorial. A set of OWL resources for each stage of the FHKB. Some blogs about the FHKB are at http://robertdavidstevens.wordpress.com. 1.5 Conventions used in this Tutorial \u00b6 All OWL is written in Manchester Syntax. When we use FHKB entities within text, we use a sans serif typeface. We use CamelCase for classes and property names. Class names start with upper case. Individual names start with a lower case letter and internal underscores to break words. Property names usually start with \u2018is\u2019 or \u2018has\u2019 and are CamelCase with a lower case initial letter. Many classes and individuals in the FHKB have annotation properties, usually human readable labels. They show up in some of the examples in Manchester syntax, but are not made explicit as part of the tasks in this tutorial. Every object property is necessarily a sub-property of topObjectProperty. It does not have to be asserted as such. Nevertheless, there might be situations where this relationship is made explicit in this tutorial for illustrative reasons. The individuals we are dealing with represent distinct persons. Throughout the tutorial, once the respective axiom is introduced (chapter 7.1.1), the reader should make sure that all his or her individuals are always made distinct, especially when he or she adds a new one. At the end of each chapter, we note the Description Logic Language (expressivity) needed to represent the ontology and the reasoning times for a number of state of the art reasoning systems. This should get the reader a sense how difficult the FHKB becomes for reasoners to deal with over time. When there is some scary OWL or the reasoner may find the FHKB hard work, you will see a \u2018here be dragons\u2019 image. 1 1 The image comes fromhttp://ancienthomeofdragon.homestead.com/May 2012. Chapter 2 \u00b6 Adding some Individuals to the FHKB \u00b6 In this chapter we will start by creating a fresh OWL ontology and adding some individuals that will be surrogates for people in the FHKB. In particular you will: Create a new OWL ontology for the FHKB; Add some individuals that will stand for members of the Stevens family. Describe parentage of people. Add some facts to specific individuals as to their parentage; See the reasoner doing some work. At the moment we will ignore sex; sex will not happen until Chapter 4. 2.1 A World of Objects \u00b6 The \u2018world\u2019 2 or field of interest we model in an ontology is made up of objects or individuals. Such objects include, but are not limited to: People, their pets, the pizzas they eat; The processes of cooking pizzas, living, running, jumping, undertaking a journey; The spaces within a room, a bowl, an artery; The attributes of things such as colour, dimensions, speed, shape of various objects; Boundaries, love, ideas, plans, hypotheses. 2 we use \u2018world\u2019 as a synonym of \u2018field of interest\u2019 or \u2018domain\u2019. \u2018World\u2019 does not restrict us to modelling the physical world outside our consciousness. We observe these objects, either outside lying around in the world or in our heads. OWL is all about modelling such individuals. Whenever we make a statement in OWL, when we write down an axiom, we are making statements about individuals. When thinking about the axioms in an ontology it is best to think about the individuals involved, even if OWL individuals do not actually appear in the ontology. All through this tutorial we will always be returning to the individuals being described in order to help us understand what we are doing and to help us make decisions about how to do it. 2.2 Asserting Parentage Facts \u00b6 Biologically, everyone has parents; a mother and a father 3 . The starting point for family history is parentage; we need to relate the family member objects by object properties. An object property relates two objects, in this case a child object with his or her mother or father object. To do this we need to create three object properties: Task 1: Creating object properties for parentage Create a new ontology; Create an object property hasMother ; Create a property isMotherOf and give hasMother the InverseOf: isMotherOf ; Do the same for the property hasFather ; Create a property hasParent ; give it the obvious inverse; Make hasMother and hasFather sub-properties of hasParent . Run the reasoner and look at the property hierarchy. Note how the reasoner has automatically completed the sub-hierarchy for isParentOf: isMotherOf and isFatherOf are inferred to be sub-properties of isParentOf . The OWL snippet below shows some parentage fact assertions on an individual. Note that rather than being assertions to an anonymous individual via some class, we are giving an assertion to a named individual. Individual: grant_plinth Facts: hasFather mr_plinth, hasMother mrs_plinth 3 Don\u2019t quibble; it\u2019s true enough here. Task 2: Create the ABox Using the information in Table A.1 (see appendix) about parentage (so the columns about fathers and mothers), enter the fact assertions for the people which appear in rows shaded in grey. We will only use the hasMother and hasFather properties in our fact assertions. You do not need to assert names and birth years yet. This exercise will require you to create an individual for every person we want to talk about, using the Firstname_Secondname_Familyname_Birthyear pattern, as for example in Robert_David_Bright_1965 . While asserting facts about all individuals in the FHKB will be a bit tedious at times, it might be useful to at least do the task for a subset of the family members. For the impatient reader, there is a convenience snapshot of the ontology including the raw individuals available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial If you are working with Prot\u00e9g\u00e9, you may want to look at the Matrix plugin for Prot\u00e9g\u00e9 at this point. The plugin allows you to add individuals quickly in the form of a regular table, and can significantly reduce the effort of adding any type of entity to the ontology. In order to install the matrix plugin, open Prot\u00e9g\u00e9 and go to File \u00bb Check for plugins. Select the \u2018Matrix Views\u2019 plugin. Click install, wait until the the installation is confirmed, close and re-open Prot\u00e9g\u00e9; go to the \u2018Window\u2019 menu item, select \u2018Tabs\u2019 and add the \u2018Individuals matrix\u2019. Now do the following: Task 3: DL queries Classify the FHKB. Issue the DL query hasFather value David_Bright_1934 and look at the answers (remember to check the respective checkbox in Prot\u00e9g\u00e9 to include individuals in your query results). Issue the DL query isFatherOf value Robert_David_Bright_1965 . Look at the answers. 4. Look at the entailed facts on Robert_David_Bright_1965 . You should find the following: David Bright (1934) is the father of Robert David Bright (1965) and Richard John Bright (1962). Robert David Bright (1965) has David Bright 1934 as a parent. Since we have said that isFatherOf has an inverse of hasFather , and we have asserted that Robert_David_Bright_1965 hasFather David_Bright_1934 , we have a simple entailment that David_Bright_1934 isFatherOf Robert_David_Bright_1965 . So, without asserting the isFatherOf facts, we have been able to ask and get answers for that DL query. As we asserted that Robert_David_Bright_1965 hasFather David_Bright_1934 , we also infer that he hasParent David_Bright_1934 ; this is because hasParent is the super-property of hasFather and the sub-property implies the super-property. This works all the way up the property tree until topObjectProperty , so all individuals are related by topObjectProperty \u2014this is always true. This implication \u2018upwards\u2019 is the way to interpret how the property hierarchies work. 2.3 Summary \u00b6 We have now covered the basics of dealing with individuals in OWL ontologies. We have set up some properties, but without domains, ranges, appropriate characteristics and then arranged them in a hierarchy. From only a few assertions in our FHKB, we can already infer many facts about an individual: Simple exploitation of inverses of properties and super-properties of the asserted properties. We have also encountered some important principles: We get inverses for free. The sub-property implies the super-property. So, hasFather implies the hasParent fact between individuals. This entailment of the super-property is very important and will drive much of the inference we do with the FHKB. Upon reasoning we get the inverses of properties between named individuals for free. Lots is still open. For example, we do not know the sex of individuals and what other children, other than those described, people in the FHKB may have. The FHKB ontology at this stage of the tutorial has an expressivity of ALHI. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.026 sec (0.00001 % of final), by Pellet 2.2.0 0.144 sec (0.00116 % of final) and by FaCT++ 1.6.4 is approximately 0. sec (0.000 % of final). 0 sec indicates failure or timeout. Chapter 3 \u00b6 Ancestors and Descendants \u00b6 In this Chapter you will: Use sub-properties and the transitive property characteristic to infer ancestors of people; Add properties to the FHKB property hierarchy that will infer ancestors and descendants of a person without adding any more facts to the FHKB; Explore the use of sub-property chains for grandparents, great grandparents and so on; Place all of these new object properties in the property hierarchy and in that way learn more about the implications of the property hierarchy. Find a snapshot of the ontology at this stage at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial. 3.1 Ancestors and Descendants \u00b6 The FHKB has parents established between individuals and we know that all people have two parents. A parent is an ancestor of its children; a person\u2019s parent\u2019s parents are its ancestors; and so on. So, in our FHKB, Robert\u2019s ancestors are David, Margaret, William, Iris, Charles, Violet, James, another Violet, another William, Sarah and so on. If my parent\u2019s parents are my ancestors, then what we need is a transitive version of the hasParent property. Obviously we do not want hasParent to be transitive, as Robert\u2019s grandparents (and so on) would become his parents (and that would be wrong). We can easily achieve what is necessary. We need a hasAncestor property that has a transitive characteristic. The trick is to make this a super-property of the hasParent property. As explained before, a sub-property implies its super-property. So, if individual x holds a hasParent property with an individual y , then it also holds an instance of its super-property hasAncestor with the individual y . If individual y then holds a hasParent property with another individual z , then there is also, by implication, a hasAncestor property between y and z . As hasAncestor is transitive, x and z also hold a hasAncestor relationship between them. The inverse of hasAncestor can either be isAncestorOf or hasDescendant . We choose the isAncestorOf option. Task 4: Object properties: exploiting the semantics Make a new object property hasRelation , make it symmetric. Make a new object property hasAncestor . Make it a sub-property of hasRelation and a super-property of hasParent . Make hasAncestor transitive. Create the inverse isAncestorOf . Do not \u2018stitch\u2019 it into the property hierarchy; the reasoner will sort it all out for you. Run the reasoner and issue the DL query hasAncestor value William_George_Bright_1901 . Issue the query isAncestorOf value Robert_David_Bright_1965 . The hasAncestor object property will look like this: ObjectProperty: hasAncestor SubPropertyOf: hasRelation SuperPropertyOf: hasParent, Characteristics: Transitive InverseOf: isAncestorOf As usual, it is best to think of the objects or individuals involved in the relationships. Consider the three individuals \u2013 Robert, David and William. Each has a hasFather property, linking Robert to David and then David to William. As hasFather implies its super-property hasParent , Robert also has a hasParent property with David, and David has a hasParent relation to William. Similarly, as hasParent implies hasAncestor , the Robert object has a hasAncestor relation to the David object and the David object has one to the William object. As hasAncestor is transitive, Robert not only holds this property to the David object, but also to the William object (and so on back through Robert\u2019s ancestors). 3.2 Grandparents and Great Grandparents \u00b6 We also want to use a sort of restricted transitivity in order to infer grandparents, great grandparents and so on. My grandparents are my parent\u2019s parents; my grandfathers are my parent\u2019s fathers. My great grandparents are my parent\u2019s parent\u2019s parents. My great grandmothers are my parent\u2019s parent\u2019s mothers. This is sort of like transitivity, but we want to make the paths only a certain length and, in the case of grandfathers, we want to move along two relationships \u2013 hasParent and then hasFather . We can do this with OWL 2\u2019s sub-property chains. The way to think about sub-property chains is: If we see property x followed by property y linking three objects, then it implies that property z is held between Figure 3.1: Three blobs representing objects of the classPerson. The three objects are linked by a hasParent property and this implies a hasGrandparent property. the first and third objects. Figure 3.1 shows this diagrammatically for the hasGrandfather property. For various grandparent object properties we need the following sets of implications: My parent\u2019s parents are my grandparents; My parent\u2019s fathers are my grandfathers; My parent\u2019s mothers are my grandmothers; My parent\u2019s parent\u2019s parents are my great grandparents or my grandparent\u2019s parents are my great grandparents. My parent\u2019s parent\u2019s fathers are my great grandfathers or my parent\u2019s grandfathers are my great grandfathers; My parent\u2019s parent\u2019s mothers are my great grandmothers (and so on). Notice that we can trace the paths in several ways, some have more steps than others, though the shorter paths themselves employ paths. Tracing these paths is what OWL 2\u2019s sub-property chains achieve. For the new object property hasGrandparent we write: ObjectProperty: hasGrandparent SubPropertyChain: hasParent o hasParent We read this as \u2018 hasParent followed by hasParent implies hasGrandparent \u2019. We also need to think where the hasGrandparent property fits in our growing hierarchy of object properties. Think about the implications: Does holding a hasParent property between two objects imply that they also hold a hasGrandparent property? Of course the answer is \u2018no\u2019. So, this new property is not a super-property of hasParent . Does the holding of a hasGrandparent property between two objects imply that they also hold an hasAncestor property? The answer is \u2018yes\u2019; so that should be a super-property of hasGrandparent . We need to ask such questions of our existing properties to work out where we put it in the object property hierarchy. At the moment, our hasGrandparent property will look like this: ObjectProperty: hasGrandParent SubPropertyOf: hasAncestor SubPropertyChain: hasParent o hasParent SuperPropertyOf: hasGrandmother, hasGrandfather InverseOf: isGrandParentOf Do the following task: Task 5: Grandparents object properties Make the hasGrandparent , hasGrandmother and hasGrandfather object properties and the obvious inverses (see OWL code above); Go to the individuals tabs and inspects the inferred object property assertions for Robert_David_Bright_1965 and his parents. Again, think of the objects involved. We can take the same three objects as before: Robert, David and William. Think about the properties that exist, both by assertion and implication, between these objects. We have asserted only hasFather between these objects. The inverse can be inferred between the actual individuals (remember that this is not the case for class level restrictions \u2013 that all instances of a class hold a property does not mean that the filler objects at the other end hold the inverse; the quantification on the restriction tells us this). Remember that: Robert holds a hasFather property with David; David holds a hasFather property with William; By implication through the hasParent super-property of hasFather , Robert holds a hasParent property with David, and the latter holds one with William; The sub-property chain on hasGrandfather then implies that Robert holds a hasGrandfather property to William. Use the diagram in figure 3.1 to trace the path; there is a hasParent path from Robert to William via David and this implies the hasGrandfather property between Robert and William. It is also useful to point out that the inverse of hasGrandfather also has the implication of the sub-property chain of the inverses of hasParent . That is, three objects linked by a path of two isParentOf properties implies that an isGrandfatherOf property is established between the first and third object, in this case William and Robert. As the inverses of hasFather are established by the reasoner, all the inverse implications also hold. 3.3 Summary \u00b6 It is important when dealing with property hierarchies to think in terms of properties between objects and of the implications \u2018up the hierarchy\u2019. A sub-property implies its super-property. So, in our FHKB, two person objects holding a hasParent property between them, by implication also hold an hasAncestor property between them. In turn, hasAncestor has a super-property hasRelation and the two objects in question also hold, by implication, this property between them as well. We made hasAncestor transitive. This means that my ancestor\u2019s ancestors are also my ancestors. That a sub-property is transitive does not imply that its super-property is transitive. We have seen that by manipulating the property hierarchy we can generate a lot of inferences without adding any more facts to the individuals in the FHKB. This will be a feature of the whole process \u2013 keep the work to the minimum (well, almost). In OWL 2, we can also trace \u2018paths\u2019 around objects. Again, think of the objects involved in the path of properties that link objects together. We have done simple paths so far \u2013 Robert linked to David via hasParent and David linked to William via hasFather implies the link between Robert and William of hasGrandfather . If this is true for all cases (for which you have to use your domain knowledge), one can capture this implication in the property hierarchy. Again, we are making our work easier by adding no new explicit facts, but making use of the implication that the reasoner works out for us. The FHKB ontology at this stage of the tutorial has an expressivity ofALRI+. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.262 sec (0.00014 % of final), by Pellet 2.2.0 0.030 sec (0.00024 % of final) and by FaCT++ 1.6.4 is approximately 0.004 sec (0.000 % of final). 0 sec indicates failure or timeout. Chapter 4 \u00b6 Modelling the Person Class \u00b6 In this Chapter you will: Create a Person class; Describe Sex classes; Define Man and Woman ; Ask which of the people in the FHKB has a father. Add domains and ranges to the properties in the FHKB. Make the FHKB inconsistent. Add some more defined classes about people and see some equivalence inferred between classes. These simple classes will form the structure for the whole FHKB. 4.1 The Class of Person \u00b6 For the FHKB, we start by thinking about the objects involved The people in a family \u2013 Robert, Richard, David, Margaret, William, Iris, Charles, Violet, Eileen, John and Peter; The sex of each of those people; The marriages in which they participated; The locations of their births; And many more... There is a class of Person that we will use to represent all these people objects. Task 6: Create the Person class Create a class called DomainEntity ; Create a subclass of DomainEntity called Person . We use DomainEntity as a house-keeping measure. All of our ontology goes underneath this class. We can put other classes \u2018outside\u2019 the ontology, as siblings of DomainEntity , such as \u2018probe\u2019 classes we wish to use to test our ontology. The main thing to remember about the Person class is that we are using it to represent all \u2018people\u2019 individuals. When we make statements about the Person class, we are making statements about all \u2018people\u2019 individuals. What do we know about people? All members of the Person class have: Sex \u2013 they are either male or female; Everyone has a birth year; Everyone has a mother and a father. There\u2019s a lot more we know about people, but we will not mention it here. 4.2 Describing Sex in the FHKB \u00b6 Each and every person object has a sex. In the FHKB we will take a simple view on sex \u2013 a person is either male or female, with no intersex or administrative sex and so on. Each person only has one sex. We have two straight-forward options for modelling sex: Each person object has their own sex object, which is either male or female. Thus Robert\u2019s maleness is different from David\u2019s maleness. There is only one Maleness object and one Femaleness object and each person object has a relationship to either one of these sex objects, but not both. We will take the approach of having a class of Maleness objects and a class of Femaleness objects. These are qualities or attributes of self-standing objects such as a person. These two classes are disjoint, and each is a subclass of a class called Sex . The disjointness means that any one instance of Sex cannot be both an instance of Maleness and an instance of Femaleness at once. We also want to put in a covering axiom on the class Sex , which means that any instance of Sex must be either Maleness or Femaleness ; there is no other kind of Sex . Again, notice that we have been thinking at the level of objects. We do the same when thinking about Person and their Sex . Each and every person is related to an instance of Sex . Each Person holds one relationship to a Sex object. To do this we create an object property called hasSex . We make this property functional, which means that any object can hold that property to only one distinct filler object. We make the domain of hasSex to be Person and the range to be Sex . The domain of Person means that any object holding that property will be inferred to be a member of the class Person . Putting the range of Sex on the hasSex property means that any object at the right-hand end of the hasSex property will be inferred to be of the class Sex . Again, think at the level of individuals or objects. We now put a restriction on the Person class to state that each and every instance of the class Person holds a hasSex property with an instance of the Sex class. It has an existential operator \u2018some\u2019 in the axiom, but the functional characteristic means that each Person object will hold only one hasSex property to a distinct instance of a Sex object 4 . 4 An individual could hold two hasSex properties, as long as the sex objects at the right-hand end of the property are not different. Task 7: Modelling sex Create a class called Sex ; Make it a subclass of DomainEntity ; Make Person and Sex disjoint; Create two subclasses of Sex , Maleness and Femaleness ; Make Maleness and Femaleness disjoint; Put a covering axiom on Sex such that it is equivalent to Maleness or Femaleness . Create an object property, hasSex , with the domain Person , the range Sex and give it the characteristic of \u2018Functional\u2019; Add a restriction hasSex some Sex to the class Person . The hasSex property looks like: ObjectProperty: hasSex Characteristics: Functional Domain: Person Range: Sex The Person class looks like: Class: Person SubClassOf: DomainEntity,(hasSex some Sex) DisjointWith: Sex 4.3 Defining Man and Woman \u00b6 We now have some of the foundations for the FHKB. We have the concept of Person , but we also need to have the concepts of Man and Woman . Now we have Person , together with Maleness and Femaleness , we have the necessary components to define Man and Woman . These two classes can be defined as: Any Person object that has a male sex can be recognised to be a man; any Person object that has a female sex can be recognised as a member of the class woman. Again, think about what conditions are sufficient for an object to be recognised to be a member of a class; this is how we create defined classes through the use of OWL equivalence axioms. To make the Man and Woman classes do the following: Task 8: Describe men and women Create a class Man ; Make it equivalent to a Person that hasSex some Maleness ; Do the same, but with Femaleness , to create the Woman class; A covering axiom can be put on the Person class to indicate that man and woman are the only kinds of person that can exist. (This is not strictly true due to the way Sex has been described.) Run the reasoner and take a look. Having run the reasoner, the Man and Woman classes should appear underneath Person 5 . 5 Actually in Prot\u00e9g\u00e9, this might happen without the need to run the reasoner. The Man and Woman classes will be important for use as domain and range constraints on many of the properties used in the FHKB. To achieve our aim of maximising inference, we should be able to infer that individuals are members of Man , Woman or Person by the properties held by an object. We should not have to state the type of an individual in the FHKB. The classes for Man and Woman should look like: Class: Man EquivalentTo: Person and (hasSex some Maleness) Class: Woman EquivalentTo: Person and (hasSex some Femaleness) 4.4 Describing Parentage in the FHKB \u00b6 To finish off the foundations of the FHKB we need to describe a person object\u2019s parentage. We know that each and every person has one mother and each and every person has one father. Here we are talking about biological mothers and fathers. The complexities of adoption and step parents are outside the scope of this FHKB tutorial. Task 9: Describing Parentage Add the domain Person and the range Woman to the property hasMother . Do the same for the property hasFather , but give it the range Man ; Give the property hasParent domain and range of Person ; Run the reasoner. The (inferred) property hierarchy in the FHKB should look like that shown in Figure 4.1. Notice that we have asserted the sub-property axioms on one side of the property hierarchy. Having done so, the reasoner uses those axioms, together with the inverses, to work out the property hierarchy for the \u2018other side\u2019. We make hasMother functional, as any one person object can hold only one hasMother property to a distinct Woman object. The range of hasMother is Woman , as a mother has to be a woman. The Person object holding the hasMother property can be either a man or a woman, so we have the domain constraint as Person ; this means any object holding a hasMother property will be inferred to be a Person . Similarly, any object at the right-hand end of a hasMother property will be inferred to be a Woman , which is the result we need. The same reasoning goes for hasFather and hasParent , with the sex constraints on the latter being only Person . The inverses of the two functional sub-properties of hasParent are not themselves functional. After all, a Woman can be the mother of many Person objects, but each Person object can have only one mother. Figure 4.1: The property hierarchy with the hasSex and the parentage properties Figure 4.2: the core TBox for the FHKB with the Person and Sex classes. Task 10: Restrict Person class As each and every person has a mother and each and every person has a father, place restrictions on the Person class as shown below. Class: Person SubClassOf: DomainEntity, (hasFather some Man), (hasMother some Woman), (hasSex some Sex) DisjointWith: Sex Task 11: DL queries for people and sex Issue the DL queries for Person , Man and Woman ; look at the answers and count the numbers in each class; which individuals have no sex and why? You should find that many people have been inferred to be either Man or Woman , but some are, as we will see below, only inferred to be Person . The domain and range constraints on our properties have also driven some entailments. We have not asserted that David_Bright_1934 is a member of Man , but the range constraint on hasFather (or the inferred domain constraint on the isFatherOf relation) has enabled this inference to be made. This goes for any individual that is the right-hand-side (either inferred or asserted) of either hasFather or hasMother (where the range is that of Woman ). For Robert David Bright, however, he is only the left-hand-side of an hasFather or an hasMother property, so we\u2019ve only entailed that this individual is a member of Person . 4.5 Who has a father? \u00b6 In our description of the Person class we have said that each and every instance of the class Person has a father (the same goes for mothers). So, when we ask the query \u2018which individuals have a father\u2019, we get all the instances of Person back, even though we have said nothing about the specific parentage of each Person . We do not know who their mothers and fathers are, but we know that they have one of each. We know all the individuals so far entered are members of the Person class; when asserting the type to be either Man or Woman (each of which is a subclass of Person ), we infer that each is a person. When asserting the type of each individual via the hasSex property, we know each is a Person , as the domain of hasSex is the Person class. As we have also given the right-hand side of hasSex as either Maleness or Femaleness , we have given sufficient information to recognise each of these Person instances to be members of either Man or Woman . 4.6 Filling in Domains and Ranges for the FHKB Properties \u00b6 So far we have not systematically added domains and ranges to the properties in the FHKB. As a reminder, when a property has a domain of X any object holding that property will be inferred to be a member of class X . A domain doesn\u2019t add a constraint that only members of class X hold that property; it is a strong implication of class membership. Similarly, a property holding a range implies that an object acting as right-hand-side to a property will be inferred to be of that class. We have already seen above that we can use domains and ranges to imply the sex of people within the FHKB. Do the following: Task 12: Domains and Ranges Make sure the appropriate Person , Man and Woman are domains and ranges for hasFather , hasMother and hasParent . Run the reasoner and look at the property hierarchy. Also look at the properties hasAncestor , hasGrandparent , hasUncle and so on; look to see what domains and ranges are found. Add any domains and ranges explicitly as necessary. Prot\u00e9g\u00e9 for example in its current version (November 2015) does not visualise inherited domains and ranges in the same way as it shows inferred inverse relations. We typically assert more domains and ranges than strictly necessary. For example, if we say that hasParent has the domain Person , this means that every object x that is connected to another object y via the hasParent relation must be a Person . Let us assume the only thing we said about x and y is that they are connected by a hasMother relation. Since this implies that x and y are also connected by a hasParent relation ( hasMother is a sub-property of hasParent ) we do not have to assert that hasFather has the domain of Person ; it is implied by what we know about the domain and range of hasParent . In order to remove as many assertions as possible, we may therefore choose to assert as much as we know starting from the top of the hierarchy, and only ever adding a domain if we want to constrain the already inferred domain even further (or range respectively). For example, in our case, we could have chosen to assert Person to be the domain of hasRelation . Since hasRelation is symmetric, it will also infer Person to be the range. We do not need to say anything for hasAncestor or hasParent , and only if we want to constrain the domain or range further (like in the case of hasFather by making the range Man ) do we need to actually assert something. It is worth noting that because we have built the object property hierarchy from the bottom ( hasMother etc.) we have ended up asserting more than necessary. 4.7 Inconsistencies \u00b6 From the Pizza Tutorial and other work with OWL you should have seen some unsatisfiabilities . In Prot\u00e9g\u00e9 this is highlighted by classes going \u2018red\u2019 and being subclasses ofNothing; that is, they can have no instances in that model. Task 13: Inconsistencies Add the fact Robert_David_Bright_1965 hasMother David_Bright_1934 . Run the classifier and see what happens. Remove that fact and run the classifier again. Now add the fact that Robert_David_Bright_1965 hasMother Iris_Ellen_Archer_1907 Run the classifier and see what happens. Add and remove the functional characteristic to these properties and see what happens. After asserting the first fact it should be reported by the reasoner that the ontology is inconsistent . This means, in lay terms, that the model you\u2019ve provided in the ontology cannot accommodate the facts you\u2019ve provided in the fact assertions in your ABox\u2014that is, there is an inconsistency between the facts and the ontology... The ontology is inconsistent because David_Bright_1934 is being inferred to be a Man and a Woman at the same time which is inconsistent with what we have said in the FHKB. When we, however, say that Robert David Bright has two different mothers, nothing bad happens! Our domain knowledge says that the two women are different, but the reasoner does not know this as yet... ; Iris Ellen Archer and Margaret Grace Rever may be the same person; we have to tell the reasoner that they are different. For the same reason the functional characteristic also has no effect until the reasoner \u2018knows\u2019 that the individuals are different. We will do this in Section 7.1.1 and live with this \u2018fault\u2019 for the moment. 4.8 Adding Some Defined Classes for Ancestors and so on \u00b6 Task 14: Adding defined classes Add a defined class for Ancestor , MaleAncestor , FemaleAncestor ; Add a defined class for Descendant , MaleDescendant and FemaleDescendant ; Run the reasoner and view the resulting hierarchy. The code for the classes looks like: Class: Ancestor EquivalentTo: Person and isAncestorOf some Person Class: FemaleAncestor EquivalentTo: Woman and isAncestorOf some Person Class: Descendant EquivalentTo: Person and hasAncestor some Person Class: MaleDescendant EquivalentTo: Man and hasAncestor some Person The TBox after reasoning can be seen in Figure 4.3. Notice that the reasoner has inferred that several of the classes are equivalent or \u2018the same\u2019. These are: Descendant and Person ; MaleDescendant and Man , FemaleDescendant and Woman . The reasoner has used the axioms within the ontology to infer that all the instances of Person are also instances of the class Descendant and that all the instances of Woman are also the same instances as the class Female Descendant . This is intuitively true; all people are descendants \u2013 they all have parents that have parents etc. and thus everyone is a descendant. All women are female people that have parents etc. As usual we should think about the objects within the classes and what we know about them. This time it is useful to think about the statements we have made about Person in this Chapter \u2013 that all instances of Person have a father and a mother; add to this the information from the property hierarchy and we know that all instances of Person have parents and ancestors. We have repeated all of this in our new defined classes for Ancestor and Descendant and the reasoner has highlighted this information. Figure 4.3: The defined classes from Section 4.8 in the FHKB\u2019s growing class hierarchy Task 15: More Ancestors Query for MaleDescendant . You should get Man back - they are equivalent (and this makes sense). As an additional exercise, also add in properties for forefathers and foremothers. You will follow the same pattern as for hasAncestor , but adding in, for instance, hasFather as the sub-property of the transitive super-property of hasForefather and setting the domains and ranges appropriately (or working out if they\u2019ll be inferred appropriately). Here we interpret a forefather as one\u2019s father\u2019s father etc. This isn\u2019t quite right, as a forefather is any male ancestor, but we\u2019ll do it that way anyway. You might want to play around with DL queries. Because of the blowup in inferred relationships, we decided to not include this pattern in the tutorial version of the FHKB. 4.9 Summary \u00b6 Most of what we have done in this chapter is straight-forward OWL, all of which would have been met in the pizza tutorial. It is, however, a useful revision and it sets the stage for refining the FHKB. Figure 4.2 shows the basic set-up we have in the FHKB in terms of classes; we have a class to represent person, man and woman, all set-up with a description of sex, maleness and femaleness. It is important to note, however, the approach we have taken: We have always thought in terms of the objects we are modelling. Here are some things that should now be understood upon completing this chapter: Restrictions on a class in our TBox mean we know stuff about individuals that are members of that class, even though we have asserted no facts on those individuals. We have said, for instance, that all members of the class Person have a mother, so any individual asserted to be a Person must have a mother. We do not necessarily know who they are, but we know they have one. Some precision is missing \u2013 we only know Robert David Bright is a Person , not that he is a Man . This is because, so far, he only has the domain constraint of hasMother and hasFather to help out. We can cause the ontology to be inconsistent, for example by providing facts that cannot be accommodated by the model of our ontology. In the example, David Bright was inferred to be a member of two disjoint classes. Finally, we looked at some defined classes. We inferred equivalence between some classes where the extents of the classes were inferred to be the same \u2013 in this case the extents of Person and Descendant are the same. That is, all the objects that can appear in Person will also be members of Descendant . We can check this implication intuitively \u2013 all people are descendants of someone. Perhaps not the most profound inference of all time, but we did no real work to place this observation in the FHKB. This last point is a good general observation. We can make the reasoner do work for us. The less maintenance we have to do in the FHKB the better. This will be a principle that works throughout the tutorial. The FHKB ontology at this stage of the tutorial has an expressivity of SRIF. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.884 sec (0.00047 % of final), by Pellet 2.2.0 0.256 sec (0.00207 % of final) and by FaCT++ 1.6.4 is approximately 0.013 sec (0.000 % of final). 0 sec indicates failure or timeout. Chapter 5 \u00b6 Siblings in the FHKB \u00b6 In this chapter you will: Explore options for determining finding siblings; Meet some of the limitations in OWL; Choose one of the options explored; Add facts for siblings; Use sub-property chains to find aunts and uncles; There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial 5.1 Blood relations \u00b6 Do the following first: Task 16: The bloodrelation object property Create an hasBloodrelation object property, making it a sub-property of hasRelation . Add appropriate property characteristics. Make the already existing hasAncestor property a sub-property of hasBloodrelation . Does a blood relation of Robert have the same relationship to Robert (symmetry)? Is a blood relation of Robert\u2019s blood relation a blood relation of Robert (transitivity)? Think of an aunt by marriage; her children are my cousins and blood relations via my uncle, but my aunt is not my blood relation. My siblings share parents; male siblings are brothers and female siblings are sisters. So far we have asserted parentage facts for the Person in our ABox. Remember that our parentage properties have inverses, so if we have added an hasFather property between a Person and a Man , we infer the isFatherOf property between that Man and that Person . 5.2 Siblings: Option One \u00b6 We should have enough information within the FHKB to infer siblings. We could use a sub-property chain such as: ObjectProperty: hasSibling SubPropertyOf: hasBloodrelation Characteristics: Symmetric, transitive SubPropertyChain: hasParent o isParentOf We make a property of hasSibling and make it a sub-property of hasBloodrelation . Remember, think of the objects involved and the implications we want to follow; being a sibling implies being a blood relation, it does not imply any of the other relationships we have in the FHKB. Note that we have made hasSibling symmetric; if Robert is sibling of Richard, then Richard is sibling of Robert. We should also think about transitivity; if David is sibling of Peter and Peter is sibling of John, then David is sibling of John. So, we make hasSibling symmetric and transitive (see Figure 5.1). However, we must take care of half-siblings: child 1 and child 2 share a mother, but not a father; child 2 and child 3 share the father, but not the mother \u2013 child 1 and child 3 are not even half-siblings. However, at least for the moment, we will simply ignore this inconvenience, largely so that we can explore what happens with different modelling options. Figure 5.1: Showing the symmetry and transitivity of the hasSibling (siblingof) property by looking at the brothers David, John and Peter We also have the implication using three objects (see Figure 5.2): Robert holds a hasParent property with David; David holds an isFatherOf property with Richard; This implies that Robert holds a hasSibling property with Richard; As hasSibling is symmetric, Richard holds an hasSibling property with Robert. Figure 5.2: Tracing out the sub-property chain for hasSibling ; note that Robert is a sibling of himself by this path Do the following tasks: Task 17: Siblings Add the hasSibling property as above; Run the reasoner; Ask the DL query hasSibling value Robert_David_Bright_1965 . From this last DL query you should get the answer that both Robert and Richard are siblings of Robert. Think about the objects involved in the sub-property chain: we go from Robert to David via the hasParent and from David to Richard via the isParentOf property; so this is OK. However, we also go from Robert to David and then we can go from David back to Robert again \u2013 so Robert is a sibling of Robert. We do not want this to be true. We can add another characteristic to the hasSibling property, the one of being irreflexive . This means that an object cannot hold the property with itself. Task 18: More siblings Add the irreflexive characteristic to the hasSibling property; Run the reasoner; Note that the reasoner claims you have an inconsistent ontology (or in some cases, you might get a message box saying \"Reasoner died\"). Looking at the hasSibling property again, the reason might not be immediately obvious. The reason for the inconsistency lies in the fact that we create a logical contradiction: through the property chain, we say that every Person is a sibling of him or herself, and again disallowing just that by adding the irreflexive characteristic. A different explanation lies within the OWL specification itself: In order to maintain decidability irreflexive properties must be simple - for example, they may not be property chains 6 . 6 http://www.w3.org/TR/owl2-syntax/#The_Restrictions_on_the_Axiom_Closure 5.2.1 Brothers and Sisters \u00b6 We have only done siblings, but we obviously need to account for brothers and sisters. In an analogous way to motherhood, fatherhood and parenthood, we can talk about sex specific sibling relationships implying the sex neutral hasSibling ; holding either a hasBrother or an isSisterOf between two objects would imply that a hasSibling property is also held between those two objects. This means that we can place these two sex specific sibling properties below hasSibling with ease. Note, however, that unlike the hasSibling property, the brother and sister properties are not symmetric. Robert hasBrother Richard and vice versa , but if Daisy hasBrother William, we do not want William to hold an hasBrother property with Daisy. Instead, we create an inverse of hasBrother , isBrotherOf , and the do the same for isSisterOf . We use similar, object based, thought processes to choose whether to have transitivity as a characteristic of hasBrother . Think of some sibling objects or individuals and place hasBrother properties between them. Make it transitive and see if you get the right answers. Put in a sister to and see if it stil works. If David hasBrother Peter and Peter hasBrother John, then David hasBrother John; so, transitivity works in this case. Think of another example. Daisy hasBrother Frederick, and Frederick hasBrother William, thus Daisy hasBrother William. The inverses work in the same way; William isBrotherOf Frederick and Frederick isBrotherOf Daisy; thus William isBrotherOf Daisy. All this seems reasonable. Task 19: Brothers and sisters Create the hasBrother object property as shown below; Add hasSister in a similar manner; 3. Add appropriate inverses, domains and ranges. ObjectProperty: hasBrother SubPropertyOf: hasSibling Characteristics: Transitive InverseOf: isBrotherOf Range: Man We have some hasSibling properties (even if they are wrong). We also know the sex of many of the people in the FHKB through the domains and ranges of properties such as hasFather , hasMother and their inverses.. Can we use sub-property chains in the same way as we have used them in the hasSibling property? The issue is that of sex; the property isFatherOf is sex neutral at the child end, as is the inverse hasFather (the same obviously goes for the mother properties). We could use a sub-property chain of the form: ObjectProperty: hasBrother SubPropertyChain: hasParent o hasSon A son is a male child and thus that object is a brother of his siblings. At the moment we do not have son or daughter properties. We can construct a property hierarchy as shown in Figure 5.3. This is made up from the following properties: hasChild and isChildOf hasSon (range Man and domain Person ) and isSonOf ; hasDaughter (range Woman domain Person ) and isDaughterOf Note that hasChild is the equivalent of the existing property isParentOf ; if I have a child, then I am its parent. OWL 2 can accommodate this fact. We can add an equivalent property axiom in the following way: ObjectProperty: isChildOf EquivalentTo: hasParent We have no way of inferring the isSonOf and isDaughterOf from what already exists. What we want to happen is the implication of \u2018 Man and hasParent Person implies isSonOf \u2019. OWL 2 and its reasoners cannot do this implication. It has been called the \u2018man man problem\u2019 7 . Solutions for this have been developed [3], but are not part of OWL 2 and its reasoners. Figure 5.3: The property hierarchy for isChildOf and associated son/daughter properties 7 http://lists.w3.org/Archives/Public/public-owl-dev/2007JulSep/0177.html Child property Parent Robert David Bright 1965 isSonOf David Bright 1934, Margaret Grace Rever 1934 Richard John Bright 1962 isSonOf David Bright 1934, Margaret Grace Rever 1934 Mark Bright 1956 isSonOf John Bright 1930, Joyce Gosport Ian Bright 1959 isSonOf John Bright 1930, Joyce Gosport Janet Bright 1964 isDaughterOf John Bright 1930, Joyce Gosport William Bright 1970 isSonOf John Bright 1930, Joyce Gosport Table 5.1: Child property assertions for the FHKB Thus we must resort to hand assertions of properties to test out our new path: Task 20: Sons and daughters Add the property hierarchy shown in Figure 5.3, together with the equivalent property axiom and the obvious inverses. As a test (after running the reasoner), ask the DL query isChildOf value David_Bright_1934 and you should have the answer of Richard and Robert; Add the sub-property paths as described in the text; Add the assertions shown in Table 5.1; Run the reasoner; Ask the DL query for the brother of Robert David Bright and the sister of Janet. Of course, it works, but we see the same problem as above. As usual, think of the objects involved. Robert isSonOf David and David isParentOf Robert, so Robert is his own brother. Irreflexivity again causes problems as it does above (Task 18). 5.3 Siblings: Option two \u00b6 Our option one has lots of problems. So, we have an option of asserting the various levels of sibling. We can take the same basic structure of sibling properties as before, but just fiddle around a bit and rely on more assertion while still trying to infer as much as possible. We will take the following approach: We will take off the sub-property chains of the sibling properties as they do not work; We will assert the leaf properties of the sibling sub-hierarchy sparsely and attempt to infer as much as possible. Person Property Person Robert David Bright 1965 isBrotherOf Richard John Bright 1962 David Bright 1934 isBrotherOf John Bright 1930 David Bright 1934 isBrotherOf Peter William Bright 1941 Janet Bright 1964 isSisterOf Mark Bright 1956 Janet Bright 1964 isSisterOf Ian Bright 1959 Janet Bright 1964 isSisterOf William Bright 1970 Mark Bright 1956 isBrotherOf Ian Bright 1959 Mark Bright 1956 isBrotherOf Janet Bright 1964 Mark Bright 1956 isBrotherOf William Bright 1970 Table 5.2: The sibling relationships to add to the FHKB. Do the following: Task 21: Add sibling assertions Remove the sub-property chains of the sibling properties and the isChildOf assertions as explained above. Add the Sibling assertions shown in table 5.2; Run the reasoner; Ask isBrotherOf value Robert_David_Bright_1965 ; Ask isBrotherOf value Richard_John_Bright_1962 ; Ask hasBrother value Robert_David_Bright_1965 ; Ask hasBrother value Richard_John_Bright_1962 ; Ask isSisterOf value William_Bright_1970 ; Ask the query Man and hasSibling value Robert_David_Bright_1965 . We can see some problems with this option as well: With these properties asserted, Richard only has a hasBrother property to Robert. We would really like an isBrotherOf to Robert to hold. The query Man and hasSibling value Robert only retrieves Robert himself. Because we only asserted that Robert is a brother of Richard, and the domain of isBrotherOf is Man we know that Robert is a Man , but we do not know anything about the Sex of Richard. 5.3.1 Which Modelling Option to Choose for Siblings? \u00b6 Which of the two options gives the worse answers and which is the least effort? Option one is obviously the least effort; we only have to assert the same parentage facts as we already have; then the sub-property chains do the rest. It works OK for hasSibling , but we cannot do brothers and sisters adequately; we need Man and hasSibling \u2290 isBrotherOf and we cannot do that implication. This means we cannot ask the questions we need to ask. So, we do option two, even though it is hard work and is still not perfect for query answering, even though we have gone for a sparse assertion mode. Doing full sibling assertion would work, but is a lot of effort. We could start again and use the isSonOfandisDaughterOf option, with the sub-property chains described above. This still has the problem of everyone being their own sibling. It can get the sex specific sibling relationships, but requires a wholesale re-assertion of parentage facts. We will continue with option two, largely because it highlights some nice problems later on. 5.4 Half-Siblings \u00b6 In Section 5.2 we briefly talked about half-siblings. So far, we have assumed full-siblings (or, rather, just talked about siblings and made no distinction). Ideally, we would like to accommodate distinctions between full- and half-siblings; here we use half-siblings, where only one parent is in common between two individuals, as the example. The short-answer is, unfortunately, that OWL 2 cannot deal with half-siblings in the way that we want - that is, such that we can infer properties between named individuals indicating full- or half-sibling relationships. It is possible to find sets of half-brothers in the FHKB by writing a defined class or DL-query for a particular individual.} The following fragment of OWL defines a class that looks for the half-brothers of an individual called \u2018Percival\u2019: Class: HalfBrotherOfPercival EquivalentTo: Man and (((hasFather some (not (isFatherOf value Percival))) and (hasMother some (isMotherOf value Percival))) or ((hasFather some (isFatherOf value Percival)) and (hasMother some (not (isMotherOf value Percival))))) Here we are asking for any man that either has Percival\u2019s father but not his mother, or his mother, but not his father. This works fine, but is obviously not a general solution. The OWL description is quite complex and the writing will not scale as the number of options (hypothetically, as the number of parents increases... ) increases; it is fine for man/woman, but go any higher and it will become very tedious to write all the combinations. Another way of doing this half-brother class to find the set of half-brothers of a individual is to use cardinality constraints: Class: HalfBrotherOfPercival EquivalentTo: Man and (hasParent exactly 1 (isParentOf value Percival)) This is more succinct. We are asking for a man that has exactly one parent from the class of individuals that are the class of Percival\u2019s parents. This works, but one more constraint has to be present in the FHKB. We need to make sure that there can be only two parents (or indeed, just a specified number of parents for a person). If we leave it open as to the number of parents a person has, the reasoner cannot work out that there is a man that shares exactly one parent, as there may be other parents. We added this constraint to the FHKB in Section 6.2; try out the classes to check that they work. These two solutions have been about finding sets of half-brothers for an individual. What we really want in the FHKB is to find half-brothers between any given pair of individuals. Unfortunately we cannot, without rules, ask OWL 2 to distinguish full- and half-siblings \u2013 we cannot count the number of routes taken between siblings via different distinct intermediate parent objects. 5.5 Aunts and Uncles \u00b6 An uncle is a brother of either my mother or father. An aunt is a sister of either my mother or father. In common practice, wives and husbands of aunts and uncles are usually uncles and aunts respectively. Formally, these aunts and uncles are aunts-in-law and uncles-in-law. Whatever approach we take, we cannot fully account for aunts and uncles until we have information about marriages, which will not have until Chapter 9. We will, however, do the first part now. Look at the objects and properties between them for the following facts: Robert has father David and mother Margaret; David has brothers Peter and John; Margaret has a sister Eileen; Robert thus has the uncles John and Peter and an aunt Eileen. As we are tracing paths or \u2018chains\u2019 of objects and properties we should use sub-property chains as a solution for the aunts and uncles. We can make an hasUncle property as follows (see Figure 5.4): ObjectProperty: hasUncle SubPropertyOf: hasBloodrelation Domain: Man Range: Person SubPropertyChain: hasParent o hasBrother InverseOf: isUncleOf Figure 5.4: Tracing out the path between objects to get the hasUncle sub-property chain. Notice we have the domain of Man and range of Person . We also have an inverse. As usual, we can read this as \u2018an object that holds an hasParent property, followed by an object holding a hasBrother property, implies that the first object holds an hasUncle property with the last object\u2019. Note also where the properties (include the ones for aunt) go in the object property hierarchy. Aunts and uncles are not ancestors that are in the direct blood line of a person, but they are blood relations (in the narrower definition that we are using). Thus the aunt and uncle properties go under the hasBloodrelation property (see Figure 5.5). Again, think of the implications between objects holding a property between them; that two objects linked by a property implies that those two objects also hold all the property\u2019s super-properties as well. As long as all the super-properties are true, the place in the object property hierarchy is correct (think about the implications going up, rather than down). Figure 5.5: The object property hierarchy with the aunt and uncle properties included. On the right side, we can see the hasUncle property as shown by Prot\u00e9g\u00e9. Do the following tasks: Task 22: Uncles and Aunts Add the hasUncle property as above; Add the hasAunt property as well; Ask for the uncles of Julie_Bright_1966 and for Mark_Bright_1956 ; Add similar properties for hasGreatUncle and hasGreatAunt and place them in the property hierarchy. We can see this works \u2013 unless we have any gaps in the sibling relationships (you may have to fix these). Great aunts and uncles are simply a matter of adding another \u2018parent\u2019 leg into the sub-property chain. We are not really learning anything new with aunts and uncles, except that we keep gaining a lot for free through sub-property chains. We just add a new property with its sub-property chain and we get a whole lot more inferences on individuals. To see what we now know about Robert David Bright, do the following: Task 23: What do we know? Save the ontology and run the reasoner; Look at inferences related to the individual Robert David Bright (see warning in the beginning of this chapter). If you chose to use DL queries in Prot\u00e9g\u00e9, do not forget to tick the appropriate check-boxes. You can now see lots of facts about Robert David Bright, with only a very few actual assertions directly on Robert David Bright. 5.6 Summary \u00b6 Siblings have revealed several things for us: We can use just the parentage facts to find siblings, but everyone ends up being their own sibling; We cannot make the properties irreflexive, as the knowledge base becomes inconsistent; We would like an implication of Man and hasSibling \u2283 isBrotherOf , but OWL 2 doesn\u2019t do this implication; Whatever way we model siblings, we end up with a bit of a mess; OWL 2 cannot do half-siblings; However, we can get close enough and we can start inferring lots of facts via sub-property chains using the sibling relationships. The FHKB ontology at this stage of the tutorial has an expressivity ofSRIF. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 1355.614 sec (0.71682 % of final), by Pellet 2.2.0 0.206 sec (0.00167 % of final) and by FaCT++ 1.6.4 is approximately 0.039 sec (0.001 % of final). 0 sec indicates failure or timeout. Chapter 6 \u00b6 Individuals in Class Expressions \u00b6 In this chapter you will: Use individuals within class expressions; Make classes to find Robert and Richard\u2019s parents, ancestors, and so on; Explore equivalence of such classes; Re-visit the closed world. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial 6.1 Richard and Robert\u2019s Parents and Ancestors \u00b6 So far we have only used object properties between unspecified objects. We can, however, specify a specific individual to act at the right-hand-side of a class restriction or type assertion on an individual. The basic syntax for so-called nominals is: Class: ParentOfRobert EquivalentTo: Person and isParentOf valueRobert_David_Bright_1965 This is an equivalence axiom that recognises any individual that is a Person and a parent of Robert David Bright. Task 24: Robert and Richards parents Create the class ParentOfRobert as described above; Classify \u2013 inspect where the class is placed in the FHKB TBox and look at which individuals classify as members of the class; Do the same for a class with the value of Richard_John_Bright_1962 and classify; Finally create a class ParentOfRichardAndRobert , defining it as Person and isParentOf some {Robert_David_Bright_1965 ,Richard_John_Bright_1962 } ; again see what happens on classification. Note that the expressions isMotherOf value Robert_David_Bright_1965 and isMotherOf some {Robert_David_Bright_1965 } are practically identical. The only difference is that using value , you can only specify one individual, while some relates to a class (a set of individuals). We see that these queries work and that we can create more complex nominal based class expressions. The disjunction above is isParentOf some {Robert_David_Bright_1965, Richard_John_Bright_1965} The \u2018{\u2019 and \u2018}\u2019 are a bit of syntax that says \u2018here\u2019s a class of individual\u2019. We also see that the classes for the parents of Robert David Bright and Richard John Bright have the same members according to the FHKB, but that the two classes are not inferred to be equivalent. Our domain knowledge indicates the two classes have the same extents (members) and thus the classes are equivalent, but the automated reasoner does not make this inference. As usual, this is because the FHKB has not given the automated reasoner enough information to make such an inference. 6.2 Closing Down What we Know About Parents and Siblings \u00b6 The classes describing the parents of Richard and Robert are not equivalent, even though, as humans, we know their classes of parent are the same. We need more constraints so that it is known that the four parents are the only ones that exist. We can try this by closing down what we know about the immediate family of Robert David Bright. In Chapter 4 we described that a Person has exactly one Woman and exactly one Man as mother and father (by saying that the hasMother and hasFather properties are functional and thus only one of each may be held by any one individual to distinct individuals). The parent properties are defined in terms of hasParent , hasMother and hasFather . The latter two imply hasParent . The two sub-properties are functional, but there are no constraints on hasParent , so an individual can hold many instances of this property. So, there is no information in the FHKB to say a Person has only two parents (we say there is one mother and one father, but not that there are only two parents). Thus Robert and Richard could have other parents and other grandparents than those in the FHKB; we have to close down our descriptions so that only two parents are possible. There are two ways of doing this: Using qualified cardinality constraints in a class restriction; Putting a covering axiom on hasParent in the same way as we did for Sex in Chapter 4. Task 25: Closing the Person class Add the restriction hasParent exactly 2 Person to the classPerson ; Run the reasoner; Inspect the hierarchy to see where ParentOfRobert and ParentOfRichard are placed and whether or not they are found to be equivalent; Now add the restriction hasParent max 2 Person to the class Person ; Run the reasoner (taking note of how long the reasoning takes) and take another look. We find that these two classes are equivalent; we have supplied enough information to infer that these two classes are equivalent. So, we know that option one above works, but what about option two? This takes a bit of care to think through, but the basic thing is to think about how many ways there are to have a hasParent relationship between two individuals. We know that we can have either a hasFather or a hasMother property between two individuals; we also know that we can have only one of each of these properties between an individual and a distinct individual. However, the open world assumption tells us that there may be other ways of having a hasParent property between two individuals; we\u2019ve not closed the possibilities. By putting on the hasParent exactly 2 Person restriction on the Person class, we are effectively closing down the options for ways that a person can have parents; we know because of the functional characteristic on hasMother and hasFather that we can have only one of each of these and the two restrictions say that one of each must exist. So, we know we have two ways of having a parent on each Person individual. So, when we say that there are exactly two parents (no more and no less) we have closed down the world of having parents\u2014thus these two classes can be inferred to be equivalent. It is also worth noting that this extra axiom on the Person class will make the reasoner run much more slowly. Finally, for option 2, we have no way of placing a covering axiom on a property. What we\u2019d like to be able to state is something like: ObjectProperty: hasParent EquivalentTo: hasFather or hasMother but we can\u2019t. 6.3 Summary \u00b6 For practice, do the following: Task 26: Additional Practice Add lots more classes using members of the ABox as nominals; Make complex expressions using nominals; After each addition of a nominal, classify and see what has been inferred within the FHKB. See if you can make classes for GrandparentOfRobert and GrandparentOfRichard and make them inferred to be equivalent. In this chapter we have seen the use of individuals within class expressions. It allows us to make useful queries and class definitions. The main things to note is that it can be done and that there is some syntax involved. More importantly, some inferences may not be as expected due to the open world assumption in OWL. By now you might have noticed a significant increase in the time the reasoner needs to classify. Closing down what we know about family relationships takes its toll on the reasoner performance, especially the usage of 'hasParent exactly 2 Person'. At this point we recommend rewriting this axiom to 'hasParent max 2 Person'. It gives us most of what we need, but has a little less negative impact on the reasoning time. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 2067.273 sec (1.09313 % of final), by Pellet 2.2.0 0.529 sec (0.00428 % of final) and by FaCT++ 1.6.4 is approximately 0.147 sec (0.004 % of final). 0 sec indicates failure or timeout. Chapter 7 \u00b6 Data Properties in the FHKB \u00b6 We now have some individuals with some basic object properties between individuals. OWL 2, however, also has data properties that can relate an object or individual to some item of data. There are data about a Person , such as years of events and names etc. So, in this Chapter you will: Make some data properties to describe event years to people; Create some simple defined classes that group people by when they were born; Try counting the numbers of children people have... Deal with the open world assumption; Add given and family names to individuals in the FHKB. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial. 7.1 Adding Some Data Properties for Event Years \u00b6 Everyone has a birth year; death year; and some have a marriage year and so on. We can model these simply with data properties and an integer as a filler. OWL 2 has a DateTime datatype, where it is possible to specify a precise time and date down to a second. 7 This proves cumbersome (see http://robertdavidstevens.wordpress.com/2011/05/05/using-the-datetime-data-type-to-describe-birthdays/ for details); all we need is a simple indication of the year in which a person was born. Of course, the integer type has a zero, which the Gregorian calendar for which we use integer as a proxy does not, but integer is sufficient to our needs. Also, there are various ontological treatments of time and information about people (this extends to names etc. as well), but we gloss over that here\u2014that\u2019s another tutorial. 7 http://www.w3.org/TR/2008/WD-owl2-quick-reference-20081202/#Built-in_Datatypes_and_Facets We can have dates for birth, death and (eventually) marriage (see Chapter 9) and we can just think of these as event years. We can make a little hierarchy of event years as shown in Figure 7.1). Task 27: Create a data property hierarchy Create the data property hasEventYear with range integer and domain Person ; Create the data property hasBirthYear and make it a sub-property of hasEventYear (that way, the domain and range of hasEventYear are inherited); Create the data property hasDeathYear and make it a sub-property of hasEventYear ; For each individual add the birth years shown in Table A.1 (see appendix). You do not actually have to go back to the table\u2014it is easier to read the birth years simply off the individual names. Again, asserting birth years for all individuals can be a bit tedious. The reader can find a convenience snapshot of the ontology at this stage at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial We now have an ABox with individuals with fact assertions to data indicating a birth year. We can, if we wish, also add a class restriction to the Person class saying that each and every instance of the class Person holds a data property to an integer and that this property is called \u2018hasBirthYear\u2019. As usual when deciding whether to place such a restriction upon a class, ask whether it is true that each and every instance of the class holds that property; this is exactly the same as we did for the object properties in Chapter 4. Everyone does have a birth year, even if it is not known. Once birth years have been added to our individuals, we can start asking some questions. Task 28: DL queries 1. Use a DL query to ask: Person born after 1960; Person born in the 1960s; Person born in the 1800s; Person that has fewer than three children; Person that has more than three children. The DL query for people born in the 1960s is: Person and hasBirthYear some int[>= 1960, < 1970] This kind of interval is known as a facet. 7.1.1 Counting Numbers of Children \u00b6 The last two queries in the list do not work as expected. We have asked, for instance, for Person that have more than three children, but we get no members of Person in the answer, though we know that there are some in the FHKB (e.g., John_Bright_1930 ). This is because there is not enough information in the FHKB to tell that this person has more than three different people as children. As humans we can look at the four children of John Bright and know that they are different \u2013 for instance, they all have different birth years. The automated reasoner, however, does not know that a Person can only have one birth year. Task 29: Make a functional object property Make the property hasBirthYear functional. Ask the query for Person that has more than three children again. This time the query should work. All the other event year properties should be made functional, expect hasEventYear , as one individual can have many event years. As the children have different birth year and an individual can only hold one hasBirthYear property, then these people must be distinct entities. Of course, making birth year functional is not a reliable way of ensuring that the automated reasoner knows that the individual are different. It is possible for two Person to have the same birth year within the same family \u2013 twins and so on. Peter_William_Bright_1941 has three children, two of which are twins, so will not be a member of the class of people with at least three children. So, we use the different individuals axiom. Most tools, including Prot\u00e9g\u00e9, have a feature that allows all individuals to be made different. Task 30: Make all individuals different Make all individuals different; Ask the above queries again. From now on, every time you add individuals, make sure the different individuals axiom is updated. 7.2 The Open World Assumption \u00b6 We have met again the open world assumption and its importance in the FHKB. In the use of the functional characteristic on the hasBirthYear property, we saw one way of constraining the interpretation of numbers of children. We also introduced the \u2018different individuals\u2019 axiom as a way of making all individuals in a knowledge base distinct. There are more questions, however, for which we need more ways of closing down the openness of OWL 2. Take the questions: People that have exactly two children; People that have only brothers; People that have only female children. We can only answer these questions if we locally close the world.We have said that David and Margaret have two children, Richard and Robert, but we have not said that there are not any others. As usual, try not to apply your domain knowledge too much; ask yourself what the automated reasoner actually knows. As we have the open world assumption, the reasoner will assume, unless otherwise said, that there could be more children; it simply doesn\u2019t know. Think of a railway journey enquiry system. If I ask a standard closed world system about the possible routes by rail, between Manchester and Buenos Aires, the answer will be \u2019none\u2019, as there are none described in the system. With the open world assumption, if there is no information in the system then the answer to the same question will simply be \u2018I don\u2019t know\u2019. We have to explicitly say that there is no railway route from Manchester to Buenos Aires for the right answer to come back. We have to do the same thing in OWL. We have to say that David and Margaret have only two children. We do this with a type assertion on individuals. So far we have only used fact assertions. A type assertion to close down David Bright\u2019 parentage looks like this: isParentOf only {Robert_David_Bright_1965,Richard_John_Bright_1962 } This has the same meaning as the closure axioms that you should be familiar with on classes. We are saying that the only fillers that can appear on the right-hand-side of the isParentOf property on this individual are the two individuals for Richard and Robert. We use the braces to represent the set of these two individuals. Task 31: Make a closure axiom Add the closure assertion above to David Bright; Issue the DL query isParentOf exactly 2 Person . The last query should return the answer of David Bright. Closing down the whole FHKB ABox is a chore and would really have to be done programmatically. OWL scripting languages such as the Ontology Preprocessing Language 8 (OPPL) [2] can help here. Also going directly to the OWL API [1] 9 , if you know what you are doing, is another route. Adding all these closure type assertions can slow down the reasoner; so think about the needs of your system \u2013 just adding it \u2018because it is right\u2019 is not necessarily the right route. 8 http://oppl2.sourceforge.net 9 http://owlapi.sourceforge.net/ 7.3 Adding Given and Family Names \u00b6 We also want to add some other useful data facts to people \u2013 their names. We have been putting names as part of labels on individuals, but data fact assertions make sense to separate out family and given names so that we can ask questions such as \u2018give me all people with the family name Bright and the first given name of either James or William\u2019. A person\u2019s name is a fact about that person and is more, in this case, than just a label of the representation of that person. So, we want family names and given names. A person may have more than one given name \u2013 \u2018Robert David\u2019, for instance \u2013 and an arbitrary number of given names can be held. For the FHKB, we have simply created two data properties of hasFirstGivenName and hasSecondGivenName ). Ideally, it would be good to have some index on the property to given name position, but OWL has no n-ary relationships. Otherwise, we could reify the hasGivenName property into a class of objects, such as the following: Class: GivenName SubClassOf:hasValue some String, hasPosition some Integer but it is really rather too much trouble for the resulting query potential. As already shown, we will use data properties relating instances of Person to strings. We want to distinguish family and given names, and then different positions of given names through simple conflating of position into the property name. Figure 7.1 shows the intended data property hierarchy. Figure 7.1: The event year and name data property hierarchies in the FHKB. Do the following: Task 32: Data properties Create the data properties as described in Figure 7.1; Give the hasName property the domain of Person and the range of String ; Make the leaf properties of given names functional; Add the names shown in Table A.1 (appendix); Again, it may be easier to read the names of the individual names. Ask the questions: all the people with the first given name \u2018James\u2019; all the people with the first given name \u2018William\u2019; All the people with the given name \u2018William\u2019; All the people with the given name \u2018William\u2019 and the family name \u2018Bright\u2019. The name data property hierarchy and the queries using those properties displays what now should be familiar. Sub-properties that imply the super-property. So, when we ask hasFirstGivenName value \"William\" and then the query hasGivenName value value \"William\" we can expect different answers. There are people with \u2018William\u2019 as either first or second given name and asking the question with the super-property for given names will collect both first and second given names. 7.4 Summary \u00b6 We have used data properties that link objects to data such as string, integer, floats and Booleans etc. OWL uses the XML data types. We have seen a simple use of data properties to simulate birth years. The full FHKB also uses them to place names (given and family) on individuals as strings. This means one can ask for the Person with the given name \"James\", of which there are many in the FHKB. Most importantly we have re-visited the open world assumption and its implications for querying an OWL ABox. We have looked at ways in which the ABox can be closed down \u2013 unreliably via the functional characteristic (in this particular case) and more generally via type assertions. All the DL queries used in this chapter can also serve as defined classes in the TBox. It is a useful exercise to progressively add more defined classes to the FHKB TBox. Make more complex queries, make them into defined classes and inspect where they appear in the class hierarchy. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 1891.157 sec (1.00000 % of final), by Pellet 2.2.0 1.134 sec (0.00917 % of final) and by FaCT++ 1.6.4 is approximately 0.201 sec (0.006 % of final). 0 sec indicates failure or timeout. Note that we now cover the whole range of expressivity of OWL 2. HermiT at least is impossibly slow by now. This may be because HermiT does more work than the others. For now, we recommend to use either Pellet or FaCT++. Chapter 8 \u00b6 Cousins in the FHKB \u00b6 In this Chapter you will Revise or get to know about degrees and removes of cousin; Add the properties and sub-property chains for first and second cousins; Add properties and sub-property chains for some removes of cousins; Find out that the siblings debacle haunts us still; Add a defined class that does first cousins properly. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial Be warned; from here on the reasoner can start running slowly! Please see warning at the beginning of the last chapter for more information. 8.1 Introducing Cousins \u00b6 Cousins can be confusing, but here is a brief summary: First cousins share a grandparent, but are not siblings; Second cousins share a great grandparent, but are not first cousins or siblings; Degrees such as first and second cousin give the distance to the nearest common ancestor; Removes give differences in generation. So, my Dad\u2019s first cousins (his generation) are my (Robert David Bright\u2019s) first cousins once removed. Simply, my first cousins are my parent\u2019s sibling\u2019s children. As usual, we can think about the objects and put in place some sub-property chains. 8.2 First Cousins \u00b6 Figure 8.1: Tracing out the sub-property chain for cousins going from a child to a parent, to its sibling, and down to its child, a cousin Figure 8.1 shows the sub-property chain for first cousins. As usual, think at the object level; to get to the first cousins of Robert David Bright, we go to the parents of Robert David Bright, to their siblings and then to their children. We go up, along and down. The OWL for this could be: ObjectProperty: hasFirstCousin SubPropertyOf: hasCousin SubPropertyChain: hasParent o hasSibling o hasChild Characteristics: Symmetric Note that we follow the definitions in Section 8.1 of first cousins sharing a grandparent, but not a parent. The sub-property chain goes up to children of a grandparent (a given person\u2019s parents), along to siblings and down to their children. We do not want this property to be transitive. One\u2019s cousins are not necessarily my cousins. The blood uncles of Robert David Bright have children that are his cousins. These first cousins, however, also have a mother that is not a blood relation of Robert David Bright and the mother\u2019s sibling\u2019s children are not cousins of Robert David Bright. We do, however, want the property to be symmetric. One\u2019s cousins have one\u2019s-self as a cousin. We need to place the cousin properties in the growing object property hierarchy. Cousins are obviously blood relations, but not ancestors, so they go off to one side, underneath hasBloodrelation . We should group the different removes and degree of cousin underneath one hasCousin property and this we will do. Do the following: Task 33: First cousins Add the property of hasCousin to the hierarchy underneath hasBloodrelation ; Add hasFirstCousin underneath this property; Add the sub-property chain as described above; Run the reasoner and look at the first cousins of Robert David Bright. You should see the following people as first cousins of Robert David Bright: Mark Anthony Heath, Nicholas Charles Heath, Mark Bright, Ian Bright, Janet Bright, William Bright, James Bright, Julie Bright, Clare Bright, Richard John Bright and Robert David Bright. The last two, as should be expected, are first cousins of Robert David Bright and this is not correct. As David Bright will be his own brother, his children are his own nieces and nephews and thus the cousins of his own children. Our inability to infer siblings correctly in the FHKB haunts us still and will continue to do so. Although the last query for the cousins of Robert David Bright should return the same results for every reasoner, we have had experiences where the results differ. 8.3 Other Degrees and Removes of Cousin \u00b6 Other degrees of cousins follow the same pattern as for first cousins; we go up, along and down. For second cousins we go up from a given individual to children of a great grandparent, along to their siblings and down to their grandchildren. The following object property declaration is for second cousins (note it uses the isGrandparentOf and its inverse properties, though the parent properties could be used) : ObjectProperty: hasSecondCousin SubPropertyOf: hasCousin SubPropertyChain: hasGrandParent o hasSibling o isGrandParentOf Characteristics: Symmetric \u2018 Removes \u2019 simply add in another \u2018leg\u2019 of either \u2018up\u2019 or \u2018down\u2019 either side of the \u2018along\u2019\u2014that is, think of the actual individuals involved and draw a little picture of blobs and lines\u2014then trace your finger up, along and down to work out the sub-property chain. The following object property declaration does it for first cousins once removed (note that this has been done by putting this extra \u2018leg\u2019 on to the hasFirstCousin property; the symmetry of the property makes it work either way around so that a given person is the first cousin once removed of his/her first cousins once removed): ObjectProperty: hasFirstCousinOnceRemoved SubPropertyOf: hasCousin SubPropertyChain: hasFirstCousin o hasChild Characteristics: Symmetric To exercise the cousin properties do the following: Task 34: Cousin properties Add properties for second degree cousins; Add removes for first and second degree cousins; Run the reasoner and check what we know about Robert David Bright\u2019 other types of cousin. You should see that we see some peculiar inferences about Robert David Bright\u2019 cousins \u2013 not only are his brother and himself his own cousins, but so are his father, mother, uncles and so on. This makes sense if we look at the general sibling problem, but also it helps to just trace the paths around. If we go up from one of Robert David Bright\u2019 true first cousins to a grandparent and down one parent relationship, we follow the first cousin once removed path and get to one of Robert David Bright\u2019 parents or uncles. This is not to be expected and we need a tighter definition that goes beyond sub-property chains so that we can exclude some implications from the FHKB. 8.4 Doing First Cousins Properly \u00b6 As far as inferring first cousin facts for Robert David Bright, we have failed. More precisely, we have recalled all Robert David Bright\u2019s cousins, but the precision is not what we would desire. What we can do is ask for Robert David Bright\u2019 cousins, but then remove the children of Robert David Bright\u2019 parents. The following DL query achieves this: Person that hasFirstCousin valueRobert_David_Bright_1965 and (not (hasFather valueDavid_Bright_1934) or not (hasMother valueMar- garet_Grace_Rever_1934) This works, but only for a named individual. We could make a defined class for this query; we could also make a defined class FirstCousin , but it is not of much utility. We would have to make sure that people whose parents are not known to have siblings with children are excluded. That is, people are not \u2018first cousins\u2019 whose only first cousins are themselves and their siblings. The following class does this: Class: FirstCousin EquivalentTo: Person that hasFirstCousin some Person Task 35: Roberts first cousins Make a defined class FirstCousin as shown above; Make a defined class FirstCousinOfRobert ; Create a DL query that looks at Robert_David_Bright_1965 first cousins and takes away the children of Robert_David_Bright_1965 \u2019 parents as shown above. This gives some practice with negation. One is making a class and then \u2018taking\u2019 some of it away \u2013 \u2018these, but not those\u2019. 8.5 Summary \u00b6 We have now expanded the FHKB to include most blood relationships. We have also found that cousins are hard to capture just using object properties and sub-property chains. Our broken sibling inferences mean that we have too many cousins inferred at the instance level. We can get cousins right at the class level by using our inference based cousins, then excluding some using negation. Perhaps not neat, but it works. We have reinforced that we can just add more and more relationships to individuals by just adding more properties to our FHKB object property hierarchy and adding more sub-property chains that use the object properties we have built up upon parentage and sibling properties; this is as it should be. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet 2.2.0 111.395 sec (0.90085 % of final) and by FaCT++ 1.6.4 is approximately 0.868 sec (0.024 % of final). 0 sec indicates failure or timeout. Chapter 9 \u00b6 Marriage in the FHKB \u00b6 In this chapter you will: Model marriages and relationships; Establish object properties for husbands, wives and various in-laws; Re-visit aunts and uncles to do them properly; Use more than one sub-property chain on a given property. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial Much of what is in this chapter is really revision; it is more of the same - making lots of properties and using lots of sub-property chains. However, it is worth it as it will test your growing skills and it also makes the reasoners and yourself work hard. There are also some good questions to ask of the FHKB as a result of adding marriages. 9.1 Marriage \u00b6 Marriage is a culturally complex situation to model. The FHKB started with a conservative model of a marriage involving only one man and one woman. 10 Later versions are more permissive; a marriage simply has a minimum of two partners. This leaves it open to numbers and sex of the people involved. In fact, \u2018marriage\u2019 is probably not the right name for it. Using BreedingRelationship as a label (the one favoured by the main author\u2019s mother) may be a little too stark and might be a little exclusive.... In any case, some more generic name is probably better and various subclasses of the FHKB\u2019s Marriage class are probably necessary. 10 There being no funny stuff in the Stevens family. To model marriage do the following: Task 36: Marriage Create a class Marriage , subclass of DomainEntity ; Create the properties: hasPartner (domain Marriage and range Person ) and isPartnerIn hasFemalePartner (domain Marriage and range Woman , sub-property of hasPartner ) and its inverse isFemalePartnerIn ; a sub-property of hasPartner has MalePartner (domain Marriage and range Man )and its inverse isMalePartnerIn ; Create the data property hasMarriageYear , making us a sub-property of hasEventYear ,make it functional; Create an individual m001 with the label Marriage of David and Margaret and add the facts: hasMalePartner David_Bright_1934 ; hasFemalePartner Margaret_Grace_Rever_1934 hasMarriageYear 1958 ; Create an individual m002 with the label Marriage of John and Joyce and add the facts: hasMalePartner John_Bright_1930 ; hasFemalePartner Joyce_Gosport (you may have to add Joyce if you did not already did that); hasMarriageYear 1955 ; Create an individual m003 with the label Marriage of Peter and Diana and add the facts: hasMalePartner Peter_William_Bright_1941 ; hasFemalePartner Diana_Pool (you may have to add Diana if you did not already did that); hasMarriageYear 1964 ; We have the basic infrastructure for marriages. We can ask the usual kinds of questions; try the following: Task 37: DL queries Ask the following DL queries: The Women partners in marriages; Marriages that happened before 1960 (see example below); Marriages that happened after 1960; Marriages that involved a man with the family name \u2018Bright\u2019. DL query: Marriage and hasMarriageYear some int[<= 1960] 9.1.1 Spouses \u00b6 This marriage infrastructure can be used to infer some slightly more interesting things for actual people. While we want marriage objects so that we can talk about marriage years and even locations, should we want to, we also want to be able to have the straight-forward spouse relationships one would expect. We can use sub-property chains in the usual manner; do the following: Task 38: Wifes and Husbands Create a property hasSpouse with two sub-properties hasHusband and hasWife . Create the inverses isSpouseOf , isWifeOf and isHusbandOf . To the hasWife property, add the sub-property chain isMalePartnerIn o hasFemalePartner . Follow the same pattern for the hasHusband property. Figure 9.1 shows what is happening with the sub-property chains. Note that the domains and ranges of the spouse properties come from the elements of the sub-property chains. Note also that the hasSpouse relationship will be implied from its sub-property chains. The following questions can now be asked: Is wife of David Bright; Has a husband born before 1940; The wife of an uncle of William Bright 1970. Figure 9.1: The sub-property chain path used to infer the spouse relationships via the marriage partnerships. and many more. This is really a chance to explore your querying abilities and make some complex nested queries that involve going up and down the hierarchy and tracing routes through the graph of relationships between the individuals you\u2019ve inferred. 9.2 In-Laws \u00b6 Now we have spouses, we can also have in-laws. The path is simple: isSpouseOf o hasMother implies hasMotherInLaw . The path involved in mother-in-laws can be seen in Figure 9.2. The following OWL code establishes the sub-property chains for hasMotherInLaw : ObjectProperty: hasMotherInLaw SubPropertyOf: hasParentInLaw SubPropertyChain: isSpouseOf o hasMother Domain: Person Range: Woman InverseOf: isMotherInLawOf Figure 9.2: Tracing out the path between objects to make the sub-property chain for mother-in-laws Do the following to make the parent in-law properties: Task 39: Parents in-law Create hasParentInLaw with two sub-properties of hasMotherInLaw and hasFatherInLaw ; Create the inverses, but remember to let the reasoner infer the hierarchy on that side of the hierarchy; Add the sub-property chains as described in the pattern for hasMotherInLaw above; Run the reasoner and check that the mother-in-law of Margaret Grace Rever is Iris Ellen Archer. 9.3 Brothers and Sisters In-Law \u00b6 Brothers and sisters in law have the interesting addition of having more than one path between objects to establish a sister or brother in law relationship. The OWL code below establishes the relationships for \u2018is sister in law of\u2019: ObjectProperty: hasSisterInLaw SubPropertyOf: hasSiblingInLaw SubPropertyChain: hasSpouse o hasSister SubPropertyChain: hasSibling o isWifeOf A wife\u2019s husband\u2019s sister is a sister in law of the wife. Figure 9.3 shows the two routes to being a sister-in-law. In addition, the wife is a sister in law of the husband\u2019s siblings. One can add as many sub-property chains to a property as one needs. You should add the properties for hasSiblingInLawOf and its obvious sub-properties following the inverse of the pattern above. Task 40: Siblings in-law Create the relationships for siblings-in-law as indicated in the owl code above. By now, chances are high that the realisation takes a long time. We recommend to remove the very computationally expensive restriction `hasParent` exactly 2 Person on the `Person` class, if you have not done it so far. Figure 9.3: The two routes to being a sister-in-law. 9.4 Aunts and Uncles in-Law \u00b6 The uncle of Robert David Bright has a wife, but she is not the aunt of Robert David Bright, she is the aunt-in-law. This is another kith relationship, not a kin relationship. The pattern has a familiar feel: ObjectProperty: isAuntInLawOf SubPropertyOf: isInLawOf SubPropertyChain: isWifeOf o isBrotherOf o isParentOf Task 41: Uncles and aunts in-law Create hasAuntInLaw and hasUncleInLaw in the usual way; Test in the usual way; Tidy up the top of the property hierarchy so that it looks like Figure 9.4. We have a top property of hasRelation and two sub-properties of isBloodRelationOf and isInLawOf to establish the kith and kin relationships respectively; All the properties created in this chapter (except for spouses) should be underneath isInLawOf . Figure 9.4: The object property hierarchy after adding the various in-law properties. 9.5 Summary \u00b6 This has really been a revision chapter; nothing new has really been introduced. We have added a lot of new object properties and one new data property. The latest object property hierarchy with the \u2018in-law\u2019 branch can be seen in Figure 9.4. Highlights have been: Having an explicit marriage object so that we can say things about the marriage itself, not just the people in the marriage; We have seen that more than one property chain can be added to a property; We have added a lot of kith relationships to join the kin or blood relationships; As usual, the reasoner can establish the hierarchy for the inverses and put a lot of the domain and ranges in for free. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet 2.2.0 123.655 sec (1.00000 % of final) and by FaCT++ 1.6.4 is approximately 1.618 sec (0.046 % of final). 0 sec indicates failure or timeout. Chapter 10 \u00b6 Extending the TBox \u00b6 In this chapter you will: Just add lots of defined classes for all the aspects we have covered in this FHKB tutorial; You will learn that the properties used in these defined classes must be chosen with care. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial 10.1 Adding Defined Classes \u00b6 Add the following defined classes: Task 42: Adding defined classes Relation and blood relation; Forefather and Foremother; Grandparent, Grandfather and Grandmother; GreatGrandparent, GreatGrandfather and GreatGrandmother; GreatGrandparentOfRobert, GreatGrandfatherOfRobert and GreatGrandMotherOfRobert Daughter, Son, Brother, Sister, Child; Aunt, Uncle, AuntInLaw, UncleInLaw, GreatAunt and GreatUncle; FirstCousin and SecondCousin; First cousin once removed; InLaw, MotherInLaw, FatherInLaw, ParentInLaw, SiblingInLaw, SisterInLaw, BrotherInLaw; Any defined class for any property in the hierarchy and any nominal variant of these classes. The three classes of Child , Son and Daughter are of note. They are coded in the following way: Class: Child EquivalentTo: Person that hasParent Some Person Class: Son EquivalentTo: Man that hasParent Some Person Class: Daughter EquivalentTo: Woman that hasParent Some Person After running the reasoner, you will find that Person is found to be equivalent to Child ; Daughter is equivalent to Woman and that Son is equivalent to Man . This does, of course, make sense \u2013 each and every person is someone\u2019s child, each and every woman is someone\u2019s daughter. We will forget evolutionary time-scales where this might be thought to break down at some point \u2013 all Person individuals are also Descendant individuals, but do we expect some molecule in some prebiotic soup to be a member of this class? Nevertheless, within the scope of the FHKB, such inferred equivalences are not unreasonable. They are also instructive; it is possible to have different intentional descriptions of a class and for them to have the same logical extents. You can see another example of this happening in the amino acids ontology, but for different reasons. Taking Grandparent as an example class, there are two ways of writing the defined class: Class: Grandparent EquivalentTo: Person and isGrandparentOf some Person Class: Grandparent EquivalentTo: Person and (isParentOf some (Person and (is- ParentOf some Person)) Each comes out at a different place in the class hierarchy. They both capture the right individuals as members (that is, those individuals in the ABox that are holding a isGrandparentOf property), but the class hierarchy is not correct. By definition, all grandparents are also parents, but the way the object property hierarchy works means that the first way of writing the defined class (with the isGrandparentOf property) is not subsumed by the class Parent . We want this to happen in any sensible class hierarchy, so we have to use the second pattern for all the classes, spelling out the sub-property path that implies the property such as isGrandparentOf within the equivalence axiom. The reason for this need for the \u2018long-form\u2019 is that the isGrandparentOf does not imply the isParentOf property. As described in Chapter 3 if this implication were the case, being a grandparent of Robert David Bright, for instance, would also imply that the same Person were a parent of Robert David Bright; an implication we do not want. As these two properties ( isParentOf and isGrandparentOf ) do not subsume each other means that the defined classes written according to pattern one above will not subsume each other in the class hierarchy. Thus we use the second pattern. If we look at the class for grandparents of Robert: Class: GrandparentOfRobert EquivalentTo: Person that isParentOf some (Person that isParentOf value Robert David Bright) If we make the equivalent class for Richard John Bright, apply the reasoner and look at the hierarchy, we see that the two classes are not logically equivalent, even though they have the same extents of William George Bright, Iris Ellen Archer, Charles Herbert Rever and Violet Sylvia Steward. We looked at this example in Section 6.2, where there is an explanation and solutions. 10.2 Summary \u00b6 We can add defined classes based on each property we have put into the object property hierarchy. We see the expected hierarchy; as can be seen from Figure 10.1 it has an obvious symmetry based on sex. We also see a lot of equivalences inferred \u2013 all women are daughters, as well as women descendants. Perhaps not the greatest insight ever gained, but it at least makes sense; all women must be daughters. It is instructive to use the explanation feature in Prot\u00e9g\u00e9 to look at why the reasoner has made these inferences. For example, take a look at the class hasGrandmother some Woman \u2013 it is instructive to see how many there are. Like the Chapter on marriage and in-law (Chapter 9), this chapter has largely been revision. One thing of note is, however, that we must not use the object properties that are inferred through sub-property chains as definitions in the TBox; we must spell out the sub-property chain in the definition, otherwise the implications do not work properly. One thing is almost certain; the resulting TBox is rather complex and would be almost impossible to maintain by hand. Figure 10.1: The full TBox hierarchy of the FHKB The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). ``` The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet 2.2.0 0.000 sec (0.00000 % of final) and by FaCT++ 1.6.4 is approximately 35.438 sec (1.000 % of final). 0 sec indicates failure or timeout. ```` Chapter 11 \u00b6 Final remarks \u00b6 If you have done all the tasks within this tutorial, then you will have touched most parts of OWL 2. Unusually for most uses of OWL we have concentrated on individuals, rather than just on the TBox. One note of warning \u2013 the full FHKB has some 450 members of the Bright family and takes a reasonably long time to classify, even on a sensible machine. The FHKB is not scalable in its current form. One reason for this is that we have deliberately maximised inference. We have attempted not to explicitly type the individuals, but drive that through domain and range constraints. We are making the property hierarchy do lots of work. For the individual Robert David Bright, we only have a couple of assertions, but we infer some 1 500 facts between Robert David Bright and other named individuals in the FHKB\u2013displaying this in Prot\u00e9g\u00e9 causes problems. We have various complex classes in the TBox and so on. We probably do not wish to drive a genealogical application using an FHKB in this form. Its purpose is educational. It touches most of OWL 2 and shows a lot of what it can do, but also a considerable amount of what it cannot do. As inference is maximised, the FHKB breaks most of the OWL 2 reasoners at the time of writing.However, it serves its role to teach about OWL 2. OWL 2 on its own and using it in this style, really does not work for family history. We have seen that siblings and cousins cause problems. rules in various forms can do this kind of thing easily\u2014it is one of the primary examples for learning about Prolog. Nevertheless, the FHKB does show how much inference between named individuals can be driven from a few fact assertions and a property hierarchy. Assuming a powerful enough reasoner and the ability to deal with many individuals, it would be possible to make a family history application using the FHKB; as long as one hid the long and sometimes complex queries and manipulations that would be necessary to \u2018prune\u2019 some of the \u2018extra\u2019 facts found about individuals. However, the FHKB does usefully show the power of OWL 2, touch a great deal of the language and demonstrate some of its limitations. Appendix A \u00b6 FHKB Family Data \u00b6 Table A.1: The list of individuals in the FHKB Person First given name Second given name Family name Birth year Mother Father Alec John Archer 1927 Alec John Archer 1927 Violet Heath 1887 James Alexander Archer 1882 Charles Herbert Rever 1895 Charles Herbert Rever 1895 Elizabeth Frances Jessop 1869 William Rever 1870 Charlotte Caroline Jane Bright 1894 Charlotte Caroline Jane Bright 1894 Charlotte Hewett 1863 Henry Edmund Bright 1862 Charlotte Hewett 1863 Charlotte none Hewett 1863 not specified not specified Clare Bright 1966 Clare none Bright 1966 Diana Pool Peter William Bright 1941 Diana Pool Diana none Pool none not specified not specified David Bright 1934 David none Bright 1934 Iris Ellen Archer 1906 William George Bright 1901 Dereck Heath Dereck none Heath 1927 not specified not specified Eileen Mary Rever 1929 Eileen Mary Rever 1929 Violet Sylvia Steward 1894 Charles Herbert Rever 1895 Elizabeth Frances Jessop 1869 Elizabeth Frances Jessop 1869 not specified not specified Ethel Archer 1912 Ethel none Archer 1912 Violet Heath 1887 James Alexander Archer 1882 Frederick Herbert Bright 1889 Frederick Herbert Bright 1889 Charlotte Hewett 1863 Henry Edmund Bright 1862 Henry Edmund Bright 1862 Henry Edmund Bright 1862 not specified not specified Henry Edmund Bright 1887 Henry Edmund Bright 1887 Charlotte Hewett 1863 Henry Edmund Bright 1862 Ian Bright 1959 Ian none Bright 1959 Joyce Gosport John Bright 1930 Iris Ellen Archer 1906 Iris Ellen Archer 1906 Violet Heath 1887 James Alexander Archer 1882 James Alexander Archer 1882 James Alexander Archer 1882 not specified not specified James Bright 1964 James none Bright 1964 Diana Pool Peter William Bright 1941 James Frank Hayden Bright 1891 James Frank Bright 1891 Charlotte Hewett 1863 Henry Edmund Bright 1862 Janet Bright 1964 Janet none Bright 1964 Joyce Gosport John Bright 1930 John Bright 1930 John none Bright 1930 Iris Ellen Archer 1906 William George Bright 1901 John Tacey Steward 1873 John Tacey Steward 1873 not specified not specified Joyce Archer 1921 Joyce none Archer 1921 Violet Heath 1887 James Alexander Archer 1882 Joyce Gosport Joyce none Gosport not specified not specified not specified Julie Bright 1966 Julie none Bright 1966 Diana Pool Peter William Bright 1941 Kathleen Minnie Bright 1904 Kathleen Minnie Bright 1904 Charlotte Hewett 1863 Henry Edmund Bright 1862 Leonard John Bright 1890 Leonard John Bright 1890 Charlotte Hewett 1863 Henry Edmund Bright 1862 Lois Green 1871 Lois none Green 1871 not specified not specified Margaret Grace Rever 1934 Margaret Grace Rever 1934 Violet Sylvia Steward 1894 Charles Herbert Rever 1895 Mark Anthony Heath 1960 Mark Anthony Heath 1960 Eileen Mary Rever 1929 Dereck Heath Mark Bright 1956 Mark none Bright 1956 Joyce Gosport John Bright 1930 Nicholas Charles Heath 1964 Nicholas Charles Heath 1964 Eileen Mary Rever 1929 Dereck Heath Nora Ada Bright 1899 Nora Ada Bright 1899 Charlotte Hewett 1863 Henry Edmund Bright 1862 Norman James Archer 1909 Norman James Archer 1909 Violet Heath 1887 James Alexander Archer 1882 Peter William Bright 1941 Peter William Bright 1941 Iris Ellen Archer 1906 William George Bright 1901 Richard John Bright 1962 Richard John Bright 1962 Margaret Grace Rever 1934 David Bright 1934 Robert David Bright 1965 Robert David Bright 1965 Margaret Grace Rever 1934 David Bright 1934 Violet Heath 1887 Violet none Heath 1887 not specified not specified Violet Sylvia Steward 1894 Violet Sylvia Steward 1894 Lois Green 1871 John Tacey Steward 1873 William Bright 1970 William none Bright 1970 Joyce Gosport John Bright 1930 William George Bright 1901 William George Bright 1901 Charlotte Hewett 1863 Henry Edmund Bright 1862 William Rever 1870 William none Rever 1870 not specified not specified Bibliography \u00b6 [1] M. Horridge and S. Bechhofer. The owl api: a java api for working with owl 2 ontologies. Proc. of OWL Experiences and Directions , 2009, 2009. [2] Luigi Iannone, Alan Rector, and Robert Stevens. Embedding knowledge patterns into owl. In European Semantic Web Conference (ESWC09) , pages 218\u2013232, 2009. [3] Dmitry Tsarkov, Uli Sattler, Margaret Stevens, and Robert Stevens. A Solution for the Man-Man Problem in the Family History Knowledge Base. In Sixth International Workshop on OWL: Experiences and Directions 2009 , 2009.","title":"Family History Knowledge Base (FHKB) tutorial"},{"location":"tutorial/fhkb/#manchester-family-history-advanced-owl","text":"This is a fork of the infamous Manchester Family History Advanced OWL Tutorial version 1.1, located at http://owl.cs.manchester.ac.uk/publications/talks-and-tutorials/fhkbtutorial/ The translation to markdown is not without issue, but we are making a start to making the tutorial a bit more accessible. This reproduction is done with kind permission by Robert Stevens.","title":"Manchester Family History Advanced OWL"},{"location":"tutorial/fhkb/#original-credits-version-11-see-pdf","text":"Authors: Robert Stevens Margaret Stevens Nicolas Matentzoglu Simon Jupp Bio-Health Informatics Group School of Computer Science University of Manchester Oxford Road Manchester United Kingdom M13 9PL robert.stevens@manchester.ac.uk","title":"Original credits (Version 1.1, see pdf):"},{"location":"tutorial/fhkb/#contributors","text":"v 1.0 Robert Stevens, Margaret Stevens, Nicolas Matentzoglu and Simon Jupp v 1.1 Robert Stevens, Nicolas Matentzoglu v 2.0 (Web version) Robert Stevens, Nicolas Matentzoglu, Shawn Tan The University of Manchester Copyright\u00a9 The University of Manchester November 25, 2015","title":"Contributors"},{"location":"tutorial/fhkb/#acknowledgements","text":"This tutorial was realised as part of the Semantic Web Authoring Tool (SWAT) project (see http://www.swatproject.org), which is supported by the UK Engineering and Physical Sciences Research Council (EPSRC) grant EP/G032459/1, to the University of Manchester, the University of Sussex and the Open University.","title":"Acknowledgements"},{"location":"tutorial/fhkb/#dedication","text":"The Stevens family\u2014all my ancestors were necessary for this to happen. Also, for my Mum who gathered all the information.","title":"Dedication"},{"location":"tutorial/fhkb/#contents","text":"Preamble 0.1 Licencing 0.2 Reporting Errors 0.3 Acknowledgements 1. Introduction 1.1 Learning Outcomes 1.2 Why Family History? 1.3 How to use this Tutorial 1.4 FHKB Resources 1.5 Conventions used in this Tutorial 2. Adding some Individuals to the FHKB 2.1 A World of Objects 2.2 Asserting Parentage Facts 2.3 Summary 3. Ancestors and Descendants 3.1 Ancestors and Descendants 3.2 Grandparents and Great Grandparents 3.3 Summary 4. Modelling the Person Class 4.1 The Class of Person 4.2 Describing Sex in the FHKB 4.3 Defining Man and Woman 4.4 Describing Parentage in the FHKB 4.5 Who has a father? 4.6 Filling in Domains and Ranges for the FHKB Properties 4.7 Inconsistencies 4.8 Adding Some Defined Classes for Ancestors and so on 4.9 Summary 5. Siblings in the FHKB 5.1 Blood relations 5.2 Siblings: Option One 5.2.1 Brothers and Sisters 5.3 Siblings: Option two 5.3.1 Which Modelling Option to Choose for Siblings? 5.4 Half-Siblings 5.5 Aunts and Uncles 5.6 Summary 6. Individuals in Class Expressions 6.1 Richard and Robert\u2019s Parents and Ancestors 6.2 Closing Down What we Know About Parents and Siblings 6.3 Summary 7. Data Properties in the FHKB 7.1 Adding Some Data Properties for Event Years 7.1.1 Counting Numbers of Children 7.2 The Open World Assumption 7.3 Adding Given and Family Names 7.4 Summary 8. Cousins in the FHKB 8.1 Introducing Cousins 8.2 First Cousins 8.3 Other Degrees and Removes of Cousin 8.4 Doing First Cousins Properly 8.5 Summary 9. Marriage in the FHKB 9.1 Marriage 9.1.1 Spouses 9.2 In-Laws 9.3 Brothers and Sisters In-Law 9.4 Aunts and Uncles in-Law 9.5 Summary 10. Extending the TBox 10.1 Adding Defined Classes 10.2 Summary 11. Final remarks A FHKB Family Data","title":"Contents"},{"location":"tutorial/fhkb/#preamble","text":"","title":"Preamble"},{"location":"tutorial/fhkb/#01-licencing","text":"The \u2018Manchester Family History Advanced OWL Tutorial\u2019 by Robert Stevens, Margaret Stevens, Nicolas Matentzoglu, Simon Jupp is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.","title":"0.1 Licencing"},{"location":"tutorial/fhkb/#02-reporting-errors","text":"This manual will almost certainly contain errors, defects and infelicities. Do report them to robert.stevens@manchester.ac.uk supplying chapter, section and some actual context in the form of words will help in fixing any of these issues.","title":"0.2 Reporting Errors"},{"location":"tutorial/fhkb/#03-acknowledgements","text":"As well as the author list, many people have contributed to this work. Any contribution, such as reporting bugs etc., is rewarded by an acknowledgement of contribution (in alphabetical order) when the authors get around to adding them: Graham Goff; Matthew Horridge; Jared Leo; Fennie Liang; Phil Lord; Fiona McNeill; Eleni Mikroyannidi; George Moulton; Bijan Parsia; Alan Rector; Uli Sattler; Dmitry Tsarkov; Danielle Welter.","title":"0.3 Acknowledgements"},{"location":"tutorial/fhkb/#chapter-1","text":"","title":"Chapter 1"},{"location":"tutorial/fhkb/#introduction","text":"This tutorial introduces the tutee to many of the more advanced features of the Web Ontology Language (OWL). The topic of family history is used to take the tutee through various modelling issues and, in doing so, using many features of OWL 2 to build a Family History Knowledge Base (FHKB). The exercises are designed to maximise inference about family history through the use of an automated reasoner on an OWL knowledge base (KB) containing many members of the Stevens family. The aim, therefore, is to enable people to learn advanced features of OWL 2 in a setting that involves both classes and individuals, while attempting to maximise the use of inference within the FHKB.","title":"Introduction"},{"location":"tutorial/fhkb/#11-learning-outcomes","text":"By doing this tutorial, a tutee should be able to: Know about the separation of entities into TBox and ABox; Use classes and individuals in modelling; Write fancy class expressions; Assert facts about individuals; Use the effects of property hierarchies, property characteristics, domain/range constraints to drive inference; Use constraints and role chains on inferences about individuals; Understand and manage the consequences of the open world assumption in the TBox and ABox; Use nominals in class expressions; Appreciate some limits of OWL 2.","title":"1.1 Learning Outcomes"},{"location":"tutorial/fhkb/#12-why-family-history","text":"Building an FHKB enables us to meet our learning outcomes through a topic that is accessible to virtually everyone. Family history or genealogy is a good topic for a general tutorial on OWL 2 as it enables us to touch many features of the language and, importantly, it is a field that everyone knows. All people have a family and therefore a family history \u2013 even if they do not know their particular family history. A small caveat was put on the topic being accessible to everyone as some cultures differ, for instance, in the description of cousins and labels given to different siblings. Nevertheless, family history remains a topic that everyone can talk about. Family history is a good topic for an OWL ontology as it obviously involves both individuals \u2013 the people involved \u2013 and classes of individuals \u2013 people, men and women, cousins, etc. Also, it is an area rich in inference; from only knowing parentage and sex of an individual, it is possible to work out all family relationships \u2013 for example, sharing parents implies a sibling relationship; one\u2019s parent\u2019s brothers are one\u2019s uncles; one\u2019s parent\u2019s parents are one\u2019s grandparents. So, we should be able to construct an ontology that allows us to both express family history, but also to infer family relationships between people from knowing relatively little about them. As we will learn through the tutorial, OWL 2 cannot actually do all that is needed to create a FHKB. This is unfortunate, but we use it to our advantage to illustrate some of the limitations of OWL 2. We know that rule based systems can do family history with ease, but that is not the point here; we are not advocating OWL DL as an appropriate mechanism for doing family history, but we do use it as a good educational example. We make the following assumptions about what people know: We assume that people know OWL to the level that is known at the end of the Pizza tutorial . Some ground will be covered again, but a lot of basic OWL is assumed. We assume people know how to use Prot\u00e9g\u00e9 or their OWL environment of choice. We do not give \u2018click by click\u2019 instructions. At some places, some guidance is given, but this is not to be relied upon as Prot\u00e9g\u00e9 changes and we will not keep up to date. We make some simplifying assumptions in this tutorial: We take a conventional western view of family history. This appears to have most effects on naming of sibling and cousin relationships. We take a straight-forward view on the sex of people; this is explored further in Chapter 4; A \u2018conventional\u2019 view of marriage is taken; this is explored further in Chapter 9. We make no special treatment of time or dates; we are only interested in years and we do not do anything fancy; this is explored more in Chapter 7. We assume the ancestors of people go back for ever; obviously this is not true, eventually one would get back to a primordial soup and one\u2019s ancestors are not humans (members of the classPerson), but we don\u2019t bother with such niceties. At the end of the tutorial, you should be able to produce a property hierarchy and a TBox or class hierarchy such as shown in Figure 1.1; all supported by use of the automated reasoner and a lot of OWL 2\u2019s features. Figure 1.1: A part of the class and property hierarchy of the final FHKB.","title":"1.2 Why Family History?"},{"location":"tutorial/fhkb/#13-how-to-use-this-tutorial","text":"Here are some tips on using this manual to the best advantage: Start at the beginning and work towards the end. You can just read the tutorial, but building the FHKB will help you learn much more and much more easily Use the reasoner in each task; a lot of the FHKB tutorial is about using the reasoner and not doing so will detract from the learning outcomes.","title":"1.3 How to use this Tutorial"},{"location":"tutorial/fhkb/#14-fhkb-resources","text":"The following resources are available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial: A full version of the Stevens FHKB. Some links to papers about the FHKB. Some slides about the FHKB tutorial. A set of OWL resources for each stage of the FHKB. Some blogs about the FHKB are at http://robertdavidstevens.wordpress.com.","title":"1.4 FHKB Resources"},{"location":"tutorial/fhkb/#15-conventions-used-in-this-tutorial","text":"All OWL is written in Manchester Syntax. When we use FHKB entities within text, we use a sans serif typeface. We use CamelCase for classes and property names. Class names start with upper case. Individual names start with a lower case letter and internal underscores to break words. Property names usually start with \u2018is\u2019 or \u2018has\u2019 and are CamelCase with a lower case initial letter. Many classes and individuals in the FHKB have annotation properties, usually human readable labels. They show up in some of the examples in Manchester syntax, but are not made explicit as part of the tasks in this tutorial. Every object property is necessarily a sub-property of topObjectProperty. It does not have to be asserted as such. Nevertheless, there might be situations where this relationship is made explicit in this tutorial for illustrative reasons. The individuals we are dealing with represent distinct persons. Throughout the tutorial, once the respective axiom is introduced (chapter 7.1.1), the reader should make sure that all his or her individuals are always made distinct, especially when he or she adds a new one. At the end of each chapter, we note the Description Logic Language (expressivity) needed to represent the ontology and the reasoning times for a number of state of the art reasoning systems. This should get the reader a sense how difficult the FHKB becomes for reasoners to deal with over time. When there is some scary OWL or the reasoner may find the FHKB hard work, you will see a \u2018here be dragons\u2019 image. 1 1 The image comes fromhttp://ancienthomeofdragon.homestead.com/May 2012.","title":"1.5 Conventions used in this Tutorial"},{"location":"tutorial/fhkb/#chapter-2","text":"","title":"Chapter 2"},{"location":"tutorial/fhkb/#adding-some-individuals-to-the-fhkb","text":"In this chapter we will start by creating a fresh OWL ontology and adding some individuals that will be surrogates for people in the FHKB. In particular you will: Create a new OWL ontology for the FHKB; Add some individuals that will stand for members of the Stevens family. Describe parentage of people. Add some facts to specific individuals as to their parentage; See the reasoner doing some work. At the moment we will ignore sex; sex will not happen until Chapter 4.","title":"Adding some Individuals to the FHKB"},{"location":"tutorial/fhkb/#21-a-world-of-objects","text":"The \u2018world\u2019 2 or field of interest we model in an ontology is made up of objects or individuals. Such objects include, but are not limited to: People, their pets, the pizzas they eat; The processes of cooking pizzas, living, running, jumping, undertaking a journey; The spaces within a room, a bowl, an artery; The attributes of things such as colour, dimensions, speed, shape of various objects; Boundaries, love, ideas, plans, hypotheses. 2 we use \u2018world\u2019 as a synonym of \u2018field of interest\u2019 or \u2018domain\u2019. \u2018World\u2019 does not restrict us to modelling the physical world outside our consciousness. We observe these objects, either outside lying around in the world or in our heads. OWL is all about modelling such individuals. Whenever we make a statement in OWL, when we write down an axiom, we are making statements about individuals. When thinking about the axioms in an ontology it is best to think about the individuals involved, even if OWL individuals do not actually appear in the ontology. All through this tutorial we will always be returning to the individuals being described in order to help us understand what we are doing and to help us make decisions about how to do it.","title":"2.1 A World of Objects"},{"location":"tutorial/fhkb/#22-asserting-parentage-facts","text":"Biologically, everyone has parents; a mother and a father 3 . The starting point for family history is parentage; we need to relate the family member objects by object properties. An object property relates two objects, in this case a child object with his or her mother or father object. To do this we need to create three object properties: Task 1: Creating object properties for parentage Create a new ontology; Create an object property hasMother ; Create a property isMotherOf and give hasMother the InverseOf: isMotherOf ; Do the same for the property hasFather ; Create a property hasParent ; give it the obvious inverse; Make hasMother and hasFather sub-properties of hasParent . Run the reasoner and look at the property hierarchy. Note how the reasoner has automatically completed the sub-hierarchy for isParentOf: isMotherOf and isFatherOf are inferred to be sub-properties of isParentOf . The OWL snippet below shows some parentage fact assertions on an individual. Note that rather than being assertions to an anonymous individual via some class, we are giving an assertion to a named individual. Individual: grant_plinth Facts: hasFather mr_plinth, hasMother mrs_plinth 3 Don\u2019t quibble; it\u2019s true enough here. Task 2: Create the ABox Using the information in Table A.1 (see appendix) about parentage (so the columns about fathers and mothers), enter the fact assertions for the people which appear in rows shaded in grey. We will only use the hasMother and hasFather properties in our fact assertions. You do not need to assert names and birth years yet. This exercise will require you to create an individual for every person we want to talk about, using the Firstname_Secondname_Familyname_Birthyear pattern, as for example in Robert_David_Bright_1965 . While asserting facts about all individuals in the FHKB will be a bit tedious at times, it might be useful to at least do the task for a subset of the family members. For the impatient reader, there is a convenience snapshot of the ontology including the raw individuals available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial If you are working with Prot\u00e9g\u00e9, you may want to look at the Matrix plugin for Prot\u00e9g\u00e9 at this point. The plugin allows you to add individuals quickly in the form of a regular table, and can significantly reduce the effort of adding any type of entity to the ontology. In order to install the matrix plugin, open Prot\u00e9g\u00e9 and go to File \u00bb Check for plugins. Select the \u2018Matrix Views\u2019 plugin. Click install, wait until the the installation is confirmed, close and re-open Prot\u00e9g\u00e9; go to the \u2018Window\u2019 menu item, select \u2018Tabs\u2019 and add the \u2018Individuals matrix\u2019. Now do the following: Task 3: DL queries Classify the FHKB. Issue the DL query hasFather value David_Bright_1934 and look at the answers (remember to check the respective checkbox in Prot\u00e9g\u00e9 to include individuals in your query results). Issue the DL query isFatherOf value Robert_David_Bright_1965 . Look at the answers. 4. Look at the entailed facts on Robert_David_Bright_1965 . You should find the following: David Bright (1934) is the father of Robert David Bright (1965) and Richard John Bright (1962). Robert David Bright (1965) has David Bright 1934 as a parent. Since we have said that isFatherOf has an inverse of hasFather , and we have asserted that Robert_David_Bright_1965 hasFather David_Bright_1934 , we have a simple entailment that David_Bright_1934 isFatherOf Robert_David_Bright_1965 . So, without asserting the isFatherOf facts, we have been able to ask and get answers for that DL query. As we asserted that Robert_David_Bright_1965 hasFather David_Bright_1934 , we also infer that he hasParent David_Bright_1934 ; this is because hasParent is the super-property of hasFather and the sub-property implies the super-property. This works all the way up the property tree until topObjectProperty , so all individuals are related by topObjectProperty \u2014this is always true. This implication \u2018upwards\u2019 is the way to interpret how the property hierarchies work.","title":"2.2 Asserting Parentage Facts"},{"location":"tutorial/fhkb/#23-summary","text":"We have now covered the basics of dealing with individuals in OWL ontologies. We have set up some properties, but without domains, ranges, appropriate characteristics and then arranged them in a hierarchy. From only a few assertions in our FHKB, we can already infer many facts about an individual: Simple exploitation of inverses of properties and super-properties of the asserted properties. We have also encountered some important principles: We get inverses for free. The sub-property implies the super-property. So, hasFather implies the hasParent fact between individuals. This entailment of the super-property is very important and will drive much of the inference we do with the FHKB. Upon reasoning we get the inverses of properties between named individuals for free. Lots is still open. For example, we do not know the sex of individuals and what other children, other than those described, people in the FHKB may have. The FHKB ontology at this stage of the tutorial has an expressivity of ALHI. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.026 sec (0.00001 % of final), by Pellet 2.2.0 0.144 sec (0.00116 % of final) and by FaCT++ 1.6.4 is approximately 0. sec (0.000 % of final). 0 sec indicates failure or timeout.","title":"2.3 Summary"},{"location":"tutorial/fhkb/#chapter-3","text":"","title":"Chapter 3"},{"location":"tutorial/fhkb/#ancestors-and-descendants","text":"In this Chapter you will: Use sub-properties and the transitive property characteristic to infer ancestors of people; Add properties to the FHKB property hierarchy that will infer ancestors and descendants of a person without adding any more facts to the FHKB; Explore the use of sub-property chains for grandparents, great grandparents and so on; Place all of these new object properties in the property hierarchy and in that way learn more about the implications of the property hierarchy. Find a snapshot of the ontology at this stage at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial.","title":"Ancestors and Descendants"},{"location":"tutorial/fhkb/#31-ancestors-and-descendants","text":"The FHKB has parents established between individuals and we know that all people have two parents. A parent is an ancestor of its children; a person\u2019s parent\u2019s parents are its ancestors; and so on. So, in our FHKB, Robert\u2019s ancestors are David, Margaret, William, Iris, Charles, Violet, James, another Violet, another William, Sarah and so on. If my parent\u2019s parents are my ancestors, then what we need is a transitive version of the hasParent property. Obviously we do not want hasParent to be transitive, as Robert\u2019s grandparents (and so on) would become his parents (and that would be wrong). We can easily achieve what is necessary. We need a hasAncestor property that has a transitive characteristic. The trick is to make this a super-property of the hasParent property. As explained before, a sub-property implies its super-property. So, if individual x holds a hasParent property with an individual y , then it also holds an instance of its super-property hasAncestor with the individual y . If individual y then holds a hasParent property with another individual z , then there is also, by implication, a hasAncestor property between y and z . As hasAncestor is transitive, x and z also hold a hasAncestor relationship between them. The inverse of hasAncestor can either be isAncestorOf or hasDescendant . We choose the isAncestorOf option. Task 4: Object properties: exploiting the semantics Make a new object property hasRelation , make it symmetric. Make a new object property hasAncestor . Make it a sub-property of hasRelation and a super-property of hasParent . Make hasAncestor transitive. Create the inverse isAncestorOf . Do not \u2018stitch\u2019 it into the property hierarchy; the reasoner will sort it all out for you. Run the reasoner and issue the DL query hasAncestor value William_George_Bright_1901 . Issue the query isAncestorOf value Robert_David_Bright_1965 . The hasAncestor object property will look like this: ObjectProperty: hasAncestor SubPropertyOf: hasRelation SuperPropertyOf: hasParent, Characteristics: Transitive InverseOf: isAncestorOf As usual, it is best to think of the objects or individuals involved in the relationships. Consider the three individuals \u2013 Robert, David and William. Each has a hasFather property, linking Robert to David and then David to William. As hasFather implies its super-property hasParent , Robert also has a hasParent property with David, and David has a hasParent relation to William. Similarly, as hasParent implies hasAncestor , the Robert object has a hasAncestor relation to the David object and the David object has one to the William object. As hasAncestor is transitive, Robert not only holds this property to the David object, but also to the William object (and so on back through Robert\u2019s ancestors).","title":"3.1 Ancestors and Descendants"},{"location":"tutorial/fhkb/#32-grandparents-and-great-grandparents","text":"We also want to use a sort of restricted transitivity in order to infer grandparents, great grandparents and so on. My grandparents are my parent\u2019s parents; my grandfathers are my parent\u2019s fathers. My great grandparents are my parent\u2019s parent\u2019s parents. My great grandmothers are my parent\u2019s parent\u2019s mothers. This is sort of like transitivity, but we want to make the paths only a certain length and, in the case of grandfathers, we want to move along two relationships \u2013 hasParent and then hasFather . We can do this with OWL 2\u2019s sub-property chains. The way to think about sub-property chains is: If we see property x followed by property y linking three objects, then it implies that property z is held between Figure 3.1: Three blobs representing objects of the classPerson. The three objects are linked by a hasParent property and this implies a hasGrandparent property. the first and third objects. Figure 3.1 shows this diagrammatically for the hasGrandfather property. For various grandparent object properties we need the following sets of implications: My parent\u2019s parents are my grandparents; My parent\u2019s fathers are my grandfathers; My parent\u2019s mothers are my grandmothers; My parent\u2019s parent\u2019s parents are my great grandparents or my grandparent\u2019s parents are my great grandparents. My parent\u2019s parent\u2019s fathers are my great grandfathers or my parent\u2019s grandfathers are my great grandfathers; My parent\u2019s parent\u2019s mothers are my great grandmothers (and so on). Notice that we can trace the paths in several ways, some have more steps than others, though the shorter paths themselves employ paths. Tracing these paths is what OWL 2\u2019s sub-property chains achieve. For the new object property hasGrandparent we write: ObjectProperty: hasGrandparent SubPropertyChain: hasParent o hasParent We read this as \u2018 hasParent followed by hasParent implies hasGrandparent \u2019. We also need to think where the hasGrandparent property fits in our growing hierarchy of object properties. Think about the implications: Does holding a hasParent property between two objects imply that they also hold a hasGrandparent property? Of course the answer is \u2018no\u2019. So, this new property is not a super-property of hasParent . Does the holding of a hasGrandparent property between two objects imply that they also hold an hasAncestor property? The answer is \u2018yes\u2019; so that should be a super-property of hasGrandparent . We need to ask such questions of our existing properties to work out where we put it in the object property hierarchy. At the moment, our hasGrandparent property will look like this: ObjectProperty: hasGrandParent SubPropertyOf: hasAncestor SubPropertyChain: hasParent o hasParent SuperPropertyOf: hasGrandmother, hasGrandfather InverseOf: isGrandParentOf Do the following task: Task 5: Grandparents object properties Make the hasGrandparent , hasGrandmother and hasGrandfather object properties and the obvious inverses (see OWL code above); Go to the individuals tabs and inspects the inferred object property assertions for Robert_David_Bright_1965 and his parents. Again, think of the objects involved. We can take the same three objects as before: Robert, David and William. Think about the properties that exist, both by assertion and implication, between these objects. We have asserted only hasFather between these objects. The inverse can be inferred between the actual individuals (remember that this is not the case for class level restrictions \u2013 that all instances of a class hold a property does not mean that the filler objects at the other end hold the inverse; the quantification on the restriction tells us this). Remember that: Robert holds a hasFather property with David; David holds a hasFather property with William; By implication through the hasParent super-property of hasFather , Robert holds a hasParent property with David, and the latter holds one with William; The sub-property chain on hasGrandfather then implies that Robert holds a hasGrandfather property to William. Use the diagram in figure 3.1 to trace the path; there is a hasParent path from Robert to William via David and this implies the hasGrandfather property between Robert and William. It is also useful to point out that the inverse of hasGrandfather also has the implication of the sub-property chain of the inverses of hasParent . That is, three objects linked by a path of two isParentOf properties implies that an isGrandfatherOf property is established between the first and third object, in this case William and Robert. As the inverses of hasFather are established by the reasoner, all the inverse implications also hold.","title":"3.2 Grandparents and Great Grandparents"},{"location":"tutorial/fhkb/#33-summary","text":"It is important when dealing with property hierarchies to think in terms of properties between objects and of the implications \u2018up the hierarchy\u2019. A sub-property implies its super-property. So, in our FHKB, two person objects holding a hasParent property between them, by implication also hold an hasAncestor property between them. In turn, hasAncestor has a super-property hasRelation and the two objects in question also hold, by implication, this property between them as well. We made hasAncestor transitive. This means that my ancestor\u2019s ancestors are also my ancestors. That a sub-property is transitive does not imply that its super-property is transitive. We have seen that by manipulating the property hierarchy we can generate a lot of inferences without adding any more facts to the individuals in the FHKB. This will be a feature of the whole process \u2013 keep the work to the minimum (well, almost). In OWL 2, we can also trace \u2018paths\u2019 around objects. Again, think of the objects involved in the path of properties that link objects together. We have done simple paths so far \u2013 Robert linked to David via hasParent and David linked to William via hasFather implies the link between Robert and William of hasGrandfather . If this is true for all cases (for which you have to use your domain knowledge), one can capture this implication in the property hierarchy. Again, we are making our work easier by adding no new explicit facts, but making use of the implication that the reasoner works out for us. The FHKB ontology at this stage of the tutorial has an expressivity ofALRI+. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.262 sec (0.00014 % of final), by Pellet 2.2.0 0.030 sec (0.00024 % of final) and by FaCT++ 1.6.4 is approximately 0.004 sec (0.000 % of final). 0 sec indicates failure or timeout.","title":"3.3 Summary"},{"location":"tutorial/fhkb/#chapter-4","text":"","title":"Chapter 4"},{"location":"tutorial/fhkb/#modelling-the-person-class","text":"In this Chapter you will: Create a Person class; Describe Sex classes; Define Man and Woman ; Ask which of the people in the FHKB has a father. Add domains and ranges to the properties in the FHKB. Make the FHKB inconsistent. Add some more defined classes about people and see some equivalence inferred between classes. These simple classes will form the structure for the whole FHKB.","title":"Modelling the Person Class"},{"location":"tutorial/fhkb/#41-the-class-of-person","text":"For the FHKB, we start by thinking about the objects involved The people in a family \u2013 Robert, Richard, David, Margaret, William, Iris, Charles, Violet, Eileen, John and Peter; The sex of each of those people; The marriages in which they participated; The locations of their births; And many more... There is a class of Person that we will use to represent all these people objects. Task 6: Create the Person class Create a class called DomainEntity ; Create a subclass of DomainEntity called Person . We use DomainEntity as a house-keeping measure. All of our ontology goes underneath this class. We can put other classes \u2018outside\u2019 the ontology, as siblings of DomainEntity , such as \u2018probe\u2019 classes we wish to use to test our ontology. The main thing to remember about the Person class is that we are using it to represent all \u2018people\u2019 individuals. When we make statements about the Person class, we are making statements about all \u2018people\u2019 individuals. What do we know about people? All members of the Person class have: Sex \u2013 they are either male or female; Everyone has a birth year; Everyone has a mother and a father. There\u2019s a lot more we know about people, but we will not mention it here.","title":"4.1 The Class of Person"},{"location":"tutorial/fhkb/#42-describing-sex-in-the-fhkb","text":"Each and every person object has a sex. In the FHKB we will take a simple view on sex \u2013 a person is either male or female, with no intersex or administrative sex and so on. Each person only has one sex. We have two straight-forward options for modelling sex: Each person object has their own sex object, which is either male or female. Thus Robert\u2019s maleness is different from David\u2019s maleness. There is only one Maleness object and one Femaleness object and each person object has a relationship to either one of these sex objects, but not both. We will take the approach of having a class of Maleness objects and a class of Femaleness objects. These are qualities or attributes of self-standing objects such as a person. These two classes are disjoint, and each is a subclass of a class called Sex . The disjointness means that any one instance of Sex cannot be both an instance of Maleness and an instance of Femaleness at once. We also want to put in a covering axiom on the class Sex , which means that any instance of Sex must be either Maleness or Femaleness ; there is no other kind of Sex . Again, notice that we have been thinking at the level of objects. We do the same when thinking about Person and their Sex . Each and every person is related to an instance of Sex . Each Person holds one relationship to a Sex object. To do this we create an object property called hasSex . We make this property functional, which means that any object can hold that property to only one distinct filler object. We make the domain of hasSex to be Person and the range to be Sex . The domain of Person means that any object holding that property will be inferred to be a member of the class Person . Putting the range of Sex on the hasSex property means that any object at the right-hand end of the hasSex property will be inferred to be of the class Sex . Again, think at the level of individuals or objects. We now put a restriction on the Person class to state that each and every instance of the class Person holds a hasSex property with an instance of the Sex class. It has an existential operator \u2018some\u2019 in the axiom, but the functional characteristic means that each Person object will hold only one hasSex property to a distinct instance of a Sex object 4 . 4 An individual could hold two hasSex properties, as long as the sex objects at the right-hand end of the property are not different. Task 7: Modelling sex Create a class called Sex ; Make it a subclass of DomainEntity ; Make Person and Sex disjoint; Create two subclasses of Sex , Maleness and Femaleness ; Make Maleness and Femaleness disjoint; Put a covering axiom on Sex such that it is equivalent to Maleness or Femaleness . Create an object property, hasSex , with the domain Person , the range Sex and give it the characteristic of \u2018Functional\u2019; Add a restriction hasSex some Sex to the class Person . The hasSex property looks like: ObjectProperty: hasSex Characteristics: Functional Domain: Person Range: Sex The Person class looks like: Class: Person SubClassOf: DomainEntity,(hasSex some Sex) DisjointWith: Sex","title":"4.2 Describing Sex in the FHKB"},{"location":"tutorial/fhkb/#43-defining-man-and-woman","text":"We now have some of the foundations for the FHKB. We have the concept of Person , but we also need to have the concepts of Man and Woman . Now we have Person , together with Maleness and Femaleness , we have the necessary components to define Man and Woman . These two classes can be defined as: Any Person object that has a male sex can be recognised to be a man; any Person object that has a female sex can be recognised as a member of the class woman. Again, think about what conditions are sufficient for an object to be recognised to be a member of a class; this is how we create defined classes through the use of OWL equivalence axioms. To make the Man and Woman classes do the following: Task 8: Describe men and women Create a class Man ; Make it equivalent to a Person that hasSex some Maleness ; Do the same, but with Femaleness , to create the Woman class; A covering axiom can be put on the Person class to indicate that man and woman are the only kinds of person that can exist. (This is not strictly true due to the way Sex has been described.) Run the reasoner and take a look. Having run the reasoner, the Man and Woman classes should appear underneath Person 5 . 5 Actually in Prot\u00e9g\u00e9, this might happen without the need to run the reasoner. The Man and Woman classes will be important for use as domain and range constraints on many of the properties used in the FHKB. To achieve our aim of maximising inference, we should be able to infer that individuals are members of Man , Woman or Person by the properties held by an object. We should not have to state the type of an individual in the FHKB. The classes for Man and Woman should look like: Class: Man EquivalentTo: Person and (hasSex some Maleness) Class: Woman EquivalentTo: Person and (hasSex some Femaleness)","title":"4.3 Defining Man and Woman"},{"location":"tutorial/fhkb/#44-describing-parentage-in-the-fhkb","text":"To finish off the foundations of the FHKB we need to describe a person object\u2019s parentage. We know that each and every person has one mother and each and every person has one father. Here we are talking about biological mothers and fathers. The complexities of adoption and step parents are outside the scope of this FHKB tutorial. Task 9: Describing Parentage Add the domain Person and the range Woman to the property hasMother . Do the same for the property hasFather , but give it the range Man ; Give the property hasParent domain and range of Person ; Run the reasoner. The (inferred) property hierarchy in the FHKB should look like that shown in Figure 4.1. Notice that we have asserted the sub-property axioms on one side of the property hierarchy. Having done so, the reasoner uses those axioms, together with the inverses, to work out the property hierarchy for the \u2018other side\u2019. We make hasMother functional, as any one person object can hold only one hasMother property to a distinct Woman object. The range of hasMother is Woman , as a mother has to be a woman. The Person object holding the hasMother property can be either a man or a woman, so we have the domain constraint as Person ; this means any object holding a hasMother property will be inferred to be a Person . Similarly, any object at the right-hand end of a hasMother property will be inferred to be a Woman , which is the result we need. The same reasoning goes for hasFather and hasParent , with the sex constraints on the latter being only Person . The inverses of the two functional sub-properties of hasParent are not themselves functional. After all, a Woman can be the mother of many Person objects, but each Person object can have only one mother. Figure 4.1: The property hierarchy with the hasSex and the parentage properties Figure 4.2: the core TBox for the FHKB with the Person and Sex classes. Task 10: Restrict Person class As each and every person has a mother and each and every person has a father, place restrictions on the Person class as shown below. Class: Person SubClassOf: DomainEntity, (hasFather some Man), (hasMother some Woman), (hasSex some Sex) DisjointWith: Sex Task 11: DL queries for people and sex Issue the DL queries for Person , Man and Woman ; look at the answers and count the numbers in each class; which individuals have no sex and why? You should find that many people have been inferred to be either Man or Woman , but some are, as we will see below, only inferred to be Person . The domain and range constraints on our properties have also driven some entailments. We have not asserted that David_Bright_1934 is a member of Man , but the range constraint on hasFather (or the inferred domain constraint on the isFatherOf relation) has enabled this inference to be made. This goes for any individual that is the right-hand-side (either inferred or asserted) of either hasFather or hasMother (where the range is that of Woman ). For Robert David Bright, however, he is only the left-hand-side of an hasFather or an hasMother property, so we\u2019ve only entailed that this individual is a member of Person .","title":"4.4 Describing Parentage in the FHKB"},{"location":"tutorial/fhkb/#45-who-has-a-father","text":"In our description of the Person class we have said that each and every instance of the class Person has a father (the same goes for mothers). So, when we ask the query \u2018which individuals have a father\u2019, we get all the instances of Person back, even though we have said nothing about the specific parentage of each Person . We do not know who their mothers and fathers are, but we know that they have one of each. We know all the individuals so far entered are members of the Person class; when asserting the type to be either Man or Woman (each of which is a subclass of Person ), we infer that each is a person. When asserting the type of each individual via the hasSex property, we know each is a Person , as the domain of hasSex is the Person class. As we have also given the right-hand side of hasSex as either Maleness or Femaleness , we have given sufficient information to recognise each of these Person instances to be members of either Man or Woman .","title":"4.5 Who has a father?"},{"location":"tutorial/fhkb/#46-filling-in-domains-and-ranges-for-the-fhkb-properties","text":"So far we have not systematically added domains and ranges to the properties in the FHKB. As a reminder, when a property has a domain of X any object holding that property will be inferred to be a member of class X . A domain doesn\u2019t add a constraint that only members of class X hold that property; it is a strong implication of class membership. Similarly, a property holding a range implies that an object acting as right-hand-side to a property will be inferred to be of that class. We have already seen above that we can use domains and ranges to imply the sex of people within the FHKB. Do the following: Task 12: Domains and Ranges Make sure the appropriate Person , Man and Woman are domains and ranges for hasFather , hasMother and hasParent . Run the reasoner and look at the property hierarchy. Also look at the properties hasAncestor , hasGrandparent , hasUncle and so on; look to see what domains and ranges are found. Add any domains and ranges explicitly as necessary. Prot\u00e9g\u00e9 for example in its current version (November 2015) does not visualise inherited domains and ranges in the same way as it shows inferred inverse relations. We typically assert more domains and ranges than strictly necessary. For example, if we say that hasParent has the domain Person , this means that every object x that is connected to another object y via the hasParent relation must be a Person . Let us assume the only thing we said about x and y is that they are connected by a hasMother relation. Since this implies that x and y are also connected by a hasParent relation ( hasMother is a sub-property of hasParent ) we do not have to assert that hasFather has the domain of Person ; it is implied by what we know about the domain and range of hasParent . In order to remove as many assertions as possible, we may therefore choose to assert as much as we know starting from the top of the hierarchy, and only ever adding a domain if we want to constrain the already inferred domain even further (or range respectively). For example, in our case, we could have chosen to assert Person to be the domain of hasRelation . Since hasRelation is symmetric, it will also infer Person to be the range. We do not need to say anything for hasAncestor or hasParent , and only if we want to constrain the domain or range further (like in the case of hasFather by making the range Man ) do we need to actually assert something. It is worth noting that because we have built the object property hierarchy from the bottom ( hasMother etc.) we have ended up asserting more than necessary.","title":"4.6 Filling in Domains and Ranges for the FHKB Properties"},{"location":"tutorial/fhkb/#47-inconsistencies","text":"From the Pizza Tutorial and other work with OWL you should have seen some unsatisfiabilities . In Prot\u00e9g\u00e9 this is highlighted by classes going \u2018red\u2019 and being subclasses ofNothing; that is, they can have no instances in that model. Task 13: Inconsistencies Add the fact Robert_David_Bright_1965 hasMother David_Bright_1934 . Run the classifier and see what happens. Remove that fact and run the classifier again. Now add the fact that Robert_David_Bright_1965 hasMother Iris_Ellen_Archer_1907 Run the classifier and see what happens. Add and remove the functional characteristic to these properties and see what happens. After asserting the first fact it should be reported by the reasoner that the ontology is inconsistent . This means, in lay terms, that the model you\u2019ve provided in the ontology cannot accommodate the facts you\u2019ve provided in the fact assertions in your ABox\u2014that is, there is an inconsistency between the facts and the ontology... The ontology is inconsistent because David_Bright_1934 is being inferred to be a Man and a Woman at the same time which is inconsistent with what we have said in the FHKB. When we, however, say that Robert David Bright has two different mothers, nothing bad happens! Our domain knowledge says that the two women are different, but the reasoner does not know this as yet... ; Iris Ellen Archer and Margaret Grace Rever may be the same person; we have to tell the reasoner that they are different. For the same reason the functional characteristic also has no effect until the reasoner \u2018knows\u2019 that the individuals are different. We will do this in Section 7.1.1 and live with this \u2018fault\u2019 for the moment.","title":"4.7 Inconsistencies"},{"location":"tutorial/fhkb/#48-adding-some-defined-classes-for-ancestors-and-so-on","text":"Task 14: Adding defined classes Add a defined class for Ancestor , MaleAncestor , FemaleAncestor ; Add a defined class for Descendant , MaleDescendant and FemaleDescendant ; Run the reasoner and view the resulting hierarchy. The code for the classes looks like: Class: Ancestor EquivalentTo: Person and isAncestorOf some Person Class: FemaleAncestor EquivalentTo: Woman and isAncestorOf some Person Class: Descendant EquivalentTo: Person and hasAncestor some Person Class: MaleDescendant EquivalentTo: Man and hasAncestor some Person The TBox after reasoning can be seen in Figure 4.3. Notice that the reasoner has inferred that several of the classes are equivalent or \u2018the same\u2019. These are: Descendant and Person ; MaleDescendant and Man , FemaleDescendant and Woman . The reasoner has used the axioms within the ontology to infer that all the instances of Person are also instances of the class Descendant and that all the instances of Woman are also the same instances as the class Female Descendant . This is intuitively true; all people are descendants \u2013 they all have parents that have parents etc. and thus everyone is a descendant. All women are female people that have parents etc. As usual we should think about the objects within the classes and what we know about them. This time it is useful to think about the statements we have made about Person in this Chapter \u2013 that all instances of Person have a father and a mother; add to this the information from the property hierarchy and we know that all instances of Person have parents and ancestors. We have repeated all of this in our new defined classes for Ancestor and Descendant and the reasoner has highlighted this information. Figure 4.3: The defined classes from Section 4.8 in the FHKB\u2019s growing class hierarchy Task 15: More Ancestors Query for MaleDescendant . You should get Man back - they are equivalent (and this makes sense). As an additional exercise, also add in properties for forefathers and foremothers. You will follow the same pattern as for hasAncestor , but adding in, for instance, hasFather as the sub-property of the transitive super-property of hasForefather and setting the domains and ranges appropriately (or working out if they\u2019ll be inferred appropriately). Here we interpret a forefather as one\u2019s father\u2019s father etc. This isn\u2019t quite right, as a forefather is any male ancestor, but we\u2019ll do it that way anyway. You might want to play around with DL queries. Because of the blowup in inferred relationships, we decided to not include this pattern in the tutorial version of the FHKB.","title":"4.8 Adding Some Defined Classes for Ancestors and so on"},{"location":"tutorial/fhkb/#49-summary","text":"Most of what we have done in this chapter is straight-forward OWL, all of which would have been met in the pizza tutorial. It is, however, a useful revision and it sets the stage for refining the FHKB. Figure 4.2 shows the basic set-up we have in the FHKB in terms of classes; we have a class to represent person, man and woman, all set-up with a description of sex, maleness and femaleness. It is important to note, however, the approach we have taken: We have always thought in terms of the objects we are modelling. Here are some things that should now be understood upon completing this chapter: Restrictions on a class in our TBox mean we know stuff about individuals that are members of that class, even though we have asserted no facts on those individuals. We have said, for instance, that all members of the class Person have a mother, so any individual asserted to be a Person must have a mother. We do not necessarily know who they are, but we know they have one. Some precision is missing \u2013 we only know Robert David Bright is a Person , not that he is a Man . This is because, so far, he only has the domain constraint of hasMother and hasFather to help out. We can cause the ontology to be inconsistent, for example by providing facts that cannot be accommodated by the model of our ontology. In the example, David Bright was inferred to be a member of two disjoint classes. Finally, we looked at some defined classes. We inferred equivalence between some classes where the extents of the classes were inferred to be the same \u2013 in this case the extents of Person and Descendant are the same. That is, all the objects that can appear in Person will also be members of Descendant . We can check this implication intuitively \u2013 all people are descendants of someone. Perhaps not the most profound inference of all time, but we did no real work to place this observation in the FHKB. This last point is a good general observation. We can make the reasoner do work for us. The less maintenance we have to do in the FHKB the better. This will be a principle that works throughout the tutorial. The FHKB ontology at this stage of the tutorial has an expressivity of SRIF. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.884 sec (0.00047 % of final), by Pellet 2.2.0 0.256 sec (0.00207 % of final) and by FaCT++ 1.6.4 is approximately 0.013 sec (0.000 % of final). 0 sec indicates failure or timeout.","title":"4.9 Summary"},{"location":"tutorial/fhkb/#chapter-5","text":"","title":"Chapter 5"},{"location":"tutorial/fhkb/#siblings-in-the-fhkb","text":"In this chapter you will: Explore options for determining finding siblings; Meet some of the limitations in OWL; Choose one of the options explored; Add facts for siblings; Use sub-property chains to find aunts and uncles; There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial","title":"Siblings in the FHKB"},{"location":"tutorial/fhkb/#51-blood-relations","text":"Do the following first: Task 16: The bloodrelation object property Create an hasBloodrelation object property, making it a sub-property of hasRelation . Add appropriate property characteristics. Make the already existing hasAncestor property a sub-property of hasBloodrelation . Does a blood relation of Robert have the same relationship to Robert (symmetry)? Is a blood relation of Robert\u2019s blood relation a blood relation of Robert (transitivity)? Think of an aunt by marriage; her children are my cousins and blood relations via my uncle, but my aunt is not my blood relation. My siblings share parents; male siblings are brothers and female siblings are sisters. So far we have asserted parentage facts for the Person in our ABox. Remember that our parentage properties have inverses, so if we have added an hasFather property between a Person and a Man , we infer the isFatherOf property between that Man and that Person .","title":"5.1 Blood relations"},{"location":"tutorial/fhkb/#52-siblings-option-one","text":"We should have enough information within the FHKB to infer siblings. We could use a sub-property chain such as: ObjectProperty: hasSibling SubPropertyOf: hasBloodrelation Characteristics: Symmetric, transitive SubPropertyChain: hasParent o isParentOf We make a property of hasSibling and make it a sub-property of hasBloodrelation . Remember, think of the objects involved and the implications we want to follow; being a sibling implies being a blood relation, it does not imply any of the other relationships we have in the FHKB. Note that we have made hasSibling symmetric; if Robert is sibling of Richard, then Richard is sibling of Robert. We should also think about transitivity; if David is sibling of Peter and Peter is sibling of John, then David is sibling of John. So, we make hasSibling symmetric and transitive (see Figure 5.1). However, we must take care of half-siblings: child 1 and child 2 share a mother, but not a father; child 2 and child 3 share the father, but not the mother \u2013 child 1 and child 3 are not even half-siblings. However, at least for the moment, we will simply ignore this inconvenience, largely so that we can explore what happens with different modelling options. Figure 5.1: Showing the symmetry and transitivity of the hasSibling (siblingof) property by looking at the brothers David, John and Peter We also have the implication using three objects (see Figure 5.2): Robert holds a hasParent property with David; David holds an isFatherOf property with Richard; This implies that Robert holds a hasSibling property with Richard; As hasSibling is symmetric, Richard holds an hasSibling property with Robert. Figure 5.2: Tracing out the sub-property chain for hasSibling ; note that Robert is a sibling of himself by this path Do the following tasks: Task 17: Siblings Add the hasSibling property as above; Run the reasoner; Ask the DL query hasSibling value Robert_David_Bright_1965 . From this last DL query you should get the answer that both Robert and Richard are siblings of Robert. Think about the objects involved in the sub-property chain: we go from Robert to David via the hasParent and from David to Richard via the isParentOf property; so this is OK. However, we also go from Robert to David and then we can go from David back to Robert again \u2013 so Robert is a sibling of Robert. We do not want this to be true. We can add another characteristic to the hasSibling property, the one of being irreflexive . This means that an object cannot hold the property with itself. Task 18: More siblings Add the irreflexive characteristic to the hasSibling property; Run the reasoner; Note that the reasoner claims you have an inconsistent ontology (or in some cases, you might get a message box saying \"Reasoner died\"). Looking at the hasSibling property again, the reason might not be immediately obvious. The reason for the inconsistency lies in the fact that we create a logical contradiction: through the property chain, we say that every Person is a sibling of him or herself, and again disallowing just that by adding the irreflexive characteristic. A different explanation lies within the OWL specification itself: In order to maintain decidability irreflexive properties must be simple - for example, they may not be property chains 6 . 6 http://www.w3.org/TR/owl2-syntax/#The_Restrictions_on_the_Axiom_Closure","title":"5.2 Siblings: Option One"},{"location":"tutorial/fhkb/#521-brothers-and-sisters","text":"We have only done siblings, but we obviously need to account for brothers and sisters. In an analogous way to motherhood, fatherhood and parenthood, we can talk about sex specific sibling relationships implying the sex neutral hasSibling ; holding either a hasBrother or an isSisterOf between two objects would imply that a hasSibling property is also held between those two objects. This means that we can place these two sex specific sibling properties below hasSibling with ease. Note, however, that unlike the hasSibling property, the brother and sister properties are not symmetric. Robert hasBrother Richard and vice versa , but if Daisy hasBrother William, we do not want William to hold an hasBrother property with Daisy. Instead, we create an inverse of hasBrother , isBrotherOf , and the do the same for isSisterOf . We use similar, object based, thought processes to choose whether to have transitivity as a characteristic of hasBrother . Think of some sibling objects or individuals and place hasBrother properties between them. Make it transitive and see if you get the right answers. Put in a sister to and see if it stil works. If David hasBrother Peter and Peter hasBrother John, then David hasBrother John; so, transitivity works in this case. Think of another example. Daisy hasBrother Frederick, and Frederick hasBrother William, thus Daisy hasBrother William. The inverses work in the same way; William isBrotherOf Frederick and Frederick isBrotherOf Daisy; thus William isBrotherOf Daisy. All this seems reasonable. Task 19: Brothers and sisters Create the hasBrother object property as shown below; Add hasSister in a similar manner; 3. Add appropriate inverses, domains and ranges. ObjectProperty: hasBrother SubPropertyOf: hasSibling Characteristics: Transitive InverseOf: isBrotherOf Range: Man We have some hasSibling properties (even if they are wrong). We also know the sex of many of the people in the FHKB through the domains and ranges of properties such as hasFather , hasMother and their inverses.. Can we use sub-property chains in the same way as we have used them in the hasSibling property? The issue is that of sex; the property isFatherOf is sex neutral at the child end, as is the inverse hasFather (the same obviously goes for the mother properties). We could use a sub-property chain of the form: ObjectProperty: hasBrother SubPropertyChain: hasParent o hasSon A son is a male child and thus that object is a brother of his siblings. At the moment we do not have son or daughter properties. We can construct a property hierarchy as shown in Figure 5.3. This is made up from the following properties: hasChild and isChildOf hasSon (range Man and domain Person ) and isSonOf ; hasDaughter (range Woman domain Person ) and isDaughterOf Note that hasChild is the equivalent of the existing property isParentOf ; if I have a child, then I am its parent. OWL 2 can accommodate this fact. We can add an equivalent property axiom in the following way: ObjectProperty: isChildOf EquivalentTo: hasParent We have no way of inferring the isSonOf and isDaughterOf from what already exists. What we want to happen is the implication of \u2018 Man and hasParent Person implies isSonOf \u2019. OWL 2 and its reasoners cannot do this implication. It has been called the \u2018man man problem\u2019 7 . Solutions for this have been developed [3], but are not part of OWL 2 and its reasoners. Figure 5.3: The property hierarchy for isChildOf and associated son/daughter properties 7 http://lists.w3.org/Archives/Public/public-owl-dev/2007JulSep/0177.html Child property Parent Robert David Bright 1965 isSonOf David Bright 1934, Margaret Grace Rever 1934 Richard John Bright 1962 isSonOf David Bright 1934, Margaret Grace Rever 1934 Mark Bright 1956 isSonOf John Bright 1930, Joyce Gosport Ian Bright 1959 isSonOf John Bright 1930, Joyce Gosport Janet Bright 1964 isDaughterOf John Bright 1930, Joyce Gosport William Bright 1970 isSonOf John Bright 1930, Joyce Gosport Table 5.1: Child property assertions for the FHKB Thus we must resort to hand assertions of properties to test out our new path: Task 20: Sons and daughters Add the property hierarchy shown in Figure 5.3, together with the equivalent property axiom and the obvious inverses. As a test (after running the reasoner), ask the DL query isChildOf value David_Bright_1934 and you should have the answer of Richard and Robert; Add the sub-property paths as described in the text; Add the assertions shown in Table 5.1; Run the reasoner; Ask the DL query for the brother of Robert David Bright and the sister of Janet. Of course, it works, but we see the same problem as above. As usual, think of the objects involved. Robert isSonOf David and David isParentOf Robert, so Robert is his own brother. Irreflexivity again causes problems as it does above (Task 18).","title":"5.2.1 Brothers and Sisters"},{"location":"tutorial/fhkb/#53-siblings-option-two","text":"Our option one has lots of problems. So, we have an option of asserting the various levels of sibling. We can take the same basic structure of sibling properties as before, but just fiddle around a bit and rely on more assertion while still trying to infer as much as possible. We will take the following approach: We will take off the sub-property chains of the sibling properties as they do not work; We will assert the leaf properties of the sibling sub-hierarchy sparsely and attempt to infer as much as possible. Person Property Person Robert David Bright 1965 isBrotherOf Richard John Bright 1962 David Bright 1934 isBrotherOf John Bright 1930 David Bright 1934 isBrotherOf Peter William Bright 1941 Janet Bright 1964 isSisterOf Mark Bright 1956 Janet Bright 1964 isSisterOf Ian Bright 1959 Janet Bright 1964 isSisterOf William Bright 1970 Mark Bright 1956 isBrotherOf Ian Bright 1959 Mark Bright 1956 isBrotherOf Janet Bright 1964 Mark Bright 1956 isBrotherOf William Bright 1970 Table 5.2: The sibling relationships to add to the FHKB. Do the following: Task 21: Add sibling assertions Remove the sub-property chains of the sibling properties and the isChildOf assertions as explained above. Add the Sibling assertions shown in table 5.2; Run the reasoner; Ask isBrotherOf value Robert_David_Bright_1965 ; Ask isBrotherOf value Richard_John_Bright_1962 ; Ask hasBrother value Robert_David_Bright_1965 ; Ask hasBrother value Richard_John_Bright_1962 ; Ask isSisterOf value William_Bright_1970 ; Ask the query Man and hasSibling value Robert_David_Bright_1965 . We can see some problems with this option as well: With these properties asserted, Richard only has a hasBrother property to Robert. We would really like an isBrotherOf to Robert to hold. The query Man and hasSibling value Robert only retrieves Robert himself. Because we only asserted that Robert is a brother of Richard, and the domain of isBrotherOf is Man we know that Robert is a Man , but we do not know anything about the Sex of Richard.","title":"5.3 Siblings: Option two"},{"location":"tutorial/fhkb/#531-which-modelling-option-to-choose-for-siblings","text":"Which of the two options gives the worse answers and which is the least effort? Option one is obviously the least effort; we only have to assert the same parentage facts as we already have; then the sub-property chains do the rest. It works OK for hasSibling , but we cannot do brothers and sisters adequately; we need Man and hasSibling \u2290 isBrotherOf and we cannot do that implication. This means we cannot ask the questions we need to ask. So, we do option two, even though it is hard work and is still not perfect for query answering, even though we have gone for a sparse assertion mode. Doing full sibling assertion would work, but is a lot of effort. We could start again and use the isSonOfandisDaughterOf option, with the sub-property chains described above. This still has the problem of everyone being their own sibling. It can get the sex specific sibling relationships, but requires a wholesale re-assertion of parentage facts. We will continue with option two, largely because it highlights some nice problems later on.","title":"5.3.1 Which Modelling Option to Choose for Siblings?"},{"location":"tutorial/fhkb/#54-half-siblings","text":"In Section 5.2 we briefly talked about half-siblings. So far, we have assumed full-siblings (or, rather, just talked about siblings and made no distinction). Ideally, we would like to accommodate distinctions between full- and half-siblings; here we use half-siblings, where only one parent is in common between two individuals, as the example. The short-answer is, unfortunately, that OWL 2 cannot deal with half-siblings in the way that we want - that is, such that we can infer properties between named individuals indicating full- or half-sibling relationships. It is possible to find sets of half-brothers in the FHKB by writing a defined class or DL-query for a particular individual.} The following fragment of OWL defines a class that looks for the half-brothers of an individual called \u2018Percival\u2019: Class: HalfBrotherOfPercival EquivalentTo: Man and (((hasFather some (not (isFatherOf value Percival))) and (hasMother some (isMotherOf value Percival))) or ((hasFather some (isFatherOf value Percival)) and (hasMother some (not (isMotherOf value Percival))))) Here we are asking for any man that either has Percival\u2019s father but not his mother, or his mother, but not his father. This works fine, but is obviously not a general solution. The OWL description is quite complex and the writing will not scale as the number of options (hypothetically, as the number of parents increases... ) increases; it is fine for man/woman, but go any higher and it will become very tedious to write all the combinations. Another way of doing this half-brother class to find the set of half-brothers of a individual is to use cardinality constraints: Class: HalfBrotherOfPercival EquivalentTo: Man and (hasParent exactly 1 (isParentOf value Percival)) This is more succinct. We are asking for a man that has exactly one parent from the class of individuals that are the class of Percival\u2019s parents. This works, but one more constraint has to be present in the FHKB. We need to make sure that there can be only two parents (or indeed, just a specified number of parents for a person). If we leave it open as to the number of parents a person has, the reasoner cannot work out that there is a man that shares exactly one parent, as there may be other parents. We added this constraint to the FHKB in Section 6.2; try out the classes to check that they work. These two solutions have been about finding sets of half-brothers for an individual. What we really want in the FHKB is to find half-brothers between any given pair of individuals. Unfortunately we cannot, without rules, ask OWL 2 to distinguish full- and half-siblings \u2013 we cannot count the number of routes taken between siblings via different distinct intermediate parent objects.","title":"5.4 Half-Siblings"},{"location":"tutorial/fhkb/#55-aunts-and-uncles","text":"An uncle is a brother of either my mother or father. An aunt is a sister of either my mother or father. In common practice, wives and husbands of aunts and uncles are usually uncles and aunts respectively. Formally, these aunts and uncles are aunts-in-law and uncles-in-law. Whatever approach we take, we cannot fully account for aunts and uncles until we have information about marriages, which will not have until Chapter 9. We will, however, do the first part now. Look at the objects and properties between them for the following facts: Robert has father David and mother Margaret; David has brothers Peter and John; Margaret has a sister Eileen; Robert thus has the uncles John and Peter and an aunt Eileen. As we are tracing paths or \u2018chains\u2019 of objects and properties we should use sub-property chains as a solution for the aunts and uncles. We can make an hasUncle property as follows (see Figure 5.4): ObjectProperty: hasUncle SubPropertyOf: hasBloodrelation Domain: Man Range: Person SubPropertyChain: hasParent o hasBrother InverseOf: isUncleOf Figure 5.4: Tracing out the path between objects to get the hasUncle sub-property chain. Notice we have the domain of Man and range of Person . We also have an inverse. As usual, we can read this as \u2018an object that holds an hasParent property, followed by an object holding a hasBrother property, implies that the first object holds an hasUncle property with the last object\u2019. Note also where the properties (include the ones for aunt) go in the object property hierarchy. Aunts and uncles are not ancestors that are in the direct blood line of a person, but they are blood relations (in the narrower definition that we are using). Thus the aunt and uncle properties go under the hasBloodrelation property (see Figure 5.5). Again, think of the implications between objects holding a property between them; that two objects linked by a property implies that those two objects also hold all the property\u2019s super-properties as well. As long as all the super-properties are true, the place in the object property hierarchy is correct (think about the implications going up, rather than down). Figure 5.5: The object property hierarchy with the aunt and uncle properties included. On the right side, we can see the hasUncle property as shown by Prot\u00e9g\u00e9. Do the following tasks: Task 22: Uncles and Aunts Add the hasUncle property as above; Add the hasAunt property as well; Ask for the uncles of Julie_Bright_1966 and for Mark_Bright_1956 ; Add similar properties for hasGreatUncle and hasGreatAunt and place them in the property hierarchy. We can see this works \u2013 unless we have any gaps in the sibling relationships (you may have to fix these). Great aunts and uncles are simply a matter of adding another \u2018parent\u2019 leg into the sub-property chain. We are not really learning anything new with aunts and uncles, except that we keep gaining a lot for free through sub-property chains. We just add a new property with its sub-property chain and we get a whole lot more inferences on individuals. To see what we now know about Robert David Bright, do the following: Task 23: What do we know? Save the ontology and run the reasoner; Look at inferences related to the individual Robert David Bright (see warning in the beginning of this chapter). If you chose to use DL queries in Prot\u00e9g\u00e9, do not forget to tick the appropriate check-boxes. You can now see lots of facts about Robert David Bright, with only a very few actual assertions directly on Robert David Bright.","title":"5.5 Aunts and Uncles"},{"location":"tutorial/fhkb/#56-summary","text":"Siblings have revealed several things for us: We can use just the parentage facts to find siblings, but everyone ends up being their own sibling; We cannot make the properties irreflexive, as the knowledge base becomes inconsistent; We would like an implication of Man and hasSibling \u2283 isBrotherOf , but OWL 2 doesn\u2019t do this implication; Whatever way we model siblings, we end up with a bit of a mess; OWL 2 cannot do half-siblings; However, we can get close enough and we can start inferring lots of facts via sub-property chains using the sibling relationships. The FHKB ontology at this stage of the tutorial has an expressivity ofSRIF. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 1355.614 sec (0.71682 % of final), by Pellet 2.2.0 0.206 sec (0.00167 % of final) and by FaCT++ 1.6.4 is approximately 0.039 sec (0.001 % of final). 0 sec indicates failure or timeout.","title":"5.6 Summary"},{"location":"tutorial/fhkb/#chapter-6","text":"","title":"Chapter 6"},{"location":"tutorial/fhkb/#individuals-in-class-expressions","text":"In this chapter you will: Use individuals within class expressions; Make classes to find Robert and Richard\u2019s parents, ancestors, and so on; Explore equivalence of such classes; Re-visit the closed world. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial","title":"Individuals in Class Expressions"},{"location":"tutorial/fhkb/#61-richard-and-roberts-parents-and-ancestors","text":"So far we have only used object properties between unspecified objects. We can, however, specify a specific individual to act at the right-hand-side of a class restriction or type assertion on an individual. The basic syntax for so-called nominals is: Class: ParentOfRobert EquivalentTo: Person and isParentOf valueRobert_David_Bright_1965 This is an equivalence axiom that recognises any individual that is a Person and a parent of Robert David Bright. Task 24: Robert and Richards parents Create the class ParentOfRobert as described above; Classify \u2013 inspect where the class is placed in the FHKB TBox and look at which individuals classify as members of the class; Do the same for a class with the value of Richard_John_Bright_1962 and classify; Finally create a class ParentOfRichardAndRobert , defining it as Person and isParentOf some {Robert_David_Bright_1965 ,Richard_John_Bright_1962 } ; again see what happens on classification. Note that the expressions isMotherOf value Robert_David_Bright_1965 and isMotherOf some {Robert_David_Bright_1965 } are practically identical. The only difference is that using value , you can only specify one individual, while some relates to a class (a set of individuals). We see that these queries work and that we can create more complex nominal based class expressions. The disjunction above is isParentOf some {Robert_David_Bright_1965, Richard_John_Bright_1965} The \u2018{\u2019 and \u2018}\u2019 are a bit of syntax that says \u2018here\u2019s a class of individual\u2019. We also see that the classes for the parents of Robert David Bright and Richard John Bright have the same members according to the FHKB, but that the two classes are not inferred to be equivalent. Our domain knowledge indicates the two classes have the same extents (members) and thus the classes are equivalent, but the automated reasoner does not make this inference. As usual, this is because the FHKB has not given the automated reasoner enough information to make such an inference.","title":"6.1 Richard and Robert\u2019s Parents and Ancestors"},{"location":"tutorial/fhkb/#62-closing-down-what-we-know-about-parents-and-siblings","text":"The classes describing the parents of Richard and Robert are not equivalent, even though, as humans, we know their classes of parent are the same. We need more constraints so that it is known that the four parents are the only ones that exist. We can try this by closing down what we know about the immediate family of Robert David Bright. In Chapter 4 we described that a Person has exactly one Woman and exactly one Man as mother and father (by saying that the hasMother and hasFather properties are functional and thus only one of each may be held by any one individual to distinct individuals). The parent properties are defined in terms of hasParent , hasMother and hasFather . The latter two imply hasParent . The two sub-properties are functional, but there are no constraints on hasParent , so an individual can hold many instances of this property. So, there is no information in the FHKB to say a Person has only two parents (we say there is one mother and one father, but not that there are only two parents). Thus Robert and Richard could have other parents and other grandparents than those in the FHKB; we have to close down our descriptions so that only two parents are possible. There are two ways of doing this: Using qualified cardinality constraints in a class restriction; Putting a covering axiom on hasParent in the same way as we did for Sex in Chapter 4. Task 25: Closing the Person class Add the restriction hasParent exactly 2 Person to the classPerson ; Run the reasoner; Inspect the hierarchy to see where ParentOfRobert and ParentOfRichard are placed and whether or not they are found to be equivalent; Now add the restriction hasParent max 2 Person to the class Person ; Run the reasoner (taking note of how long the reasoning takes) and take another look. We find that these two classes are equivalent; we have supplied enough information to infer that these two classes are equivalent. So, we know that option one above works, but what about option two? This takes a bit of care to think through, but the basic thing is to think about how many ways there are to have a hasParent relationship between two individuals. We know that we can have either a hasFather or a hasMother property between two individuals; we also know that we can have only one of each of these properties between an individual and a distinct individual. However, the open world assumption tells us that there may be other ways of having a hasParent property between two individuals; we\u2019ve not closed the possibilities. By putting on the hasParent exactly 2 Person restriction on the Person class, we are effectively closing down the options for ways that a person can have parents; we know because of the functional characteristic on hasMother and hasFather that we can have only one of each of these and the two restrictions say that one of each must exist. So, we know we have two ways of having a parent on each Person individual. So, when we say that there are exactly two parents (no more and no less) we have closed down the world of having parents\u2014thus these two classes can be inferred to be equivalent. It is also worth noting that this extra axiom on the Person class will make the reasoner run much more slowly. Finally, for option 2, we have no way of placing a covering axiom on a property. What we\u2019d like to be able to state is something like: ObjectProperty: hasParent EquivalentTo: hasFather or hasMother but we can\u2019t.","title":"6.2 Closing Down What we Know About Parents and Siblings"},{"location":"tutorial/fhkb/#63-summary","text":"For practice, do the following: Task 26: Additional Practice Add lots more classes using members of the ABox as nominals; Make complex expressions using nominals; After each addition of a nominal, classify and see what has been inferred within the FHKB. See if you can make classes for GrandparentOfRobert and GrandparentOfRichard and make them inferred to be equivalent. In this chapter we have seen the use of individuals within class expressions. It allows us to make useful queries and class definitions. The main things to note is that it can be done and that there is some syntax involved. More importantly, some inferences may not be as expected due to the open world assumption in OWL. By now you might have noticed a significant increase in the time the reasoner needs to classify. Closing down what we know about family relationships takes its toll on the reasoner performance, especially the usage of 'hasParent exactly 2 Person'. At this point we recommend rewriting this axiom to 'hasParent max 2 Person'. It gives us most of what we need, but has a little less negative impact on the reasoning time. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 2067.273 sec (1.09313 % of final), by Pellet 2.2.0 0.529 sec (0.00428 % of final) and by FaCT++ 1.6.4 is approximately 0.147 sec (0.004 % of final). 0 sec indicates failure or timeout.","title":"6.3 Summary"},{"location":"tutorial/fhkb/#chapter-7","text":"","title":"Chapter 7"},{"location":"tutorial/fhkb/#data-properties-in-the-fhkb","text":"We now have some individuals with some basic object properties between individuals. OWL 2, however, also has data properties that can relate an object or individual to some item of data. There are data about a Person , such as years of events and names etc. So, in this Chapter you will: Make some data properties to describe event years to people; Create some simple defined classes that group people by when they were born; Try counting the numbers of children people have... Deal with the open world assumption; Add given and family names to individuals in the FHKB. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial.","title":"Data Properties in the FHKB"},{"location":"tutorial/fhkb/#71-adding-some-data-properties-for-event-years","text":"Everyone has a birth year; death year; and some have a marriage year and so on. We can model these simply with data properties and an integer as a filler. OWL 2 has a DateTime datatype, where it is possible to specify a precise time and date down to a second. 7 This proves cumbersome (see http://robertdavidstevens.wordpress.com/2011/05/05/using-the-datetime-data-type-to-describe-birthdays/ for details); all we need is a simple indication of the year in which a person was born. Of course, the integer type has a zero, which the Gregorian calendar for which we use integer as a proxy does not, but integer is sufficient to our needs. Also, there are various ontological treatments of time and information about people (this extends to names etc. as well), but we gloss over that here\u2014that\u2019s another tutorial. 7 http://www.w3.org/TR/2008/WD-owl2-quick-reference-20081202/#Built-in_Datatypes_and_Facets We can have dates for birth, death and (eventually) marriage (see Chapter 9) and we can just think of these as event years. We can make a little hierarchy of event years as shown in Figure 7.1). Task 27: Create a data property hierarchy Create the data property hasEventYear with range integer and domain Person ; Create the data property hasBirthYear and make it a sub-property of hasEventYear (that way, the domain and range of hasEventYear are inherited); Create the data property hasDeathYear and make it a sub-property of hasEventYear ; For each individual add the birth years shown in Table A.1 (see appendix). You do not actually have to go back to the table\u2014it is easier to read the birth years simply off the individual names. Again, asserting birth years for all individuals can be a bit tedious. The reader can find a convenience snapshot of the ontology at this stage at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial We now have an ABox with individuals with fact assertions to data indicating a birth year. We can, if we wish, also add a class restriction to the Person class saying that each and every instance of the class Person holds a data property to an integer and that this property is called \u2018hasBirthYear\u2019. As usual when deciding whether to place such a restriction upon a class, ask whether it is true that each and every instance of the class holds that property; this is exactly the same as we did for the object properties in Chapter 4. Everyone does have a birth year, even if it is not known. Once birth years have been added to our individuals, we can start asking some questions. Task 28: DL queries 1. Use a DL query to ask: Person born after 1960; Person born in the 1960s; Person born in the 1800s; Person that has fewer than three children; Person that has more than three children. The DL query for people born in the 1960s is: Person and hasBirthYear some int[>= 1960, < 1970] This kind of interval is known as a facet.","title":"7.1 Adding Some Data Properties for Event Years"},{"location":"tutorial/fhkb/#711-counting-numbers-of-children","text":"The last two queries in the list do not work as expected. We have asked, for instance, for Person that have more than three children, but we get no members of Person in the answer, though we know that there are some in the FHKB (e.g., John_Bright_1930 ). This is because there is not enough information in the FHKB to tell that this person has more than three different people as children. As humans we can look at the four children of John Bright and know that they are different \u2013 for instance, they all have different birth years. The automated reasoner, however, does not know that a Person can only have one birth year. Task 29: Make a functional object property Make the property hasBirthYear functional. Ask the query for Person that has more than three children again. This time the query should work. All the other event year properties should be made functional, expect hasEventYear , as one individual can have many event years. As the children have different birth year and an individual can only hold one hasBirthYear property, then these people must be distinct entities. Of course, making birth year functional is not a reliable way of ensuring that the automated reasoner knows that the individual are different. It is possible for two Person to have the same birth year within the same family \u2013 twins and so on. Peter_William_Bright_1941 has three children, two of which are twins, so will not be a member of the class of people with at least three children. So, we use the different individuals axiom. Most tools, including Prot\u00e9g\u00e9, have a feature that allows all individuals to be made different. Task 30: Make all individuals different Make all individuals different; Ask the above queries again. From now on, every time you add individuals, make sure the different individuals axiom is updated.","title":"7.1.1 Counting Numbers of Children"},{"location":"tutorial/fhkb/#72-the-open-world-assumption","text":"We have met again the open world assumption and its importance in the FHKB. In the use of the functional characteristic on the hasBirthYear property, we saw one way of constraining the interpretation of numbers of children. We also introduced the \u2018different individuals\u2019 axiom as a way of making all individuals in a knowledge base distinct. There are more questions, however, for which we need more ways of closing down the openness of OWL 2. Take the questions: People that have exactly two children; People that have only brothers; People that have only female children. We can only answer these questions if we locally close the world.We have said that David and Margaret have two children, Richard and Robert, but we have not said that there are not any others. As usual, try not to apply your domain knowledge too much; ask yourself what the automated reasoner actually knows. As we have the open world assumption, the reasoner will assume, unless otherwise said, that there could be more children; it simply doesn\u2019t know. Think of a railway journey enquiry system. If I ask a standard closed world system about the possible routes by rail, between Manchester and Buenos Aires, the answer will be \u2019none\u2019, as there are none described in the system. With the open world assumption, if there is no information in the system then the answer to the same question will simply be \u2018I don\u2019t know\u2019. We have to explicitly say that there is no railway route from Manchester to Buenos Aires for the right answer to come back. We have to do the same thing in OWL. We have to say that David and Margaret have only two children. We do this with a type assertion on individuals. So far we have only used fact assertions. A type assertion to close down David Bright\u2019 parentage looks like this: isParentOf only {Robert_David_Bright_1965,Richard_John_Bright_1962 } This has the same meaning as the closure axioms that you should be familiar with on classes. We are saying that the only fillers that can appear on the right-hand-side of the isParentOf property on this individual are the two individuals for Richard and Robert. We use the braces to represent the set of these two individuals. Task 31: Make a closure axiom Add the closure assertion above to David Bright; Issue the DL query isParentOf exactly 2 Person . The last query should return the answer of David Bright. Closing down the whole FHKB ABox is a chore and would really have to be done programmatically. OWL scripting languages such as the Ontology Preprocessing Language 8 (OPPL) [2] can help here. Also going directly to the OWL API [1] 9 , if you know what you are doing, is another route. Adding all these closure type assertions can slow down the reasoner; so think about the needs of your system \u2013 just adding it \u2018because it is right\u2019 is not necessarily the right route. 8 http://oppl2.sourceforge.net 9 http://owlapi.sourceforge.net/","title":"7.2 The Open World Assumption"},{"location":"tutorial/fhkb/#73-adding-given-and-family-names","text":"We also want to add some other useful data facts to people \u2013 their names. We have been putting names as part of labels on individuals, but data fact assertions make sense to separate out family and given names so that we can ask questions such as \u2018give me all people with the family name Bright and the first given name of either James or William\u2019. A person\u2019s name is a fact about that person and is more, in this case, than just a label of the representation of that person. So, we want family names and given names. A person may have more than one given name \u2013 \u2018Robert David\u2019, for instance \u2013 and an arbitrary number of given names can be held. For the FHKB, we have simply created two data properties of hasFirstGivenName and hasSecondGivenName ). Ideally, it would be good to have some index on the property to given name position, but OWL has no n-ary relationships. Otherwise, we could reify the hasGivenName property into a class of objects, such as the following: Class: GivenName SubClassOf:hasValue some String, hasPosition some Integer but it is really rather too much trouble for the resulting query potential. As already shown, we will use data properties relating instances of Person to strings. We want to distinguish family and given names, and then different positions of given names through simple conflating of position into the property name. Figure 7.1 shows the intended data property hierarchy. Figure 7.1: The event year and name data property hierarchies in the FHKB. Do the following: Task 32: Data properties Create the data properties as described in Figure 7.1; Give the hasName property the domain of Person and the range of String ; Make the leaf properties of given names functional; Add the names shown in Table A.1 (appendix); Again, it may be easier to read the names of the individual names. Ask the questions: all the people with the first given name \u2018James\u2019; all the people with the first given name \u2018William\u2019; All the people with the given name \u2018William\u2019; All the people with the given name \u2018William\u2019 and the family name \u2018Bright\u2019. The name data property hierarchy and the queries using those properties displays what now should be familiar. Sub-properties that imply the super-property. So, when we ask hasFirstGivenName value \"William\" and then the query hasGivenName value value \"William\" we can expect different answers. There are people with \u2018William\u2019 as either first or second given name and asking the question with the super-property for given names will collect both first and second given names.","title":"7.3 Adding Given and Family Names"},{"location":"tutorial/fhkb/#74-summary","text":"We have used data properties that link objects to data such as string, integer, floats and Booleans etc. OWL uses the XML data types. We have seen a simple use of data properties to simulate birth years. The full FHKB also uses them to place names (given and family) on individuals as strings. This means one can ask for the Person with the given name \"James\", of which there are many in the FHKB. Most importantly we have re-visited the open world assumption and its implications for querying an OWL ABox. We have looked at ways in which the ABox can be closed down \u2013 unreliably via the functional characteristic (in this particular case) and more generally via type assertions. All the DL queries used in this chapter can also serve as defined classes in the TBox. It is a useful exercise to progressively add more defined classes to the FHKB TBox. Make more complex queries, make them into defined classes and inspect where they appear in the class hierarchy. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 1891.157 sec (1.00000 % of final), by Pellet 2.2.0 1.134 sec (0.00917 % of final) and by FaCT++ 1.6.4 is approximately 0.201 sec (0.006 % of final). 0 sec indicates failure or timeout. Note that we now cover the whole range of expressivity of OWL 2. HermiT at least is impossibly slow by now. This may be because HermiT does more work than the others. For now, we recommend to use either Pellet or FaCT++.","title":"7.4 Summary"},{"location":"tutorial/fhkb/#chapter-8","text":"","title":"Chapter 8"},{"location":"tutorial/fhkb/#cousins-in-the-fhkb","text":"In this Chapter you will Revise or get to know about degrees and removes of cousin; Add the properties and sub-property chains for first and second cousins; Add properties and sub-property chains for some removes of cousins; Find out that the siblings debacle haunts us still; Add a defined class that does first cousins properly. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial Be warned; from here on the reasoner can start running slowly! Please see warning at the beginning of the last chapter for more information.","title":"Cousins in the FHKB"},{"location":"tutorial/fhkb/#81-introducing-cousins","text":"Cousins can be confusing, but here is a brief summary: First cousins share a grandparent, but are not siblings; Second cousins share a great grandparent, but are not first cousins or siblings; Degrees such as first and second cousin give the distance to the nearest common ancestor; Removes give differences in generation. So, my Dad\u2019s first cousins (his generation) are my (Robert David Bright\u2019s) first cousins once removed. Simply, my first cousins are my parent\u2019s sibling\u2019s children. As usual, we can think about the objects and put in place some sub-property chains.","title":"8.1 Introducing Cousins"},{"location":"tutorial/fhkb/#82-first-cousins","text":"Figure 8.1: Tracing out the sub-property chain for cousins going from a child to a parent, to its sibling, and down to its child, a cousin Figure 8.1 shows the sub-property chain for first cousins. As usual, think at the object level; to get to the first cousins of Robert David Bright, we go to the parents of Robert David Bright, to their siblings and then to their children. We go up, along and down. The OWL for this could be: ObjectProperty: hasFirstCousin SubPropertyOf: hasCousin SubPropertyChain: hasParent o hasSibling o hasChild Characteristics: Symmetric Note that we follow the definitions in Section 8.1 of first cousins sharing a grandparent, but not a parent. The sub-property chain goes up to children of a grandparent (a given person\u2019s parents), along to siblings and down to their children. We do not want this property to be transitive. One\u2019s cousins are not necessarily my cousins. The blood uncles of Robert David Bright have children that are his cousins. These first cousins, however, also have a mother that is not a blood relation of Robert David Bright and the mother\u2019s sibling\u2019s children are not cousins of Robert David Bright. We do, however, want the property to be symmetric. One\u2019s cousins have one\u2019s-self as a cousin. We need to place the cousin properties in the growing object property hierarchy. Cousins are obviously blood relations, but not ancestors, so they go off to one side, underneath hasBloodrelation . We should group the different removes and degree of cousin underneath one hasCousin property and this we will do. Do the following: Task 33: First cousins Add the property of hasCousin to the hierarchy underneath hasBloodrelation ; Add hasFirstCousin underneath this property; Add the sub-property chain as described above; Run the reasoner and look at the first cousins of Robert David Bright. You should see the following people as first cousins of Robert David Bright: Mark Anthony Heath, Nicholas Charles Heath, Mark Bright, Ian Bright, Janet Bright, William Bright, James Bright, Julie Bright, Clare Bright, Richard John Bright and Robert David Bright. The last two, as should be expected, are first cousins of Robert David Bright and this is not correct. As David Bright will be his own brother, his children are his own nieces and nephews and thus the cousins of his own children. Our inability to infer siblings correctly in the FHKB haunts us still and will continue to do so. Although the last query for the cousins of Robert David Bright should return the same results for every reasoner, we have had experiences where the results differ.","title":"8.2 First Cousins"},{"location":"tutorial/fhkb/#83-other-degrees-and-removes-of-cousin","text":"Other degrees of cousins follow the same pattern as for first cousins; we go up, along and down. For second cousins we go up from a given individual to children of a great grandparent, along to their siblings and down to their grandchildren. The following object property declaration is for second cousins (note it uses the isGrandparentOf and its inverse properties, though the parent properties could be used) : ObjectProperty: hasSecondCousin SubPropertyOf: hasCousin SubPropertyChain: hasGrandParent o hasSibling o isGrandParentOf Characteristics: Symmetric \u2018 Removes \u2019 simply add in another \u2018leg\u2019 of either \u2018up\u2019 or \u2018down\u2019 either side of the \u2018along\u2019\u2014that is, think of the actual individuals involved and draw a little picture of blobs and lines\u2014then trace your finger up, along and down to work out the sub-property chain. The following object property declaration does it for first cousins once removed (note that this has been done by putting this extra \u2018leg\u2019 on to the hasFirstCousin property; the symmetry of the property makes it work either way around so that a given person is the first cousin once removed of his/her first cousins once removed): ObjectProperty: hasFirstCousinOnceRemoved SubPropertyOf: hasCousin SubPropertyChain: hasFirstCousin o hasChild Characteristics: Symmetric To exercise the cousin properties do the following: Task 34: Cousin properties Add properties for second degree cousins; Add removes for first and second degree cousins; Run the reasoner and check what we know about Robert David Bright\u2019 other types of cousin. You should see that we see some peculiar inferences about Robert David Bright\u2019 cousins \u2013 not only are his brother and himself his own cousins, but so are his father, mother, uncles and so on. This makes sense if we look at the general sibling problem, but also it helps to just trace the paths around. If we go up from one of Robert David Bright\u2019 true first cousins to a grandparent and down one parent relationship, we follow the first cousin once removed path and get to one of Robert David Bright\u2019 parents or uncles. This is not to be expected and we need a tighter definition that goes beyond sub-property chains so that we can exclude some implications from the FHKB.","title":"8.3 Other Degrees and Removes of Cousin"},{"location":"tutorial/fhkb/#84-doing-first-cousins-properly","text":"As far as inferring first cousin facts for Robert David Bright, we have failed. More precisely, we have recalled all Robert David Bright\u2019s cousins, but the precision is not what we would desire. What we can do is ask for Robert David Bright\u2019 cousins, but then remove the children of Robert David Bright\u2019 parents. The following DL query achieves this: Person that hasFirstCousin valueRobert_David_Bright_1965 and (not (hasFather valueDavid_Bright_1934) or not (hasMother valueMar- garet_Grace_Rever_1934) This works, but only for a named individual. We could make a defined class for this query; we could also make a defined class FirstCousin , but it is not of much utility. We would have to make sure that people whose parents are not known to have siblings with children are excluded. That is, people are not \u2018first cousins\u2019 whose only first cousins are themselves and their siblings. The following class does this: Class: FirstCousin EquivalentTo: Person that hasFirstCousin some Person Task 35: Roberts first cousins Make a defined class FirstCousin as shown above; Make a defined class FirstCousinOfRobert ; Create a DL query that looks at Robert_David_Bright_1965 first cousins and takes away the children of Robert_David_Bright_1965 \u2019 parents as shown above. This gives some practice with negation. One is making a class and then \u2018taking\u2019 some of it away \u2013 \u2018these, but not those\u2019.","title":"8.4 Doing First Cousins Properly"},{"location":"tutorial/fhkb/#85-summary","text":"We have now expanded the FHKB to include most blood relationships. We have also found that cousins are hard to capture just using object properties and sub-property chains. Our broken sibling inferences mean that we have too many cousins inferred at the instance level. We can get cousins right at the class level by using our inference based cousins, then excluding some using negation. Perhaps not neat, but it works. We have reinforced that we can just add more and more relationships to individuals by just adding more properties to our FHKB object property hierarchy and adding more sub-property chains that use the object properties we have built up upon parentage and sibling properties; this is as it should be. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet 2.2.0 111.395 sec (0.90085 % of final) and by FaCT++ 1.6.4 is approximately 0.868 sec (0.024 % of final). 0 sec indicates failure or timeout.","title":"8.5 Summary"},{"location":"tutorial/fhkb/#chapter-9","text":"","title":"Chapter 9"},{"location":"tutorial/fhkb/#marriage-in-the-fhkb","text":"In this chapter you will: Model marriages and relationships; Establish object properties for husbands, wives and various in-laws; Re-visit aunts and uncles to do them properly; Use more than one sub-property chain on a given property. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial Much of what is in this chapter is really revision; it is more of the same - making lots of properties and using lots of sub-property chains. However, it is worth it as it will test your growing skills and it also makes the reasoners and yourself work hard. There are also some good questions to ask of the FHKB as a result of adding marriages.","title":"Marriage in the FHKB"},{"location":"tutorial/fhkb/#91-marriage","text":"Marriage is a culturally complex situation to model. The FHKB started with a conservative model of a marriage involving only one man and one woman. 10 Later versions are more permissive; a marriage simply has a minimum of two partners. This leaves it open to numbers and sex of the people involved. In fact, \u2018marriage\u2019 is probably not the right name for it. Using BreedingRelationship as a label (the one favoured by the main author\u2019s mother) may be a little too stark and might be a little exclusive.... In any case, some more generic name is probably better and various subclasses of the FHKB\u2019s Marriage class are probably necessary. 10 There being no funny stuff in the Stevens family. To model marriage do the following: Task 36: Marriage Create a class Marriage , subclass of DomainEntity ; Create the properties: hasPartner (domain Marriage and range Person ) and isPartnerIn hasFemalePartner (domain Marriage and range Woman , sub-property of hasPartner ) and its inverse isFemalePartnerIn ; a sub-property of hasPartner has MalePartner (domain Marriage and range Man )and its inverse isMalePartnerIn ; Create the data property hasMarriageYear , making us a sub-property of hasEventYear ,make it functional; Create an individual m001 with the label Marriage of David and Margaret and add the facts: hasMalePartner David_Bright_1934 ; hasFemalePartner Margaret_Grace_Rever_1934 hasMarriageYear 1958 ; Create an individual m002 with the label Marriage of John and Joyce and add the facts: hasMalePartner John_Bright_1930 ; hasFemalePartner Joyce_Gosport (you may have to add Joyce if you did not already did that); hasMarriageYear 1955 ; Create an individual m003 with the label Marriage of Peter and Diana and add the facts: hasMalePartner Peter_William_Bright_1941 ; hasFemalePartner Diana_Pool (you may have to add Diana if you did not already did that); hasMarriageYear 1964 ; We have the basic infrastructure for marriages. We can ask the usual kinds of questions; try the following: Task 37: DL queries Ask the following DL queries: The Women partners in marriages; Marriages that happened before 1960 (see example below); Marriages that happened after 1960; Marriages that involved a man with the family name \u2018Bright\u2019. DL query: Marriage and hasMarriageYear some int[<= 1960]","title":"9.1 Marriage"},{"location":"tutorial/fhkb/#911-spouses","text":"This marriage infrastructure can be used to infer some slightly more interesting things for actual people. While we want marriage objects so that we can talk about marriage years and even locations, should we want to, we also want to be able to have the straight-forward spouse relationships one would expect. We can use sub-property chains in the usual manner; do the following: Task 38: Wifes and Husbands Create a property hasSpouse with two sub-properties hasHusband and hasWife . Create the inverses isSpouseOf , isWifeOf and isHusbandOf . To the hasWife property, add the sub-property chain isMalePartnerIn o hasFemalePartner . Follow the same pattern for the hasHusband property. Figure 9.1 shows what is happening with the sub-property chains. Note that the domains and ranges of the spouse properties come from the elements of the sub-property chains. Note also that the hasSpouse relationship will be implied from its sub-property chains. The following questions can now be asked: Is wife of David Bright; Has a husband born before 1940; The wife of an uncle of William Bright 1970. Figure 9.1: The sub-property chain path used to infer the spouse relationships via the marriage partnerships. and many more. This is really a chance to explore your querying abilities and make some complex nested queries that involve going up and down the hierarchy and tracing routes through the graph of relationships between the individuals you\u2019ve inferred.","title":"9.1.1 Spouses"},{"location":"tutorial/fhkb/#92-in-laws","text":"Now we have spouses, we can also have in-laws. The path is simple: isSpouseOf o hasMother implies hasMotherInLaw . The path involved in mother-in-laws can be seen in Figure 9.2. The following OWL code establishes the sub-property chains for hasMotherInLaw : ObjectProperty: hasMotherInLaw SubPropertyOf: hasParentInLaw SubPropertyChain: isSpouseOf o hasMother Domain: Person Range: Woman InverseOf: isMotherInLawOf Figure 9.2: Tracing out the path between objects to make the sub-property chain for mother-in-laws Do the following to make the parent in-law properties: Task 39: Parents in-law Create hasParentInLaw with two sub-properties of hasMotherInLaw and hasFatherInLaw ; Create the inverses, but remember to let the reasoner infer the hierarchy on that side of the hierarchy; Add the sub-property chains as described in the pattern for hasMotherInLaw above; Run the reasoner and check that the mother-in-law of Margaret Grace Rever is Iris Ellen Archer.","title":"9.2 In-Laws"},{"location":"tutorial/fhkb/#93-brothers-and-sisters-in-law","text":"Brothers and sisters in law have the interesting addition of having more than one path between objects to establish a sister or brother in law relationship. The OWL code below establishes the relationships for \u2018is sister in law of\u2019: ObjectProperty: hasSisterInLaw SubPropertyOf: hasSiblingInLaw SubPropertyChain: hasSpouse o hasSister SubPropertyChain: hasSibling o isWifeOf A wife\u2019s husband\u2019s sister is a sister in law of the wife. Figure 9.3 shows the two routes to being a sister-in-law. In addition, the wife is a sister in law of the husband\u2019s siblings. One can add as many sub-property chains to a property as one needs. You should add the properties for hasSiblingInLawOf and its obvious sub-properties following the inverse of the pattern above. Task 40: Siblings in-law Create the relationships for siblings-in-law as indicated in the owl code above. By now, chances are high that the realisation takes a long time. We recommend to remove the very computationally expensive restriction `hasParent` exactly 2 Person on the `Person` class, if you have not done it so far. Figure 9.3: The two routes to being a sister-in-law.","title":"9.3 Brothers and Sisters In-Law"},{"location":"tutorial/fhkb/#94-aunts-and-uncles-in-law","text":"The uncle of Robert David Bright has a wife, but she is not the aunt of Robert David Bright, she is the aunt-in-law. This is another kith relationship, not a kin relationship. The pattern has a familiar feel: ObjectProperty: isAuntInLawOf SubPropertyOf: isInLawOf SubPropertyChain: isWifeOf o isBrotherOf o isParentOf Task 41: Uncles and aunts in-law Create hasAuntInLaw and hasUncleInLaw in the usual way; Test in the usual way; Tidy up the top of the property hierarchy so that it looks like Figure 9.4. We have a top property of hasRelation and two sub-properties of isBloodRelationOf and isInLawOf to establish the kith and kin relationships respectively; All the properties created in this chapter (except for spouses) should be underneath isInLawOf . Figure 9.4: The object property hierarchy after adding the various in-law properties.","title":"9.4 Aunts and Uncles in-Law"},{"location":"tutorial/fhkb/#95-summary","text":"This has really been a revision chapter; nothing new has really been introduced. We have added a lot of new object properties and one new data property. The latest object property hierarchy with the \u2018in-law\u2019 branch can be seen in Figure 9.4. Highlights have been: Having an explicit marriage object so that we can say things about the marriage itself, not just the people in the marriage; We have seen that more than one property chain can be added to a property; We have added a lot of kith relationships to join the kin or blood relationships; As usual, the reasoner can establish the hierarchy for the inverses and put a lot of the domain and ranges in for free. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet 2.2.0 123.655 sec (1.00000 % of final) and by FaCT++ 1.6.4 is approximately 1.618 sec (0.046 % of final). 0 sec indicates failure or timeout.","title":"9.5 Summary"},{"location":"tutorial/fhkb/#chapter-10","text":"","title":"Chapter 10"},{"location":"tutorial/fhkb/#extending-the-tbox","text":"In this chapter you will: Just add lots of defined classes for all the aspects we have covered in this FHKB tutorial; You will learn that the properties used in these defined classes must be chosen with care. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial","title":"Extending the TBox"},{"location":"tutorial/fhkb/#101-adding-defined-classes","text":"Add the following defined classes: Task 42: Adding defined classes Relation and blood relation; Forefather and Foremother; Grandparent, Grandfather and Grandmother; GreatGrandparent, GreatGrandfather and GreatGrandmother; GreatGrandparentOfRobert, GreatGrandfatherOfRobert and GreatGrandMotherOfRobert Daughter, Son, Brother, Sister, Child; Aunt, Uncle, AuntInLaw, UncleInLaw, GreatAunt and GreatUncle; FirstCousin and SecondCousin; First cousin once removed; InLaw, MotherInLaw, FatherInLaw, ParentInLaw, SiblingInLaw, SisterInLaw, BrotherInLaw; Any defined class for any property in the hierarchy and any nominal variant of these classes. The three classes of Child , Son and Daughter are of note. They are coded in the following way: Class: Child EquivalentTo: Person that hasParent Some Person Class: Son EquivalentTo: Man that hasParent Some Person Class: Daughter EquivalentTo: Woman that hasParent Some Person After running the reasoner, you will find that Person is found to be equivalent to Child ; Daughter is equivalent to Woman and that Son is equivalent to Man . This does, of course, make sense \u2013 each and every person is someone\u2019s child, each and every woman is someone\u2019s daughter. We will forget evolutionary time-scales where this might be thought to break down at some point \u2013 all Person individuals are also Descendant individuals, but do we expect some molecule in some prebiotic soup to be a member of this class? Nevertheless, within the scope of the FHKB, such inferred equivalences are not unreasonable. They are also instructive; it is possible to have different intentional descriptions of a class and for them to have the same logical extents. You can see another example of this happening in the amino acids ontology, but for different reasons. Taking Grandparent as an example class, there are two ways of writing the defined class: Class: Grandparent EquivalentTo: Person and isGrandparentOf some Person Class: Grandparent EquivalentTo: Person and (isParentOf some (Person and (is- ParentOf some Person)) Each comes out at a different place in the class hierarchy. They both capture the right individuals as members (that is, those individuals in the ABox that are holding a isGrandparentOf property), but the class hierarchy is not correct. By definition, all grandparents are also parents, but the way the object property hierarchy works means that the first way of writing the defined class (with the isGrandparentOf property) is not subsumed by the class Parent . We want this to happen in any sensible class hierarchy, so we have to use the second pattern for all the classes, spelling out the sub-property path that implies the property such as isGrandparentOf within the equivalence axiom. The reason for this need for the \u2018long-form\u2019 is that the isGrandparentOf does not imply the isParentOf property. As described in Chapter 3 if this implication were the case, being a grandparent of Robert David Bright, for instance, would also imply that the same Person were a parent of Robert David Bright; an implication we do not want. As these two properties ( isParentOf and isGrandparentOf ) do not subsume each other means that the defined classes written according to pattern one above will not subsume each other in the class hierarchy. Thus we use the second pattern. If we look at the class for grandparents of Robert: Class: GrandparentOfRobert EquivalentTo: Person that isParentOf some (Person that isParentOf value Robert David Bright) If we make the equivalent class for Richard John Bright, apply the reasoner and look at the hierarchy, we see that the two classes are not logically equivalent, even though they have the same extents of William George Bright, Iris Ellen Archer, Charles Herbert Rever and Violet Sylvia Steward. We looked at this example in Section 6.2, where there is an explanation and solutions.","title":"10.1 Adding Defined Classes"},{"location":"tutorial/fhkb/#102-summary","text":"We can add defined classes based on each property we have put into the object property hierarchy. We see the expected hierarchy; as can be seen from Figure 10.1 it has an obvious symmetry based on sex. We also see a lot of equivalences inferred \u2013 all women are daughters, as well as women descendants. Perhaps not the greatest insight ever gained, but it at least makes sense; all women must be daughters. It is instructive to use the explanation feature in Prot\u00e9g\u00e9 to look at why the reasoner has made these inferences. For example, take a look at the class hasGrandmother some Woman \u2013 it is instructive to see how many there are. Like the Chapter on marriage and in-law (Chapter 9), this chapter has largely been revision. One thing of note is, however, that we must not use the object properties that are inferred through sub-property chains as definitions in the TBox; we must spell out the sub-property chain in the definition, otherwise the implications do not work properly. One thing is almost certain; the resulting TBox is rather complex and would be almost impossible to maintain by hand. Figure 10.1: The full TBox hierarchy of the FHKB The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). ``` The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet 2.2.0 0.000 sec (0.00000 % of final) and by FaCT++ 1.6.4 is approximately 35.438 sec (1.000 % of final). 0 sec indicates failure or timeout. ````","title":"10.2 Summary"},{"location":"tutorial/fhkb/#chapter-11","text":"","title":"Chapter 11"},{"location":"tutorial/fhkb/#final-remarks","text":"If you have done all the tasks within this tutorial, then you will have touched most parts of OWL 2. Unusually for most uses of OWL we have concentrated on individuals, rather than just on the TBox. One note of warning \u2013 the full FHKB has some 450 members of the Bright family and takes a reasonably long time to classify, even on a sensible machine. The FHKB is not scalable in its current form. One reason for this is that we have deliberately maximised inference. We have attempted not to explicitly type the individuals, but drive that through domain and range constraints. We are making the property hierarchy do lots of work. For the individual Robert David Bright, we only have a couple of assertions, but we infer some 1 500 facts between Robert David Bright and other named individuals in the FHKB\u2013displaying this in Prot\u00e9g\u00e9 causes problems. We have various complex classes in the TBox and so on. We probably do not wish to drive a genealogical application using an FHKB in this form. Its purpose is educational. It touches most of OWL 2 and shows a lot of what it can do, but also a considerable amount of what it cannot do. As inference is maximised, the FHKB breaks most of the OWL 2 reasoners at the time of writing.However, it serves its role to teach about OWL 2. OWL 2 on its own and using it in this style, really does not work for family history. We have seen that siblings and cousins cause problems. rules in various forms can do this kind of thing easily\u2014it is one of the primary examples for learning about Prolog. Nevertheless, the FHKB does show how much inference between named individuals can be driven from a few fact assertions and a property hierarchy. Assuming a powerful enough reasoner and the ability to deal with many individuals, it would be possible to make a family history application using the FHKB; as long as one hid the long and sometimes complex queries and manipulations that would be necessary to \u2018prune\u2019 some of the \u2018extra\u2019 facts found about individuals. However, the FHKB does usefully show the power of OWL 2, touch a great deal of the language and demonstrate some of its limitations.","title":"Final remarks"},{"location":"tutorial/fhkb/#appendix-a","text":"","title":"Appendix A"},{"location":"tutorial/fhkb/#fhkb-family-data","text":"Table A.1: The list of individuals in the FHKB Person First given name Second given name Family name Birth year Mother Father Alec John Archer 1927 Alec John Archer 1927 Violet Heath 1887 James Alexander Archer 1882 Charles Herbert Rever 1895 Charles Herbert Rever 1895 Elizabeth Frances Jessop 1869 William Rever 1870 Charlotte Caroline Jane Bright 1894 Charlotte Caroline Jane Bright 1894 Charlotte Hewett 1863 Henry Edmund Bright 1862 Charlotte Hewett 1863 Charlotte none Hewett 1863 not specified not specified Clare Bright 1966 Clare none Bright 1966 Diana Pool Peter William Bright 1941 Diana Pool Diana none Pool none not specified not specified David Bright 1934 David none Bright 1934 Iris Ellen Archer 1906 William George Bright 1901 Dereck Heath Dereck none Heath 1927 not specified not specified Eileen Mary Rever 1929 Eileen Mary Rever 1929 Violet Sylvia Steward 1894 Charles Herbert Rever 1895 Elizabeth Frances Jessop 1869 Elizabeth Frances Jessop 1869 not specified not specified Ethel Archer 1912 Ethel none Archer 1912 Violet Heath 1887 James Alexander Archer 1882 Frederick Herbert Bright 1889 Frederick Herbert Bright 1889 Charlotte Hewett 1863 Henry Edmund Bright 1862 Henry Edmund Bright 1862 Henry Edmund Bright 1862 not specified not specified Henry Edmund Bright 1887 Henry Edmund Bright 1887 Charlotte Hewett 1863 Henry Edmund Bright 1862 Ian Bright 1959 Ian none Bright 1959 Joyce Gosport John Bright 1930 Iris Ellen Archer 1906 Iris Ellen Archer 1906 Violet Heath 1887 James Alexander Archer 1882 James Alexander Archer 1882 James Alexander Archer 1882 not specified not specified James Bright 1964 James none Bright 1964 Diana Pool Peter William Bright 1941 James Frank Hayden Bright 1891 James Frank Bright 1891 Charlotte Hewett 1863 Henry Edmund Bright 1862 Janet Bright 1964 Janet none Bright 1964 Joyce Gosport John Bright 1930 John Bright 1930 John none Bright 1930 Iris Ellen Archer 1906 William George Bright 1901 John Tacey Steward 1873 John Tacey Steward 1873 not specified not specified Joyce Archer 1921 Joyce none Archer 1921 Violet Heath 1887 James Alexander Archer 1882 Joyce Gosport Joyce none Gosport not specified not specified not specified Julie Bright 1966 Julie none Bright 1966 Diana Pool Peter William Bright 1941 Kathleen Minnie Bright 1904 Kathleen Minnie Bright 1904 Charlotte Hewett 1863 Henry Edmund Bright 1862 Leonard John Bright 1890 Leonard John Bright 1890 Charlotte Hewett 1863 Henry Edmund Bright 1862 Lois Green 1871 Lois none Green 1871 not specified not specified Margaret Grace Rever 1934 Margaret Grace Rever 1934 Violet Sylvia Steward 1894 Charles Herbert Rever 1895 Mark Anthony Heath 1960 Mark Anthony Heath 1960 Eileen Mary Rever 1929 Dereck Heath Mark Bright 1956 Mark none Bright 1956 Joyce Gosport John Bright 1930 Nicholas Charles Heath 1964 Nicholas Charles Heath 1964 Eileen Mary Rever 1929 Dereck Heath Nora Ada Bright 1899 Nora Ada Bright 1899 Charlotte Hewett 1863 Henry Edmund Bright 1862 Norman James Archer 1909 Norman James Archer 1909 Violet Heath 1887 James Alexander Archer 1882 Peter William Bright 1941 Peter William Bright 1941 Iris Ellen Archer 1906 William George Bright 1901 Richard John Bright 1962 Richard John Bright 1962 Margaret Grace Rever 1934 David Bright 1934 Robert David Bright 1965 Robert David Bright 1965 Margaret Grace Rever 1934 David Bright 1934 Violet Heath 1887 Violet none Heath 1887 not specified not specified Violet Sylvia Steward 1894 Violet Sylvia Steward 1894 Lois Green 1871 John Tacey Steward 1873 William Bright 1970 William none Bright 1970 Joyce Gosport John Bright 1930 William George Bright 1901 William George Bright 1901 Charlotte Hewett 1863 Henry Edmund Bright 1862 William Rever 1870 William none Rever 1870 not specified not specified","title":"FHKB Family Data"},{"location":"tutorial/fhkb/#bibliography","text":"[1] M. Horridge and S. Bechhofer. The owl api: a java api for working with owl 2 ontologies. Proc. of OWL Experiences and Directions , 2009, 2009. [2] Luigi Iannone, Alan Rector, and Robert Stevens. Embedding knowledge patterns into owl. In European Semantic Web Conference (ESWC09) , pages 218\u2013232, 2009. [3] Dmitry Tsarkov, Uli Sattler, Margaret Stevens, and Robert Stevens. A Solution for the Man-Man Problem in the Family History Knowledge Base. In Sixth International Workshop on OWL: Experiences and Directions 2009 , 2009.","title":"Bibliography"},{"location":"tutorial/github_fundamentals/","text":"Introduction to GitHub \u00b6 Back to Getting Started \u00b6 Back to Main Repo \u00b6 Overview: \u00b6 Getting started Organization Markdown Content types Getting started \u00b6 GitHub is increasingly used by software developers, programmers and project managers for uploading and sharing content, as well as basic project management. You build a profile, upload projects to share and connect with other users by \"following\" their accounts. Many users store programs and code projects, but you can also upload text documents or other file types in your project folders to share publicly (or privately). It is capable of storing any file type from text, to structured data, to software. And more features are being added by the day. The real power of Git, however, is less about individuals publishing content (many places can do that, including google docs etc). It is more about that content being easily shared, built upon, and credited in a way that is robust to the realities of distributed collaboration. You don't have to know how to code or use the command line. It is a powerful way to organize projects with multiple participants. Organization \u00b6 Git supports the following types of primary entities: Individual: A person who contributes to GitHub (that's you!) Example individual http://github.com/nicolevasilevsky Organization: An entity that may correspond to an actual organization (such as a university) or to a meaningful grouping of repositories. Organizations are like individuals except that they can establish teams. Example organization: https://github.com/monarch-initiative Repository: A collection of versioned files (of any type) Example repository https://github.com/monarch-initiative/mondo/ Teams : A group of individuals assembled by the administrators of an organization. An individual may participate in many teams and organizations, however a team is always bound to a single organization. Nesting teams saves time; instructions here . The relationships between any combination of these entities is many-to-many, with the nuanced exception of repositories. For our purposes today we will oversimplify by saying that a repositoy belongs either to a single organization or to a single individual . Markdown \u00b6 Content in GitHub is written using Markdown, a text-to-HTML conversion tool for web writers (ref) . For more help with Markdown, see this GitHub guide . Raw markup syntax As rendered Header - use # for H1, ## for H2, etc. # Header, ## Header (note, the header is not displaying properly in this table) Emphasis, aka italics, with *asterisks* or _underscores_. Emphasis, aka italics, with asterisks or underscores . Strong emphasis, aka bold, with **asterisks** or __underscores__. Strong emphasis, aka bold, with asterisks or underscores . Combined emphasis with **asterisks and _underscores_**. Combined emphasis with asterisks and underscores . Strikethrough uses two tildes. ~~Scratch this.~~ Strikethrough uses two tildes. ~~Scratch this.~~ Lists: To introduce line breaks in markdown, add two spaces For a bulleted list, use * or - (followed by a space) Here is an example of a list: One Two Three Here is an example of a bulleted list: - One - Two - Three Content types \u00b6 GitHub can store any kind of content, provided it isn't too big. (And now even this is possible ). However, it is more capable for some filetypes than it is for others. Certain filetypes can be viewed 'natively' within the GitHub interface. These are: Images: png, jpg, svg GEOJSON CSV, TSV (note that files named type '.tab' will not render properly in the UI.) Markdown Software code (eg. including json, HTML, xml etc) Task - create a new GitHub repository \u00b6 Create your GitHub account if you do not already have one Customize your avatar if you haven't already Go to settings and upload any picture (it doesn't have to be your face) Create a repository Task - update the content in your README \u00b6 Go back to the repository you just created Click the pencil icon in the right corner of your README.md file Add some content to your file that includes a header, italics, bold, strikethrough, and lists You can preview your changes before committing by clicking 'Preview changes'. Commit your changes by clicking the commit button at the bottom of the page. Task - add content to your repository \u00b6 Click on the code button Click upload file Upload a file by dragging and dropping or browse for file Trying uploading an Excel file vs a TSV or CSV file. How are these displayed differently? Additional Resources \u00b6 Frequently Asked Questions Git and GitHub for Documentation Markdown Cheatsheet Git 101: Git and GitHub for Beginners Mastering Issues (10 min read) Acknowledgements \u00b6 Adopted from CD2H MTIP tutorial","title":"GitHub Fundamentals for OBO Engineers"},{"location":"tutorial/github_fundamentals/#introduction-to-github","text":"","title":"Introduction to GitHub"},{"location":"tutorial/github_fundamentals/#back-to-getting-started","text":"","title":"Back to Getting Started"},{"location":"tutorial/github_fundamentals/#back-to-main-repo","text":"","title":"Back to Main Repo"},{"location":"tutorial/github_fundamentals/#overview","text":"Getting started Organization Markdown Content types","title":"Overview:"},{"location":"tutorial/github_fundamentals/#getting-started","text":"GitHub is increasingly used by software developers, programmers and project managers for uploading and sharing content, as well as basic project management. You build a profile, upload projects to share and connect with other users by \"following\" their accounts. Many users store programs and code projects, but you can also upload text documents or other file types in your project folders to share publicly (or privately). It is capable of storing any file type from text, to structured data, to software. And more features are being added by the day. The real power of Git, however, is less about individuals publishing content (many places can do that, including google docs etc). It is more about that content being easily shared, built upon, and credited in a way that is robust to the realities of distributed collaboration. You don't have to know how to code or use the command line. It is a powerful way to organize projects with multiple participants.","title":"Getting started"},{"location":"tutorial/github_fundamentals/#organization","text":"Git supports the following types of primary entities: Individual: A person who contributes to GitHub (that's you!) Example individual http://github.com/nicolevasilevsky Organization: An entity that may correspond to an actual organization (such as a university) or to a meaningful grouping of repositories. Organizations are like individuals except that they can establish teams. Example organization: https://github.com/monarch-initiative Repository: A collection of versioned files (of any type) Example repository https://github.com/monarch-initiative/mondo/ Teams : A group of individuals assembled by the administrators of an organization. An individual may participate in many teams and organizations, however a team is always bound to a single organization. Nesting teams saves time; instructions here . The relationships between any combination of these entities is many-to-many, with the nuanced exception of repositories. For our purposes today we will oversimplify by saying that a repositoy belongs either to a single organization or to a single individual .","title":"Organization"},{"location":"tutorial/github_fundamentals/#markdown","text":"Content in GitHub is written using Markdown, a text-to-HTML conversion tool for web writers (ref) . For more help with Markdown, see this GitHub guide . Raw markup syntax As rendered Header - use # for H1, ## for H2, etc. # Header, ## Header (note, the header is not displaying properly in this table) Emphasis, aka italics, with *asterisks* or _underscores_. Emphasis, aka italics, with asterisks or underscores . Strong emphasis, aka bold, with **asterisks** or __underscores__. Strong emphasis, aka bold, with asterisks or underscores . Combined emphasis with **asterisks and _underscores_**. Combined emphasis with asterisks and underscores . Strikethrough uses two tildes. ~~Scratch this.~~ Strikethrough uses two tildes. ~~Scratch this.~~ Lists: To introduce line breaks in markdown, add two spaces For a bulleted list, use * or - (followed by a space) Here is an example of a list: One Two Three Here is an example of a bulleted list: - One - Two - Three","title":"Markdown"},{"location":"tutorial/github_fundamentals/#content-types","text":"GitHub can store any kind of content, provided it isn't too big. (And now even this is possible ). However, it is more capable for some filetypes than it is for others. Certain filetypes can be viewed 'natively' within the GitHub interface. These are: Images: png, jpg, svg GEOJSON CSV, TSV (note that files named type '.tab' will not render properly in the UI.) Markdown Software code (eg. including json, HTML, xml etc)","title":"Content types"},{"location":"tutorial/github_fundamentals/#task-create-a-new-github-repository","text":"Create your GitHub account if you do not already have one Customize your avatar if you haven't already Go to settings and upload any picture (it doesn't have to be your face) Create a repository","title":"Task - create a new GitHub repository"},{"location":"tutorial/github_fundamentals/#task-update-the-content-in-your-readme","text":"Go back to the repository you just created Click the pencil icon in the right corner of your README.md file Add some content to your file that includes a header, italics, bold, strikethrough, and lists You can preview your changes before committing by clicking 'Preview changes'. Commit your changes by clicking the commit button at the bottom of the page.","title":"Task - update the content in your README"},{"location":"tutorial/github_fundamentals/#task-add-content-to-your-repository","text":"Click on the code button Click upload file Upload a file by dragging and dropping or browse for file Trying uploading an Excel file vs a TSV or CSV file. How are these displayed differently?","title":"Task - add content to your repository"},{"location":"tutorial/github_fundamentals/#additional-resources","text":"Frequently Asked Questions Git and GitHub for Documentation Markdown Cheatsheet Git 101: Git and GitHub for Beginners Mastering Issues (10 min read)","title":"Additional Resources"},{"location":"tutorial/github_fundamentals/#acknowledgements","text":"Adopted from CD2H MTIP tutorial","title":"Acknowledgements"},{"location":"tutorial/github_issues/","text":"Intro to managing and tracking issues in GitHub \u00b6 Overview \u00b6 How to create issues How to assign issues How to communicate about issues How to organize issues How to query issues How to close issues How to assign teams Where to go when you need help Miscellany that is good to know Create issues \u00b6 Back to top Why: \"Issues are a great way to keep track of tasks , enhancements , and bugs for your projects or for anyone else's. As long as you are a registered GitHub user you can log an issue, or comment on an issue for any open repo on GitHub. Issues are a bit like email\u2014except they can be shared, intelligently organized, and discussed with the rest of your team. GitHub\u2019s tracker is called Issues, and has its own section in every repository.\" (From: https://guides.github.com/features/issues/) How: How to create an issue in GitHub: We will practice creating tickets in this repository https://github.com/nicolevasilevsky/c-path-practice Click \"issues\" Click \"New Issue\" (note the word 'issue' and 'ticket' are frequently used interchangeably) Write an informative title Write a detailed explanation of your issue In the case of reporting software bugs, provide some context in which the issue was encountered (e.g. bug detected when using Google Chrome on a Mac OS) If you know the sub-tasks that are involved, list these using - [ ] markdown syntax before each bullet. Note, you can also add sub-tasks by clicking the 'add a task list' button in the tool bar. The status of the tasks in an issue (eg. https://github.com/nicolevasilevsky/c-path-practice/issues/1 will then be reflected in any summary view. Eg. https://github.com/nicolevasilevsky/c-path-practice/issues . Click Submit new issue Edit the issue (if needed) (Note that post-hoc edits will not propagate to email notifications). Your turn: Follow the instructions above to create a ticket about a hypothetical issue (such as an improvement to this tutorial) that includes a sub-task list. Assign issues \u00b6 Back to top Assign issues to people On the top right hand side, click \"Assignees\" You can assign issues to yourself or other people who are part of the repository In the box, start typing type their name or GitHub handle It is possible to assign up to 10 handles at once (assignment to a team is currently not supported) Add labels On the top right hand side, click \"Labels\" Assign a relevant label to your ticket Note, by default, every GitHub repo comes with some standard labels You can also create new labels that are specific to your project. For example see the labels on the Mondo GitHub tracker New Labels In GitHub, navigate to the Issues page or the pull requests tab Click Labels button next to the search field Click New Label to create a new label, or click Edit to edit an existing one. In the text box, type your new label name. Select a color for the label from the color bar. You can customize this color by editing the hexadecimal number above the color bar. For a list of hexadecimal numbers see HTML color codes Click Create Label to save the new label. Your turn: On the ticket you previously created: Assign the ticket to someone Add a label for an enhancement Create a new label and add it to the ticket Communicate about issues \u00b6 Back to top Comment on issues Click on an issue in the issue tracker in the https://github.com/nicolevasilevsky/c-path-practice/issues repo Scroll to the bottom of the issue, and add content in the \"Leave a comment\" field Use the top tool bar to format your text, add bold , italic , lists etc. Preview your text to see how your formatting looks Click Comment. Close issues - If an issue has been addressed, click Close Issue. Best practice is to point to the work (whether code, documentation, etc) that has been done to close it. - Only close the ticket if the issue has been resolved, usually someone will write a comment describing the action they did to close the issue and click Close Issue. - The issue will no longer be dispalyed in the list of open issue, but will be archived. - When making a change to code or documentation in GitHub, it is possible to automatically couple a change to an issue and close it. Just use 'fixes' or 'closes' followed by the issue number. Use direct @ mentions You can mention someone in a issue without assigning it to them In the comments section, type @github handle. For example, to mention Nicole, you would type @nicolevasilevsky. You can either start typing their name or GitHub handle and GitHub will autosuggest their handle. Link documents You can link documents and files by: copy and pasting URL you can attach files by dragging and dropping You can link one issue to another in the same repo by typing '#' followed by the title of the ticket This approach also works across repos but you need to use the full URL (no autocomplete available). Doing this will also cause the referent issue to display that it has been referenced. Cross reference to another ticket If your ticket is a duplicate or related to another ticket, you can cross reference another ticket Type # and you will see a list of other tickets in that repo Type #TicketNumber and that will link to the other ticket. Before saving your changes, you can preview the comment to ensure the correct formatting. Your turn: Follow the instructions above to comment on a ticket that someone created. Mention Nicole Attach a picture (such as a picture you copy from the internet, or attach a picture you have saved on your computer) Include a comment that says, 'related to #1' and link to ticket #1 Organize issues \u00b6 Back to top Milestones To create a milestone, navigate to the issues page in your repository Next to the search field, click Milestones Click New Milestone to create a new milestone, click Edit to edit an existing one Create a milestone that is broad enough to be meaningful, but specific enough to be actionable. Set a due date for the milestone (note that specific tasks can not be formally assigned due dates, though you can mention a desired due date in the narrative text of a ticket. Each ticket can only be associated to ONE milestone, however it can have as many labels as appropriate. A given issue can be part of multiple \"project\" boards (see below) Your turn Create a new milestone, and add the milestone to an existing ticket. Projects Projects is a lot like Trello, it uses cards on a list that you can name and organize as you see fit. You can create as many projects within a repository as you like To create project: Click on Projects Click New Project Name the project Write a description of the project Create columns and give them names Add 'cards' to the columns Your turn Create a new project and add columns and add cards to the columns. Query issues \u00b6 Back to top Once you start using GitHub for lots of things it is easy to get overwhelmed by the number of issues. The query dashboard https://github.com/issues allows you to filter on tickets. - All issues assigned to me: https://github.com/issues/assigned - All issues on which I am @ mentioned: https://github.com/issues/mentioned More complex queries are also possible. All issues either assigned to me OR on which I have commented OR am mentioned: https://github.com/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+involves%3Anicolevasilevsky+ Further reading on Issue querys Nofifications \u00b6 When you join a repository, by default, you are 'watching' all activity. You can change the setting to 'Participating and @mentions' to only get notifications that mention you You can set rules in your email to filter for emails that mention you, ie @username. Help \u00b6 Back to top You may find the answers you seek in StackOverflow , although it is primarily geared towards programmers. GitHub kind of monitors https://github.com/isaacs/github/issues/ but not with any rigor. To be safe, contact GitHub directly at https://github.com/contact , but recognize that they support literally millions of users and responsiveness is not guaranteed. Forums like isaacs sometimes offer some help because other users can help identify workarounds, for instance, as shown here . Acknowledgements \u00b6 Adopted from CD2H MTIP tutorial","title":"GitHub Issue for OBO Engineers"},{"location":"tutorial/github_issues/#intro-to-managing-and-tracking-issues-in-github","text":"","title":"Intro to managing and tracking issues in GitHub"},{"location":"tutorial/github_issues/#overview","text":"How to create issues How to assign issues How to communicate about issues How to organize issues How to query issues How to close issues How to assign teams Where to go when you need help Miscellany that is good to know","title":"Overview"},{"location":"tutorial/github_issues/#create-issues","text":"Back to top Why: \"Issues are a great way to keep track of tasks , enhancements , and bugs for your projects or for anyone else's. As long as you are a registered GitHub user you can log an issue, or comment on an issue for any open repo on GitHub. Issues are a bit like email\u2014except they can be shared, intelligently organized, and discussed with the rest of your team. GitHub\u2019s tracker is called Issues, and has its own section in every repository.\" (From: https://guides.github.com/features/issues/) How: How to create an issue in GitHub: We will practice creating tickets in this repository https://github.com/nicolevasilevsky/c-path-practice Click \"issues\" Click \"New Issue\" (note the word 'issue' and 'ticket' are frequently used interchangeably) Write an informative title Write a detailed explanation of your issue In the case of reporting software bugs, provide some context in which the issue was encountered (e.g. bug detected when using Google Chrome on a Mac OS) If you know the sub-tasks that are involved, list these using - [ ] markdown syntax before each bullet. Note, you can also add sub-tasks by clicking the 'add a task list' button in the tool bar. The status of the tasks in an issue (eg. https://github.com/nicolevasilevsky/c-path-practice/issues/1 will then be reflected in any summary view. Eg. https://github.com/nicolevasilevsky/c-path-practice/issues . Click Submit new issue Edit the issue (if needed) (Note that post-hoc edits will not propagate to email notifications). Your turn: Follow the instructions above to create a ticket about a hypothetical issue (such as an improvement to this tutorial) that includes a sub-task list.","title":"Create issues"},{"location":"tutorial/github_issues/#assign-issues","text":"Back to top Assign issues to people On the top right hand side, click \"Assignees\" You can assign issues to yourself or other people who are part of the repository In the box, start typing type their name or GitHub handle It is possible to assign up to 10 handles at once (assignment to a team is currently not supported) Add labels On the top right hand side, click \"Labels\" Assign a relevant label to your ticket Note, by default, every GitHub repo comes with some standard labels You can also create new labels that are specific to your project. For example see the labels on the Mondo GitHub tracker New Labels In GitHub, navigate to the Issues page or the pull requests tab Click Labels button next to the search field Click New Label to create a new label, or click Edit to edit an existing one. In the text box, type your new label name. Select a color for the label from the color bar. You can customize this color by editing the hexadecimal number above the color bar. For a list of hexadecimal numbers see HTML color codes Click Create Label to save the new label. Your turn: On the ticket you previously created: Assign the ticket to someone Add a label for an enhancement Create a new label and add it to the ticket","title":"Assign issues"},{"location":"tutorial/github_issues/#communicate-about-issues","text":"Back to top Comment on issues Click on an issue in the issue tracker in the https://github.com/nicolevasilevsky/c-path-practice/issues repo Scroll to the bottom of the issue, and add content in the \"Leave a comment\" field Use the top tool bar to format your text, add bold , italic , lists etc. Preview your text to see how your formatting looks Click Comment. Close issues - If an issue has been addressed, click Close Issue. Best practice is to point to the work (whether code, documentation, etc) that has been done to close it. - Only close the ticket if the issue has been resolved, usually someone will write a comment describing the action they did to close the issue and click Close Issue. - The issue will no longer be dispalyed in the list of open issue, but will be archived. - When making a change to code or documentation in GitHub, it is possible to automatically couple a change to an issue and close it. Just use 'fixes' or 'closes' followed by the issue number. Use direct @ mentions You can mention someone in a issue without assigning it to them In the comments section, type @github handle. For example, to mention Nicole, you would type @nicolevasilevsky. You can either start typing their name or GitHub handle and GitHub will autosuggest their handle. Link documents You can link documents and files by: copy and pasting URL you can attach files by dragging and dropping You can link one issue to another in the same repo by typing '#' followed by the title of the ticket This approach also works across repos but you need to use the full URL (no autocomplete available). Doing this will also cause the referent issue to display that it has been referenced. Cross reference to another ticket If your ticket is a duplicate or related to another ticket, you can cross reference another ticket Type # and you will see a list of other tickets in that repo Type #TicketNumber and that will link to the other ticket. Before saving your changes, you can preview the comment to ensure the correct formatting. Your turn: Follow the instructions above to comment on a ticket that someone created. Mention Nicole Attach a picture (such as a picture you copy from the internet, or attach a picture you have saved on your computer) Include a comment that says, 'related to #1' and link to ticket #1","title":"Communicate about issues"},{"location":"tutorial/github_issues/#organize-issues","text":"Back to top Milestones To create a milestone, navigate to the issues page in your repository Next to the search field, click Milestones Click New Milestone to create a new milestone, click Edit to edit an existing one Create a milestone that is broad enough to be meaningful, but specific enough to be actionable. Set a due date for the milestone (note that specific tasks can not be formally assigned due dates, though you can mention a desired due date in the narrative text of a ticket. Each ticket can only be associated to ONE milestone, however it can have as many labels as appropriate. A given issue can be part of multiple \"project\" boards (see below) Your turn Create a new milestone, and add the milestone to an existing ticket. Projects Projects is a lot like Trello, it uses cards on a list that you can name and organize as you see fit. You can create as many projects within a repository as you like To create project: Click on Projects Click New Project Name the project Write a description of the project Create columns and give them names Add 'cards' to the columns Your turn Create a new project and add columns and add cards to the columns.","title":"Organize issues"},{"location":"tutorial/github_issues/#query-issues","text":"Back to top Once you start using GitHub for lots of things it is easy to get overwhelmed by the number of issues. The query dashboard https://github.com/issues allows you to filter on tickets. - All issues assigned to me: https://github.com/issues/assigned - All issues on which I am @ mentioned: https://github.com/issues/mentioned More complex queries are also possible. All issues either assigned to me OR on which I have commented OR am mentioned: https://github.com/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+involves%3Anicolevasilevsky+ Further reading on Issue querys","title":"Query issues"},{"location":"tutorial/github_issues/#nofifications","text":"When you join a repository, by default, you are 'watching' all activity. You can change the setting to 'Participating and @mentions' to only get notifications that mention you You can set rules in your email to filter for emails that mention you, ie @username.","title":"Nofifications"},{"location":"tutorial/github_issues/#help","text":"Back to top You may find the answers you seek in StackOverflow , although it is primarily geared towards programmers. GitHub kind of monitors https://github.com/isaacs/github/issues/ but not with any rigor. To be safe, contact GitHub directly at https://github.com/contact , but recognize that they support literally millions of users and responsiveness is not guaranteed. Forums like isaacs sometimes offer some help because other users can help identify workarounds, for instance, as shown here .","title":"Help"},{"location":"tutorial/github_issues/#acknowledgements","text":"Adopted from CD2H MTIP tutorial","title":"Acknowledgements"},{"location":"tutorial/icbo2021/","text":"ICBO Tutorial 2021 \u00b6 Wednesday, September 15, 2021 4 PM to 7 PM CEST 10 AM to 1 PM EDT 7 AM to 10 AM PDT Goal \u00b6 The goal of this tutorial is to provide a flavor of the OBO landscape, from the OBO Foundry organization to the ontology curators and OBO engineers that are doing the daily ontology development. Organizers \u00b6 James A. Overton , Knocean Inc. Chris Mungall , Lawrence Berkeley National Laboratory Nicole Vasilevsky , University of Colorado Anschutz Medical Campus Nico Matentzoglu , semanticly Ltd Randi Vita , La Jolla Institute for Allergy & Immunology Agenda \u00b6 Time CEST Presenter Topic 4:00 - 4:10pm James Overton Workshop overview 4:10 - 4:20pm James Overton OBO Foundry Overview 4:20 - 4:30pm Nicole Vasilevsky Controlled Vocabularies and Ontologies 4:30 - 4:50pm Nicole Vasilevsky Using and Reusing Ontology Terms 4:50 - 5:25pm Nicole Vasilevsky A day in the life of an Ontology Curator 5:25 - 5:30pm Break 5:30pm - 5:40pm Nico Matentzoglu Ontology 201 Overview 5:40 - 6:15 pm James Overton ROBOT Tutorial 6:15 - 6:35 pm Nico Matentzoglu ODK presentation 6:35 - 6:55 pm Nico Matentzoglu A brief introduction into ontology QC using the OBO dashboard 6:55 - 7:00 pm James Overton Wrap up","title":"ICBO 2021 Tutorial"},{"location":"tutorial/icbo2021/#icbo-tutorial-2021","text":"Wednesday, September 15, 2021 4 PM to 7 PM CEST 10 AM to 1 PM EDT 7 AM to 10 AM PDT","title":"ICBO Tutorial 2021"},{"location":"tutorial/icbo2021/#goal","text":"The goal of this tutorial is to provide a flavor of the OBO landscape, from the OBO Foundry organization to the ontology curators and OBO engineers that are doing the daily ontology development.","title":"Goal"},{"location":"tutorial/icbo2021/#organizers","text":"James A. Overton , Knocean Inc. Chris Mungall , Lawrence Berkeley National Laboratory Nicole Vasilevsky , University of Colorado Anschutz Medical Campus Nico Matentzoglu , semanticly Ltd Randi Vita , La Jolla Institute for Allergy & Immunology","title":"Organizers"},{"location":"tutorial/icbo2021/#agenda","text":"Time CEST Presenter Topic 4:00 - 4:10pm James Overton Workshop overview 4:10 - 4:20pm James Overton OBO Foundry Overview 4:20 - 4:30pm Nicole Vasilevsky Controlled Vocabularies and Ontologies 4:30 - 4:50pm Nicole Vasilevsky Using and Reusing Ontology Terms 4:50 - 5:25pm Nicole Vasilevsky A day in the life of an Ontology Curator 5:25 - 5:30pm Break 5:30pm - 5:40pm Nico Matentzoglu Ontology 201 Overview 5:40 - 6:15 pm James Overton ROBOT Tutorial 6:15 - 6:35 pm Nico Matentzoglu ODK presentation 6:35 - 6:55 pm Nico Matentzoglu A brief introduction into ontology QC using the OBO dashboard 6:55 - 7:00 pm James Overton Wrap up","title":"Agenda"},{"location":"tutorial/linking-data/","text":"Tutorial: From Tables to Linked Data \u00b6 These are the kinds of things that I do when I need to work with a new dataset. My goal is to have data that makes good sense and that I can integrate with other data using standard technologies: Linked Data. 0. Before \u00b6 The boss just sent me this new table to figure out: datetime investigator subject species strain sex group protocol organ disease qualifier comment 1/1/14 10:21 AM JAO 12 RAT F 344/N FEMALE 1 HISTOPATHOLOGY LUNG ADENOCARCINOMA SEVERE 1/1/14 10:30 AM JO 31 MOUSE B6C3F1 MALE 2 HISTOPATHOLOGY NOSE INFLAMMATION MILD 1/1/14 10:45 AM JAO 45 RAT F 344/N MALE 1 HISTOPATHOLOGY ADRENAL CORTEX NECROSIS MODERATE It doesn't seem too bad, but there's lots of stuff that I don't quite understand. Where to start? 1. Getting Organized \u00b6 Before I do anything else, I'm going to set up a new project for working with this data. Maybe I'll change my mind later and want to merge the new project with an existing project, but it never hurts to start from a nice clean state. I'll make a new directory in a sensible place with a sensible name. In my case I have a ~/Repositories/ directory, with subdirectories for GitHub and various GitLab servers, a local directory for projects I don't plan to share, and a temp directory for projects that I don't need to keep. I'm not sure if I'm going to share this work, so it can go in a new subdirectory of local . I'll call it \"linking-data-tutorial\" for now. Then I'll run git init to turn that directory into a git repository. For now I'm just going to work locally, but later I can make a repository on GitHub and push my local repository there. Next I'll create a README.md file where I'll keep notes for myself to read later. My preferred editor is Kakoune . So I'll open a terminal and run these commands: $ cd ~/Repositories/local/ $ mkdir linking-data-tutorial $ cd linking-data-tutorial $ git init $ kak README.md In the README I'll start writing something like this: # Linking Data Tutorial An example of how to convert a dataset to Linked Data. The source data is available from <https://github.com/jamesaoverton/obook/tree/master/03-RDF/data.csv> Maybe this information should go somewhere else eventually, but the README is a good place to start. \"Commit early, commit often\" they say, so: $ git add README.md $ git commit -m \"Initial commit\" 2. Getting Copies \u00b6 Data has an annoying tendency to get changed. You don't want it changing out from under you while you're in the middle of something. So the next thing to do is get a copy of the data and store it locally. If it's big, you can store a compressed copy. If it's too big to fit on your local machine, well keep the best notes you can of how to get to the data, and what operations you're doing on it. I'm going to make a cache directory and store all my \"upstream\" data there. I'm going to fetch the data and that's it -- I'm not going to edit these files. When I want to change the data I'll make copies in another directory. I don't want git to track the cached data, so I'll add /cache/ to .gitignore and tell git to track that . Then I'll use curl to download the file. $ mkdir cache $ echo \"/cache/\" >> .gitignore $ git add .gitignore $ git commit -m \"Ignore /cache/ directory\" $ cd cache $ curl -LO \"https://github.com/jamesaoverton/obook/raw/master/03-RDF/data.csv\" $ ls data.csv $ cd .. $ ls -a .gitignore data README.md 3. Getting My Bearings \u00b6 The first thing to do is look at the data. In this case I have just one table in CSV format, so I can use any number of tools to open the file and look around. I bet the majority of people would reach for Excel. My (idiosyncratic) preference is VisiData . What am I looking for? A bunch of different things: what do the rows represent? what columns do I have? for each column, what sorts of values do I have? In my README file I'll make a list of the columns like this: - datetime - investigator - subject - species - strain - sex - group - protocol - organ - disease - qualifier - comment Then I'll make some notes for myself: - datetime: American-style dates, D/M/Y or M/D/Y? - investigator: initials, ORCID? - subject: integer ID - species: common name for species, NCBITaxon? - strain: some sort of code with letters, numbers, spaces, some punctuation - sex: string female/male - group: integer ID - protocol: string, OBI? - organ: string, UBERON? - disease: string, DO/MONDO? - qualifier: string, PATO? - comment: ??? You can see that I'm trying to figure out what's in each column. I'm also thinking ahead to OBO ontologies that I know of that may have terms that I can use for each column. 4. Getting Structured \u00b6 In the end, I want to have nice, clean Linked Data. But I don't have to get there in one giant leap. Instead I'll take a bunch of small, incremental steps. There's lots of tools I can use, but this time I'll use SQLite. First I'll set up some more directories. I'll create a build directory where I'll store temporary files. I don't want git to track this directory, so I'll add it to .gitignore . $ mkdir build/ $ echo \"/build/\" >> .gitignore $ git add .gitignore $ git commit -m \"Ignore /build/ directory\" I'll also add a src directory to store code. I do want to track src with git. $ mkdir src $ kak src/data.sql In src/data.sql I'll add just enough to import build/data.csv : ```sql #src/data.sql -- import build/data.csv .mode csv .import build/data.csv data_csv This will create a `build/data.db` file and import `build/data.csv` into a `data_csv` table. Does it work? ```sh $ sqlite3 build/data.db < src/data.sql $ sqlite3 build/data.db <<< \"SELECT * FROM data_csv LIMIT 1;\" 2014-01-01 10:21:00-0500|JAO|12|RAT|F 344/N|FEMALE|1|HISTOPATHOLOGY|LUNG|ADENOCARCINOMA|SEVERE| Nice! Note that I didn't even specify a schema for data_csv . It uses the first row as the column names, and the type of every column is TEXT . Here's the schema I end up with: $ sqlite3 build/data.db <<< \".schema data_csv\" CREATE TABLE data_csv( \"datetime\" TEXT, \"investigator\" TEXT, \"subject\" TEXT, \"species\" TEXT, \"strain\" TEXT, \"sex\" TEXT, \"group\" TEXT, \"protocol\" TEXT, \"organ\" TEXT, \"disease\" TEXT, \"qualifier\" TEXT, \"comment\" TEXT ); I'm going to want to update src/data.sql then rebuild the database over and over. It's small, so this will only take a second. If it was big, then I would copy a subset into build/data.csv for now so that I the script still runs in a second or two and I can iterate quickly. I'll write a src/build.sh script to make life a little easier: #!/bin/sh rm -f build/* cp cache/data.csv build/data.csv sqlite3 build/data.db < src/data.sql Does it work? $ sh src/build.sh Nice! Time to update the README: ## Requirements - [SQLite3](https://sqlite.org/index.html) ## Usage Run `sh src/build.sh` I'll commit my work in progress: $ git add src/data.sql src/build.sh $ git add --update $ git commit -m \"Load data.csv into SQLite\" Now I have a script that executes a SQL file that loads the source data into a new database. I'll modify the src/data.sql file in a series of small steps until it has the structure that I want. 5. Getting Clean \u00b6 In the real world, data is always a mess. It takes real work to clean it up. And really, it's almost never perfectly clean. It's important to recognize that cleaning data has diminishing returns. There's low hanging fruit: easy to clean, often with code, and bringing big benefits. Then there's tough stuff that requires an expert to work through the details, row by row. The first thing to do is figure out the schema you want. I'll create a new data table and start with the default schema from data_csv . Notice that in the default schema all the column names are quoted. That's kind of annoying. But when I remove the quotation marks I realize that one of the column names is \"datetime\", but datetime is a keyword in SQLite! You can't use it as a column name without quoting. I'll rename it to \"assay_datetime\". I have the same problem with \"group\". I'll rename \"group\" to \"group_id\" and \"subject\" to \"subject_id\". The rest of the column names seem fine. I want \"assay_datetime\" to be in standard ISO datetime format, but SQLite stores these as TEXT. The \"subject\" and \"group\" columns are currently integers, but I plan to make them into URIs to CURIEs. So everything will still be TEXT. CREATE TABLE data( assay_datetime TEXT, investigator TEXT, subject_id TEXT, species TEXT, strain TEXT, sex TEXT, group_id TEXT, protocol TEXT, organ TEXT, disease TEXT, qualifier TEXT, comment TEXT ); The dates currently look like \"1/1/14 10:21 AM\". Say I know that they were done on Eastern Standard Time. How do I convert to ISO dates like \"2014-01-01 10:21:00-0500\"? Well SQLite isn't the right tool for this. The Unix date command does a nice job, though: $ date -d \"1/1/14 10:21 AM EST\" +\"%Y-%m-%d %H:%M:%S%z\" 2014-01-01 10:21:00-0500 I can run that over each line of the file using awk . So I update the src/build.sh to rework the build/data.csv before I import: ```sh #src/build.sh !/bin/sh \u00b6 rm -f build/* head -n1 cache/data.csv > build/data.csv tail -n+2 cache/data.csv \\ | awk 'BEGIN{FS=\",\"; OFS=\",\"} { \"date -d \\\"\"$1\" EST\\\" +\\\"%Y-%m-%d %H:%M:%S%z\\\"\" | getline $1; print $0 }' \\ build/data.csv sqlite3 build/data.db < src/data.sql One more problem I could clean up is that \"JO\" should really be \"JAO\" -- that's just a typo, and they should both refer to James A. Overton. I could make that change in `src/build.sh`, but I'll do it in `src/data.sql` instead. I'll write a query to copy all the rows of `data_csv` into `data` and then I'll update `data` with some fixes. ```sql -- copy from data_csv to data INSERT INTO data SELECT * FROM data_csv; -- clean data UPDATE data SET investigator=\"JAO\" WHERE investigator=\"JO\"; Honestly, it took me quite a while to write that awk command. It's a very powerful tool, but I don't use it enough to remember how it works. You might prefer to write yourself a Python script, or some R code. You could use that instead of this SQL UPDATE as well. I just wanted to show you two of the thousands of ways to do this. If there's a lot of replacements like \"JO\", then you might also consider listing them in another table that you can read into your script. The important part is to automate your cleaning! Why didn't I just edit cache/data.csv in Excel? In step 2 I saved a copy of the data because I didn't want it to change while I was working on it, but I do expect it to change! By automating the cleaning process, I should be able to just update cache/data.csv run everything again, and the fixes will be applied again. I don't want to do all this work manually every time the upstream data is updated. I'll commit my work in progress: $ git add --update $ git commit -m \"Start cleaning data\" Cleaning can take a lot of work. This is example table is pretty clean already. The next hard part is sorting out your terminology. 6. Getting Connected \u00b6 It's pretty easy to convert a table structure to triples. The hard part is converting the table contents . There are some identifiers in the table that would be better as URLs, and there's a bunch of terminology that would be better if it was linked to an ontology or other system. I'll start with the identifiers that are local to this data: subject_id and group_id. I can convert them to URLs by defining a prefix and then just using that prefix. I'll use string concatenation to update the table: -- update subject and groupd IDs UPDATE data SET subject_id='ex:subject-' || subject_id; UPDATE data SET group_id='ex:group-' || group_id; Now I'll check my work: $ sqlite3 build/data.db <<< \"SELECT * FROM data_csv LIMIT 1;\" 2014-01-01 10:21:00-0500|JAO|ex:subject-12|RAT|F 344/N|FEMALE|ex:group-1|HISTOPATHOLOGY|LUNG|ADENOCARCINOMA|SEVERE| I should take a moment to tell you, that while I was writing the Turtle conversion code later in this essay, I had to come back here and change these identifiers. The thing is that Turtle is often more strict than I expect about identifier syntax. Turtle identifiers look like CURIEs , but they're actually QNames . CURIEs are pretty much just just URLs shortened with a prefix, so almost anything goes. QNames come from XML , and Turtle identifiers have to be valid XML element names. I always remember that I need to stick to alphanumeric characters, and that I have to replace whitespace and punctuation with a - or _ . I didn't remember that the local part (aka \"suffix\", aka \"NCName\") can't start with a digit. So I tried to use \"subject:12\" and \"group:1\" as my identifiers. That worked fine until I generated Turtle. The Turtle looked fine, so it took me quite a while to figure out why it looked very wrong when I converted it into RDXML format. This kind of thing happens to me all the time. I'm almost always using a mixture of technologies based on different sets of assumptions, and there are always things that don't line up. That's why I like to work in small iterations, checking my work as I go (preferrably with automated tests), and keeping everything in version control. When I need to make a change like this one, I just circle back and iterate again. The next thing is to tackle the terminology. First I'll just make a list of the terms I'm using from the relevant columns in build/term.tsv : ```sh #collect $ sqlite3 build/data.db << EOF > build/term.tsv SELECT investigator FROM data UNION SELECT species FROM data UNION SELECT strain FROM data UNION SELECT strain FROM data UNION SELECT sex FROM data UNION SELECT protocol FROM data UNION SELECT organ FROM data UNION SELECT disease FROM data UNION SELECT qualifier FROM data; EOF It's a lot of work to go through all those terms and find good ontology terms. I'm going to do that hard work for you (just this once!) so we can keep moving. I'll add this table to `src/term.tsv` id | code | label ---|---|--- obo:NCBITaxon_10116 | RAT | Rattus norvegicus obo:NCBITaxon_10090 | MOUSE | Mus musculus ex:F344N | F 344/N | F 344/N ex:B6C3F1 | B6C3F1 | B6C3F1 obo:PATO_0000383 | FEMALE | female obo:PATO_0000384 | MALE | male obo:OBI_0600020 | HISTOPATHOLOGY | histology obo:UBERON_0002048 | LUNG | lung obo:UBERON_0007827 | NOSE | external nose obo:UBERON_0001235 | ADRENAL CORTEX | adrenal cortex obo:MPATH_268 | ADENOCARCINOMA | adenocarcinoma obo:MPATH_212 | INFLAMMATION | inflammation obo:MPATH_4 | NECROSIS | necrosis obo:PATO_0000396 | SEVERE | severe intensity obo:PATO_0000394 | MILD | mild intensity obo:PATO_0000395 | MODERATE | moderate intensity orcid:0000-0001-5139-5557 | JAO | James A. Overton And I'll add these prefixes to `src/prefix.tsv`: prefix | base ---|--- rdf | http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs | http://www.w3.org/2000/01/rdf-schema# xsd | http://www.w3.org/2001/XMLSchema# owl | http://www.w3.org/2002/07/owl# obo | http://purl.obolibrary.org/obo/ orcid | http://orcid.org/ ex | https://example.com/ subject | https://example.com/subject/ group | https://example.com/group/ Now I can import these tables into SQL and use the term table as a FOREIGN KEY constraint on data: ```sql #src/data.sql .mode tabs CREATE TABLE prefix ( prefix TEXT PRIMARY KEY, base TEXT UNIQUE ); .import --skip 1 src/prefix.tsv prefix CREATE TABLE term ( id TEXT PRIMARY KEY, code TEXT UNIQUE, label TEXT UNIQUE ); .import --skip 1 src/term.tsv term CREATE TABLE data( assay_datetime TEXT, investigator TEXT, subject_id TEXT, species TEXT, strain TEXT, sex TEXT, group_id TEXT, protocol TEXT, organ TEXT, disease TEXT, qualifier TEXT, comment TEXT, FOREIGN KEY(investigator) REFERENCES term(investigator), FOREIGN KEY(species) REFERENCES term(species), FOREIGN KEY(strain) REFERENCES term(strain), FOREIGN KEY(sex) REFERENCES term(sex), FOREIGN KEY(protocol) REFERENCES term(protocol), FOREIGN KEY(organ) REFERENCES term(organ), FOREIGN KEY(disease) REFERENCES term(disease), FOREIGN KEY(qualifier) REFERENCES term(qualifier) ); -- copy from data_csv to data INSERT INTO data SELECT * FROM data_csv; -- clean data UPDATE data SET investigator='JAO' WHERE investigator='JO'; -- update subject and groupd IDs UPDATE data SET subject_id='ex:subject-' || subject_id; UPDATE data SET group_id='ex:group-' || group_id; I'll update the README: See `src/` for: - `prefix.tsv`: shared prefixes - `term.tsv`: terminology I'll commit my work in progress: $ git add src/prefix.tsv src/term.tsv $ git add --update $ git commit -m \"Add and apply prefix and term tables\" Now all the terms are linked to controlled vocabularies of one sort or another. If I want to see the IDs for those links instead of the \"codes\" I can define a VIEW: ```sql #src/data.sql CREATE VIEW linked_data_id AS SELECT assay_datetime, investigator_term.id AS investigator, subject_id, species_term.id AS species, strain_term.id AS strain, sex_term.id AS sex, group_id, protocol_term.id AS protocol, organ_term.id AS organ, disease_term.id AS disease, qualifier_term.id AS qualifier FROM data JOIN term as investigator_term ON data.investigator = investigator_term.code JOIN term as species_term ON data.species = species_term.code JOIN term as strain_term ON data.strain = strain_term.code JOIN term as sex_term ON data.sex = sex_term.code JOIN term as protocol_term ON data.protocol = protocol_term.code JOIN term as organ_term ON data.organ = organ_term.code JOIN term as disease_term ON data.disease = disease_term.code JOIN term as qualifier_term ON data.qualifier = qualifier_term.code; I'll check: ```sh $ sqlite3 build/data.db <<< \"SELECT * FROM linked_ids LIMIT 1;\" 2014-01-01 10:21:00-0500|orcid:0000-0001-5139-5557|ex:subject-12|obo:NCBITaxon_10116|ex:F344N|obo:PATO_0000383|ex:group-1|obo:OBI_0600020|obo:UBERON_0002048|obo:MPATH_268|obo:PATO_0000396 I can also define a similar view for their \"official\" labels: ```sql #src/data.sql CREATE VIEW linked_data_label AS SELECT assay_datetime, investigator_term.label AS investigator, subject_id, species_term.label AS species, strain_term.label AS strain, sex_term.label AS sex, group_id, protocol_term.label AS protocol, organ_term.label AS organ, disease_term.label AS disease, qualifier_term.label AS qualifier FROM data JOIN term as investigator_term ON data.investigator = investigator_term.code JOIN term as species_term ON data.species = species_term.code JOIN term as strain_term ON data.strain = strain_term.code JOIN term as sex_term ON data.sex = sex_term.code JOIN term as protocol_term ON data.protocol = protocol_term.code JOIN term as organ_term ON data.organ = organ_term.code JOIN term as disease_term ON data.disease = disease_term.code JOIN term as qualifier_term ON data.qualifier = qualifier_term.code; I'll check: ```sh $ sqlite3 build/data.db <<< \"SELECT * FROM linked_data_label LIMIT 1;\" 2014-01-01 10:21:00-0500|James A. Overton|ex:subject-12|Rattus norvegicus|F 344/N|female|ex:group-1|histology|lung|adenocarcinoma|severe intensity I'll commit my work in progress: $ git add --update $ git commit -m \"Add linked_data tables\" Now the tables use URLs and is connected to ontologies and stuff. But are we Linked yet? 7. Getting Triples \u00b6 SQL tables aren't an official Linked Data format. Of all the RDF formats, I prefer Turtle. It's tedious but not difficult to get Turtle out of SQL. These query do what I need them to do, but note that if the literal data contained quotation marks (for instance) then I'd have to do more work to escape those. First I create a triple table: ```sql #src/data.sql CREATE TABLE triple ( subject TEXT, predicate TEXT, object TEXT, literal INTEGER -- 0 for object IRI, 1 for object literal ); -- create triples from term table INSERT INTO triple(subject, predicate, object, literal) SELECT id, 'rdfs:label', label, 1 FROM term; -- create triples from data table INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-assay_datetime', assay_datetime, 1 FROM data; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-investigator', term.id, 0 FROM data JOIN term AS term ON data.investigator = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-subject_id', subject_id, 0 FROM data; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-species', term.id, 0 FROM data JOIN term AS term ON data.species = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-strain', term.id, 0 FROM data JOIN term AS term ON data.strain = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-sex', term.id, 0 FROM data JOIN term AS term ON data.sex = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-group_id', group_id, 0 FROM data; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-protocol', term.id, 0 FROM data JOIN term AS term ON data.protocol = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-organ',term.id, 0 FROM data JOIN term AS term ON data.organ= term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-disease', term.id, 0 FROM data JOIN term AS term ON data.disease = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-qualifier', term.id, 0 FROM data JOIN term AS term ON data.qualifier = term.code; Then I can turn triples into Turtle using string concatenation: ```sql #src/turtle.sql SELECT '@prefix ' || prefix || ': <' || base || '> .' FROM prefix UNION ALL SELECT '' UNION ALL SELECT subject || ' ' || predicate || ' ' || CASE literal WHEN 1 THEN '\"' || object || '\"' ELSE object END || ' . ' FROM triple; I can add this to the src/build.sh : ```sh #src/build.sh sqlite3 build/data.db < src/turtle.sql > build/data.ttl Here's just a bit of that `build/data.ttl` file: ```ttl @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . orcid:0000-0001-5139-5557 rdfs:label \"James A. Overton\" . assay:1 column:assay_datetime \"2014-01-01 10:21:00-0500\"^^xsd:datetime . assay:1 column:investigator orcid:0000-0001-5139-5557 . SQL is not a particularly expressive language. Building the triple table is straightforward but verbose. I could have done the same thing with much less Python code. (Or I could have been clever and generated some SQL to execute!) I'll commit my work in progress: $ git add src/turtle.sql $ git add --update $ git commit -m \"Convert to Turtle\" So technically I have a Turtle file. Linked Data! Right? Well, it's kind of \"flat\". It still looks more like a table than a graph. 8. Getting Linked \u00b6 The table I started with is very much focused on the data: there was some sort of assay done, and this is the information that someone recorded about it. The Turtle I just ended up with is basically the same. Other people may have assay data. They may have tables that they converted into Turtle. So can I just merge them? Technically yes: I can put all these triples in one graph together. But I'll still just have \"flat\" chunks of data representing rows sitting next to other rows, without really linking together. The next thing I would do with this data is reorganized it based on the thing it's talking about. I know that: there was an assay the assay was performed at a certain time, using a certain protocol there was a person who performed the assay there was a subject animal of some species, strain, and sex the subject animal belonged to a study group the subject animal had some organs the assay resulted in some measurements Most of these are things that I could point to in the world, or could have pointed to if I was in the right place at the right time. By thinking about these things, I'm stepping beyond what it was convenient for someone to record, and thinking about what happened in the world. If somebody else has some assay data, then they might have recorded it differently for whatever reason, and so it wouldn't line up with my rows. I'm trying my best to use the same terms for the same things. I also want to use the same \"shapes\" for the same things. When trying to come to an agreement about what is connected to what, life is easier if I can point to the things I want to talk about: \"See, here is the person, and the mouse came from here, and he did this and this.\" I could model the data in SQL by breaking the big table into smaller tables. I could have tables for: person group subject: species, strain, sex, group assay: date, investigator, subject, protocol measurement: assay, organ, disease, qualifier Then I would convert each table to triples more carefully. That's a good idea. Actually it's a better idea than what I'm about to do... Since we're getting near the end, I'm going to show you how you can do that modelling in SPARQL. SPARQL has a CONSTRUCT operation that you use to build triples. There's lots of tools that I could use to run SPARQL but I'll use ROBOT . I'll start with the \"flat\" triples in build/data.ttl , select them with my WHERE clause, then CONSTRUCT better triples, and save them in build/model.ttl . ```sparql #src/model.rq PREFIX rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# PREFIX rdfs: http://www.w3.org/2000/01/rdf-schema# PREFIX xsd: http://www.w3.org/2001/XMLSchema# PREFIX owl: http://www.w3.org/2002/07/owl# PREFIX obo: http://purl.obolibrary.org/obo/ PREFIX ex: https://example.com/ CONSTRUCT { ?group rdfs:label ?group_label . ?subject rdf:type ?species ; rdfs:label ?subject_label ; ex:strain ?strain ; obo:RO_0000086 ?sex ; # has quality ex:group ?group . ?sex rdf:type ?sex_type ; rdfs:label ?sex_label . ?organ rdf:type ?organ_type ; rdfs:label ?organ_label ; obo:BFO_0000050 ?subject . # part of ?assay rdf:type ?assay_type ; rdfs:label ?assay_label ; obo:OBI_0000293 ?subject ; # has specified input obo:IAO_0000136 ?organ . # is about } WHERE { ?subject_row ex:column-assay_datetime ?datetime ; ex:column-investigator ?investigator ; ex:column-subject_id ?subject ; ex:column-species ?species ; ex:column-sex ?sex_type ; ex:column-group_id ?group ; ex:column-protocol ?assay_type ; ex:column-organ ?organ_type ; ex:column-disease ?disease ; ex:column-qualifier ?qualifier . ?assay_type rdfs:label ?assay_type_label . ?sex_type rdfs:label ?sex_type_label . ?organ_type rdfs:label ?organ_type_label . BIND (URI(CONCAT(STR(?subject), \"-assay\")) AS ?assay) BIND (URI(CONCAT(STR(?subject), \"-sex\")) AS ?sex) BIND (URI(CONCAT(STR(?subject), \"-organ\")) AS ?organ) BIND (CONCAT(\"subject \", REPLACE(STR(?subject), \"^. -\", \"\")) AS ?subject_label) BIND (CONCAT(\"group \", REPLACE(STR(?group), \"^. -\", \"\")) AS ?group_label) BIND (CONCAT(?subject_label, \" \", ?assay_type_label) AS ?assay_label) BIND (CONCAT(?subject_label, \" sex: \", ?sex_type_label) AS ?sex_label) BIND (CONCAT(?subject_label, \" \", ?organ_type_label) AS ?organ_label) } I can add this to the `src/build.sh`: ```sh #src/build.sh java -jar robot.jar query \\ --input build/data.ttl \\ --query src/model.rq build/model.ttl Then I get build/model.ttl that looks (in part) like this: ex:subject-31 a obo:NCBITaxon_10090 ; rdfs:label \"subject 31\" ; obo:RO_0000086 ex:subject-31-sex ; ex:group ex:group-2 . ex:group-2 rdfs:label \"group 2\" . Now that's what I call Linked Data! I'll update the README: ## Modelling The data refers to: - investigator - subject - group - assay - measurement data - subject organ - disease TODO: A pretty diagram. I'll commit my work in progress: $ git add src/model.rq $ git add --update $ git commit -m \"Build model.ttl\" 9. Getting It Done \u00b6 That was a lot of work for a small table. And I did all the hard work of mapping the terminology to ontology terms for you! There's lots more I can do. The SPARQL is just one big chunk, but it would be better in smaller pieces. The modelling isn't all that great yet. Before changing that I want to run it past the boss and see what she thinks. It's getting close to the end of the day. Before I quit I should update the README, clean up anything that's no longer relevant or correct, and make any necessary notes to my future self: $ git add --update $ git commit -m \"Update README\" $ quit","title":"From Tables to Linked Data"},{"location":"tutorial/linking-data/#tutorial-from-tables-to-linked-data","text":"These are the kinds of things that I do when I need to work with a new dataset. My goal is to have data that makes good sense and that I can integrate with other data using standard technologies: Linked Data.","title":"Tutorial: From Tables to Linked Data"},{"location":"tutorial/linking-data/#0-before","text":"The boss just sent me this new table to figure out: datetime investigator subject species strain sex group protocol organ disease qualifier comment 1/1/14 10:21 AM JAO 12 RAT F 344/N FEMALE 1 HISTOPATHOLOGY LUNG ADENOCARCINOMA SEVERE 1/1/14 10:30 AM JO 31 MOUSE B6C3F1 MALE 2 HISTOPATHOLOGY NOSE INFLAMMATION MILD 1/1/14 10:45 AM JAO 45 RAT F 344/N MALE 1 HISTOPATHOLOGY ADRENAL CORTEX NECROSIS MODERATE It doesn't seem too bad, but there's lots of stuff that I don't quite understand. Where to start?","title":"0. Before"},{"location":"tutorial/linking-data/#1-getting-organized","text":"Before I do anything else, I'm going to set up a new project for working with this data. Maybe I'll change my mind later and want to merge the new project with an existing project, but it never hurts to start from a nice clean state. I'll make a new directory in a sensible place with a sensible name. In my case I have a ~/Repositories/ directory, with subdirectories for GitHub and various GitLab servers, a local directory for projects I don't plan to share, and a temp directory for projects that I don't need to keep. I'm not sure if I'm going to share this work, so it can go in a new subdirectory of local . I'll call it \"linking-data-tutorial\" for now. Then I'll run git init to turn that directory into a git repository. For now I'm just going to work locally, but later I can make a repository on GitHub and push my local repository there. Next I'll create a README.md file where I'll keep notes for myself to read later. My preferred editor is Kakoune . So I'll open a terminal and run these commands: $ cd ~/Repositories/local/ $ mkdir linking-data-tutorial $ cd linking-data-tutorial $ git init $ kak README.md In the README I'll start writing something like this: # Linking Data Tutorial An example of how to convert a dataset to Linked Data. The source data is available from <https://github.com/jamesaoverton/obook/tree/master/03-RDF/data.csv> Maybe this information should go somewhere else eventually, but the README is a good place to start. \"Commit early, commit often\" they say, so: $ git add README.md $ git commit -m \"Initial commit\"","title":"1. Getting Organized"},{"location":"tutorial/linking-data/#2-getting-copies","text":"Data has an annoying tendency to get changed. You don't want it changing out from under you while you're in the middle of something. So the next thing to do is get a copy of the data and store it locally. If it's big, you can store a compressed copy. If it's too big to fit on your local machine, well keep the best notes you can of how to get to the data, and what operations you're doing on it. I'm going to make a cache directory and store all my \"upstream\" data there. I'm going to fetch the data and that's it -- I'm not going to edit these files. When I want to change the data I'll make copies in another directory. I don't want git to track the cached data, so I'll add /cache/ to .gitignore and tell git to track that . Then I'll use curl to download the file. $ mkdir cache $ echo \"/cache/\" >> .gitignore $ git add .gitignore $ git commit -m \"Ignore /cache/ directory\" $ cd cache $ curl -LO \"https://github.com/jamesaoverton/obook/raw/master/03-RDF/data.csv\" $ ls data.csv $ cd .. $ ls -a .gitignore data README.md","title":"2. Getting Copies"},{"location":"tutorial/linking-data/#3-getting-my-bearings","text":"The first thing to do is look at the data. In this case I have just one table in CSV format, so I can use any number of tools to open the file and look around. I bet the majority of people would reach for Excel. My (idiosyncratic) preference is VisiData . What am I looking for? A bunch of different things: what do the rows represent? what columns do I have? for each column, what sorts of values do I have? In my README file I'll make a list of the columns like this: - datetime - investigator - subject - species - strain - sex - group - protocol - organ - disease - qualifier - comment Then I'll make some notes for myself: - datetime: American-style dates, D/M/Y or M/D/Y? - investigator: initials, ORCID? - subject: integer ID - species: common name for species, NCBITaxon? - strain: some sort of code with letters, numbers, spaces, some punctuation - sex: string female/male - group: integer ID - protocol: string, OBI? - organ: string, UBERON? - disease: string, DO/MONDO? - qualifier: string, PATO? - comment: ??? You can see that I'm trying to figure out what's in each column. I'm also thinking ahead to OBO ontologies that I know of that may have terms that I can use for each column.","title":"3. Getting My Bearings"},{"location":"tutorial/linking-data/#4-getting-structured","text":"In the end, I want to have nice, clean Linked Data. But I don't have to get there in one giant leap. Instead I'll take a bunch of small, incremental steps. There's lots of tools I can use, but this time I'll use SQLite. First I'll set up some more directories. I'll create a build directory where I'll store temporary files. I don't want git to track this directory, so I'll add it to .gitignore . $ mkdir build/ $ echo \"/build/\" >> .gitignore $ git add .gitignore $ git commit -m \"Ignore /build/ directory\" I'll also add a src directory to store code. I do want to track src with git. $ mkdir src $ kak src/data.sql In src/data.sql I'll add just enough to import build/data.csv : ```sql #src/data.sql -- import build/data.csv .mode csv .import build/data.csv data_csv This will create a `build/data.db` file and import `build/data.csv` into a `data_csv` table. Does it work? ```sh $ sqlite3 build/data.db < src/data.sql $ sqlite3 build/data.db <<< \"SELECT * FROM data_csv LIMIT 1;\" 2014-01-01 10:21:00-0500|JAO|12|RAT|F 344/N|FEMALE|1|HISTOPATHOLOGY|LUNG|ADENOCARCINOMA|SEVERE| Nice! Note that I didn't even specify a schema for data_csv . It uses the first row as the column names, and the type of every column is TEXT . Here's the schema I end up with: $ sqlite3 build/data.db <<< \".schema data_csv\" CREATE TABLE data_csv( \"datetime\" TEXT, \"investigator\" TEXT, \"subject\" TEXT, \"species\" TEXT, \"strain\" TEXT, \"sex\" TEXT, \"group\" TEXT, \"protocol\" TEXT, \"organ\" TEXT, \"disease\" TEXT, \"qualifier\" TEXT, \"comment\" TEXT ); I'm going to want to update src/data.sql then rebuild the database over and over. It's small, so this will only take a second. If it was big, then I would copy a subset into build/data.csv for now so that I the script still runs in a second or two and I can iterate quickly. I'll write a src/build.sh script to make life a little easier: #!/bin/sh rm -f build/* cp cache/data.csv build/data.csv sqlite3 build/data.db < src/data.sql Does it work? $ sh src/build.sh Nice! Time to update the README: ## Requirements - [SQLite3](https://sqlite.org/index.html) ## Usage Run `sh src/build.sh` I'll commit my work in progress: $ git add src/data.sql src/build.sh $ git add --update $ git commit -m \"Load data.csv into SQLite\" Now I have a script that executes a SQL file that loads the source data into a new database. I'll modify the src/data.sql file in a series of small steps until it has the structure that I want.","title":"4. Getting Structured"},{"location":"tutorial/linking-data/#5-getting-clean","text":"In the real world, data is always a mess. It takes real work to clean it up. And really, it's almost never perfectly clean. It's important to recognize that cleaning data has diminishing returns. There's low hanging fruit: easy to clean, often with code, and bringing big benefits. Then there's tough stuff that requires an expert to work through the details, row by row. The first thing to do is figure out the schema you want. I'll create a new data table and start with the default schema from data_csv . Notice that in the default schema all the column names are quoted. That's kind of annoying. But when I remove the quotation marks I realize that one of the column names is \"datetime\", but datetime is a keyword in SQLite! You can't use it as a column name without quoting. I'll rename it to \"assay_datetime\". I have the same problem with \"group\". I'll rename \"group\" to \"group_id\" and \"subject\" to \"subject_id\". The rest of the column names seem fine. I want \"assay_datetime\" to be in standard ISO datetime format, but SQLite stores these as TEXT. The \"subject\" and \"group\" columns are currently integers, but I plan to make them into URIs to CURIEs. So everything will still be TEXT. CREATE TABLE data( assay_datetime TEXT, investigator TEXT, subject_id TEXT, species TEXT, strain TEXT, sex TEXT, group_id TEXT, protocol TEXT, organ TEXT, disease TEXT, qualifier TEXT, comment TEXT ); The dates currently look like \"1/1/14 10:21 AM\". Say I know that they were done on Eastern Standard Time. How do I convert to ISO dates like \"2014-01-01 10:21:00-0500\"? Well SQLite isn't the right tool for this. The Unix date command does a nice job, though: $ date -d \"1/1/14 10:21 AM EST\" +\"%Y-%m-%d %H:%M:%S%z\" 2014-01-01 10:21:00-0500 I can run that over each line of the file using awk . So I update the src/build.sh to rework the build/data.csv before I import: ```sh #src/build.sh","title":"5. Getting Clean"},{"location":"tutorial/linking-data/#binsh","text":"rm -f build/* head -n1 cache/data.csv > build/data.csv tail -n+2 cache/data.csv \\ | awk 'BEGIN{FS=\",\"; OFS=\",\"} { \"date -d \\\"\"$1\" EST\\\" +\\\"%Y-%m-%d %H:%M:%S%z\\\"\" | getline $1; print $0 }' \\ build/data.csv sqlite3 build/data.db < src/data.sql One more problem I could clean up is that \"JO\" should really be \"JAO\" -- that's just a typo, and they should both refer to James A. Overton. I could make that change in `src/build.sh`, but I'll do it in `src/data.sql` instead. I'll write a query to copy all the rows of `data_csv` into `data` and then I'll update `data` with some fixes. ```sql -- copy from data_csv to data INSERT INTO data SELECT * FROM data_csv; -- clean data UPDATE data SET investigator=\"JAO\" WHERE investigator=\"JO\"; Honestly, it took me quite a while to write that awk command. It's a very powerful tool, but I don't use it enough to remember how it works. You might prefer to write yourself a Python script, or some R code. You could use that instead of this SQL UPDATE as well. I just wanted to show you two of the thousands of ways to do this. If there's a lot of replacements like \"JO\", then you might also consider listing them in another table that you can read into your script. The important part is to automate your cleaning! Why didn't I just edit cache/data.csv in Excel? In step 2 I saved a copy of the data because I didn't want it to change while I was working on it, but I do expect it to change! By automating the cleaning process, I should be able to just update cache/data.csv run everything again, and the fixes will be applied again. I don't want to do all this work manually every time the upstream data is updated. I'll commit my work in progress: $ git add --update $ git commit -m \"Start cleaning data\" Cleaning can take a lot of work. This is example table is pretty clean already. The next hard part is sorting out your terminology.","title":"!/bin/sh"},{"location":"tutorial/linking-data/#6-getting-connected","text":"It's pretty easy to convert a table structure to triples. The hard part is converting the table contents . There are some identifiers in the table that would be better as URLs, and there's a bunch of terminology that would be better if it was linked to an ontology or other system. I'll start with the identifiers that are local to this data: subject_id and group_id. I can convert them to URLs by defining a prefix and then just using that prefix. I'll use string concatenation to update the table: -- update subject and groupd IDs UPDATE data SET subject_id='ex:subject-' || subject_id; UPDATE data SET group_id='ex:group-' || group_id; Now I'll check my work: $ sqlite3 build/data.db <<< \"SELECT * FROM data_csv LIMIT 1;\" 2014-01-01 10:21:00-0500|JAO|ex:subject-12|RAT|F 344/N|FEMALE|ex:group-1|HISTOPATHOLOGY|LUNG|ADENOCARCINOMA|SEVERE| I should take a moment to tell you, that while I was writing the Turtle conversion code later in this essay, I had to come back here and change these identifiers. The thing is that Turtle is often more strict than I expect about identifier syntax. Turtle identifiers look like CURIEs , but they're actually QNames . CURIEs are pretty much just just URLs shortened with a prefix, so almost anything goes. QNames come from XML , and Turtle identifiers have to be valid XML element names. I always remember that I need to stick to alphanumeric characters, and that I have to replace whitespace and punctuation with a - or _ . I didn't remember that the local part (aka \"suffix\", aka \"NCName\") can't start with a digit. So I tried to use \"subject:12\" and \"group:1\" as my identifiers. That worked fine until I generated Turtle. The Turtle looked fine, so it took me quite a while to figure out why it looked very wrong when I converted it into RDXML format. This kind of thing happens to me all the time. I'm almost always using a mixture of technologies based on different sets of assumptions, and there are always things that don't line up. That's why I like to work in small iterations, checking my work as I go (preferrably with automated tests), and keeping everything in version control. When I need to make a change like this one, I just circle back and iterate again. The next thing is to tackle the terminology. First I'll just make a list of the terms I'm using from the relevant columns in build/term.tsv : ```sh #collect $ sqlite3 build/data.db << EOF > build/term.tsv SELECT investigator FROM data UNION SELECT species FROM data UNION SELECT strain FROM data UNION SELECT strain FROM data UNION SELECT sex FROM data UNION SELECT protocol FROM data UNION SELECT organ FROM data UNION SELECT disease FROM data UNION SELECT qualifier FROM data; EOF It's a lot of work to go through all those terms and find good ontology terms. I'm going to do that hard work for you (just this once!) so we can keep moving. I'll add this table to `src/term.tsv` id | code | label ---|---|--- obo:NCBITaxon_10116 | RAT | Rattus norvegicus obo:NCBITaxon_10090 | MOUSE | Mus musculus ex:F344N | F 344/N | F 344/N ex:B6C3F1 | B6C3F1 | B6C3F1 obo:PATO_0000383 | FEMALE | female obo:PATO_0000384 | MALE | male obo:OBI_0600020 | HISTOPATHOLOGY | histology obo:UBERON_0002048 | LUNG | lung obo:UBERON_0007827 | NOSE | external nose obo:UBERON_0001235 | ADRENAL CORTEX | adrenal cortex obo:MPATH_268 | ADENOCARCINOMA | adenocarcinoma obo:MPATH_212 | INFLAMMATION | inflammation obo:MPATH_4 | NECROSIS | necrosis obo:PATO_0000396 | SEVERE | severe intensity obo:PATO_0000394 | MILD | mild intensity obo:PATO_0000395 | MODERATE | moderate intensity orcid:0000-0001-5139-5557 | JAO | James A. Overton And I'll add these prefixes to `src/prefix.tsv`: prefix | base ---|--- rdf | http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs | http://www.w3.org/2000/01/rdf-schema# xsd | http://www.w3.org/2001/XMLSchema# owl | http://www.w3.org/2002/07/owl# obo | http://purl.obolibrary.org/obo/ orcid | http://orcid.org/ ex | https://example.com/ subject | https://example.com/subject/ group | https://example.com/group/ Now I can import these tables into SQL and use the term table as a FOREIGN KEY constraint on data: ```sql #src/data.sql .mode tabs CREATE TABLE prefix ( prefix TEXT PRIMARY KEY, base TEXT UNIQUE ); .import --skip 1 src/prefix.tsv prefix CREATE TABLE term ( id TEXT PRIMARY KEY, code TEXT UNIQUE, label TEXT UNIQUE ); .import --skip 1 src/term.tsv term CREATE TABLE data( assay_datetime TEXT, investigator TEXT, subject_id TEXT, species TEXT, strain TEXT, sex TEXT, group_id TEXT, protocol TEXT, organ TEXT, disease TEXT, qualifier TEXT, comment TEXT, FOREIGN KEY(investigator) REFERENCES term(investigator), FOREIGN KEY(species) REFERENCES term(species), FOREIGN KEY(strain) REFERENCES term(strain), FOREIGN KEY(sex) REFERENCES term(sex), FOREIGN KEY(protocol) REFERENCES term(protocol), FOREIGN KEY(organ) REFERENCES term(organ), FOREIGN KEY(disease) REFERENCES term(disease), FOREIGN KEY(qualifier) REFERENCES term(qualifier) ); -- copy from data_csv to data INSERT INTO data SELECT * FROM data_csv; -- clean data UPDATE data SET investigator='JAO' WHERE investigator='JO'; -- update subject and groupd IDs UPDATE data SET subject_id='ex:subject-' || subject_id; UPDATE data SET group_id='ex:group-' || group_id; I'll update the README: See `src/` for: - `prefix.tsv`: shared prefixes - `term.tsv`: terminology I'll commit my work in progress: $ git add src/prefix.tsv src/term.tsv $ git add --update $ git commit -m \"Add and apply prefix and term tables\" Now all the terms are linked to controlled vocabularies of one sort or another. If I want to see the IDs for those links instead of the \"codes\" I can define a VIEW: ```sql #src/data.sql CREATE VIEW linked_data_id AS SELECT assay_datetime, investigator_term.id AS investigator, subject_id, species_term.id AS species, strain_term.id AS strain, sex_term.id AS sex, group_id, protocol_term.id AS protocol, organ_term.id AS organ, disease_term.id AS disease, qualifier_term.id AS qualifier FROM data JOIN term as investigator_term ON data.investigator = investigator_term.code JOIN term as species_term ON data.species = species_term.code JOIN term as strain_term ON data.strain = strain_term.code JOIN term as sex_term ON data.sex = sex_term.code JOIN term as protocol_term ON data.protocol = protocol_term.code JOIN term as organ_term ON data.organ = organ_term.code JOIN term as disease_term ON data.disease = disease_term.code JOIN term as qualifier_term ON data.qualifier = qualifier_term.code; I'll check: ```sh $ sqlite3 build/data.db <<< \"SELECT * FROM linked_ids LIMIT 1;\" 2014-01-01 10:21:00-0500|orcid:0000-0001-5139-5557|ex:subject-12|obo:NCBITaxon_10116|ex:F344N|obo:PATO_0000383|ex:group-1|obo:OBI_0600020|obo:UBERON_0002048|obo:MPATH_268|obo:PATO_0000396 I can also define a similar view for their \"official\" labels: ```sql #src/data.sql CREATE VIEW linked_data_label AS SELECT assay_datetime, investigator_term.label AS investigator, subject_id, species_term.label AS species, strain_term.label AS strain, sex_term.label AS sex, group_id, protocol_term.label AS protocol, organ_term.label AS organ, disease_term.label AS disease, qualifier_term.label AS qualifier FROM data JOIN term as investigator_term ON data.investigator = investigator_term.code JOIN term as species_term ON data.species = species_term.code JOIN term as strain_term ON data.strain = strain_term.code JOIN term as sex_term ON data.sex = sex_term.code JOIN term as protocol_term ON data.protocol = protocol_term.code JOIN term as organ_term ON data.organ = organ_term.code JOIN term as disease_term ON data.disease = disease_term.code JOIN term as qualifier_term ON data.qualifier = qualifier_term.code; I'll check: ```sh $ sqlite3 build/data.db <<< \"SELECT * FROM linked_data_label LIMIT 1;\" 2014-01-01 10:21:00-0500|James A. Overton|ex:subject-12|Rattus norvegicus|F 344/N|female|ex:group-1|histology|lung|adenocarcinoma|severe intensity I'll commit my work in progress: $ git add --update $ git commit -m \"Add linked_data tables\" Now the tables use URLs and is connected to ontologies and stuff. But are we Linked yet?","title":"6. Getting Connected"},{"location":"tutorial/linking-data/#7-getting-triples","text":"SQL tables aren't an official Linked Data format. Of all the RDF formats, I prefer Turtle. It's tedious but not difficult to get Turtle out of SQL. These query do what I need them to do, but note that if the literal data contained quotation marks (for instance) then I'd have to do more work to escape those. First I create a triple table: ```sql #src/data.sql CREATE TABLE triple ( subject TEXT, predicate TEXT, object TEXT, literal INTEGER -- 0 for object IRI, 1 for object literal ); -- create triples from term table INSERT INTO triple(subject, predicate, object, literal) SELECT id, 'rdfs:label', label, 1 FROM term; -- create triples from data table INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-assay_datetime', assay_datetime, 1 FROM data; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-investigator', term.id, 0 FROM data JOIN term AS term ON data.investigator = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-subject_id', subject_id, 0 FROM data; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-species', term.id, 0 FROM data JOIN term AS term ON data.species = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-strain', term.id, 0 FROM data JOIN term AS term ON data.strain = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-sex', term.id, 0 FROM data JOIN term AS term ON data.sex = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-group_id', group_id, 0 FROM data; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-protocol', term.id, 0 FROM data JOIN term AS term ON data.protocol = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-organ',term.id, 0 FROM data JOIN term AS term ON data.organ= term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-disease', term.id, 0 FROM data JOIN term AS term ON data.disease = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-qualifier', term.id, 0 FROM data JOIN term AS term ON data.qualifier = term.code; Then I can turn triples into Turtle using string concatenation: ```sql #src/turtle.sql SELECT '@prefix ' || prefix || ': <' || base || '> .' FROM prefix UNION ALL SELECT '' UNION ALL SELECT subject || ' ' || predicate || ' ' || CASE literal WHEN 1 THEN '\"' || object || '\"' ELSE object END || ' . ' FROM triple; I can add this to the src/build.sh : ```sh #src/build.sh sqlite3 build/data.db < src/turtle.sql > build/data.ttl Here's just a bit of that `build/data.ttl` file: ```ttl @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . orcid:0000-0001-5139-5557 rdfs:label \"James A. Overton\" . assay:1 column:assay_datetime \"2014-01-01 10:21:00-0500\"^^xsd:datetime . assay:1 column:investigator orcid:0000-0001-5139-5557 . SQL is not a particularly expressive language. Building the triple table is straightforward but verbose. I could have done the same thing with much less Python code. (Or I could have been clever and generated some SQL to execute!) I'll commit my work in progress: $ git add src/turtle.sql $ git add --update $ git commit -m \"Convert to Turtle\" So technically I have a Turtle file. Linked Data! Right? Well, it's kind of \"flat\". It still looks more like a table than a graph.","title":"7. Getting Triples"},{"location":"tutorial/linking-data/#8-getting-linked","text":"The table I started with is very much focused on the data: there was some sort of assay done, and this is the information that someone recorded about it. The Turtle I just ended up with is basically the same. Other people may have assay data. They may have tables that they converted into Turtle. So can I just merge them? Technically yes: I can put all these triples in one graph together. But I'll still just have \"flat\" chunks of data representing rows sitting next to other rows, without really linking together. The next thing I would do with this data is reorganized it based on the thing it's talking about. I know that: there was an assay the assay was performed at a certain time, using a certain protocol there was a person who performed the assay there was a subject animal of some species, strain, and sex the subject animal belonged to a study group the subject animal had some organs the assay resulted in some measurements Most of these are things that I could point to in the world, or could have pointed to if I was in the right place at the right time. By thinking about these things, I'm stepping beyond what it was convenient for someone to record, and thinking about what happened in the world. If somebody else has some assay data, then they might have recorded it differently for whatever reason, and so it wouldn't line up with my rows. I'm trying my best to use the same terms for the same things. I also want to use the same \"shapes\" for the same things. When trying to come to an agreement about what is connected to what, life is easier if I can point to the things I want to talk about: \"See, here is the person, and the mouse came from here, and he did this and this.\" I could model the data in SQL by breaking the big table into smaller tables. I could have tables for: person group subject: species, strain, sex, group assay: date, investigator, subject, protocol measurement: assay, organ, disease, qualifier Then I would convert each table to triples more carefully. That's a good idea. Actually it's a better idea than what I'm about to do... Since we're getting near the end, I'm going to show you how you can do that modelling in SPARQL. SPARQL has a CONSTRUCT operation that you use to build triples. There's lots of tools that I could use to run SPARQL but I'll use ROBOT . I'll start with the \"flat\" triples in build/data.ttl , select them with my WHERE clause, then CONSTRUCT better triples, and save them in build/model.ttl . ```sparql #src/model.rq PREFIX rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# PREFIX rdfs: http://www.w3.org/2000/01/rdf-schema# PREFIX xsd: http://www.w3.org/2001/XMLSchema# PREFIX owl: http://www.w3.org/2002/07/owl# PREFIX obo: http://purl.obolibrary.org/obo/ PREFIX ex: https://example.com/ CONSTRUCT { ?group rdfs:label ?group_label . ?subject rdf:type ?species ; rdfs:label ?subject_label ; ex:strain ?strain ; obo:RO_0000086 ?sex ; # has quality ex:group ?group . ?sex rdf:type ?sex_type ; rdfs:label ?sex_label . ?organ rdf:type ?organ_type ; rdfs:label ?organ_label ; obo:BFO_0000050 ?subject . # part of ?assay rdf:type ?assay_type ; rdfs:label ?assay_label ; obo:OBI_0000293 ?subject ; # has specified input obo:IAO_0000136 ?organ . # is about } WHERE { ?subject_row ex:column-assay_datetime ?datetime ; ex:column-investigator ?investigator ; ex:column-subject_id ?subject ; ex:column-species ?species ; ex:column-sex ?sex_type ; ex:column-group_id ?group ; ex:column-protocol ?assay_type ; ex:column-organ ?organ_type ; ex:column-disease ?disease ; ex:column-qualifier ?qualifier . ?assay_type rdfs:label ?assay_type_label . ?sex_type rdfs:label ?sex_type_label . ?organ_type rdfs:label ?organ_type_label . BIND (URI(CONCAT(STR(?subject), \"-assay\")) AS ?assay) BIND (URI(CONCAT(STR(?subject), \"-sex\")) AS ?sex) BIND (URI(CONCAT(STR(?subject), \"-organ\")) AS ?organ) BIND (CONCAT(\"subject \", REPLACE(STR(?subject), \"^. -\", \"\")) AS ?subject_label) BIND (CONCAT(\"group \", REPLACE(STR(?group), \"^. -\", \"\")) AS ?group_label) BIND (CONCAT(?subject_label, \" \", ?assay_type_label) AS ?assay_label) BIND (CONCAT(?subject_label, \" sex: \", ?sex_type_label) AS ?sex_label) BIND (CONCAT(?subject_label, \" \", ?organ_type_label) AS ?organ_label) } I can add this to the `src/build.sh`: ```sh #src/build.sh java -jar robot.jar query \\ --input build/data.ttl \\ --query src/model.rq build/model.ttl Then I get build/model.ttl that looks (in part) like this: ex:subject-31 a obo:NCBITaxon_10090 ; rdfs:label \"subject 31\" ; obo:RO_0000086 ex:subject-31-sex ; ex:group ex:group-2 . ex:group-2 rdfs:label \"group 2\" . Now that's what I call Linked Data! I'll update the README: ## Modelling The data refers to: - investigator - subject - group - assay - measurement data - subject organ - disease TODO: A pretty diagram. I'll commit my work in progress: $ git add src/model.rq $ git add --update $ git commit -m \"Build model.ttl\"","title":"8. Getting Linked"},{"location":"tutorial/linking-data/#9-getting-it-done","text":"That was a lot of work for a small table. And I did all the hard work of mapping the terminology to ontology terms for you! There's lots more I can do. The SPARQL is just one big chunk, but it would be better in smaller pieces. The modelling isn't all that great yet. Before changing that I want to run it past the boss and see what she thinks. It's getting close to the end of the day. Before I quit I should update the README, clean up anything that's no longer relevant or correct, and make any necessary notes to my future self: $ git add --update $ git commit -m \"Update README\" $ quit","title":"9. Getting It Done"},{"location":"tutorial/managing-dynamic-imports-odk/","text":"Managing Dynamic Imports with the Ontology Development Kit \u00b6 In this tutorial, we discuss the general workflow of managing dynamic imports, i.e. importing terms from other ontologies which can be kept up to date. Tutorial \u00b6 Follow instructions for the PATO dynamic import process here .","title":"ODK - Managing imports"},{"location":"tutorial/managing-dynamic-imports-odk/#managing-dynamic-imports-with-the-ontology-development-kit","text":"In this tutorial, we discuss the general workflow of managing dynamic imports, i.e. importing terms from other ontologies which can be kept up to date.","title":"Managing Dynamic Imports with the Ontology Development Kit"},{"location":"tutorial/managing-dynamic-imports-odk/#tutorial","text":"Follow instructions for the PATO dynamic import process here .","title":"Tutorial"},{"location":"tutorial/managing-ontology-project/","text":"Tutorial on Managing OBO Ontology Projects \u00b6 This tutorial is not about editing ontologies and managing the evolution of its content (aka ontology curation), but the general process of managing an ontology project overall. In this lesson, we will cover the following: How to effectively manage an ontology project using GitHub projects and teams How to coordinate the evolution of ontologies across projects and grants It is important to understand that the following is just one good way of doing project management for OBO ontologies, and most projects will do it slightly differently. We do however believe that thinking about your project management process and the roles involved will benefit your work in the long term, and hope that the following will help you as a starting point. Roles in OBO Ontology project management activities \u00b6 Ontology Editor (OE): manage the content of ontologies and interact with users Principal Ontology Editor (POE): coordinate the curation activities and have always fixed hours assigned to the project. Ontology Pipeline Developer (OPD): Manage the technical workflows around ontologies, such as release workflows, continuous integration and QC, and setting up data pipelines. Also helps with bulk editing activities. Principal Investigators (PI): Manage the projects that fund ontology curation activities. For an effective management of an ontology, the following criteria are recommended: There should be at least one Principal Ontology Editor for every ontology project. The importance is not whether this editor (or sometimes called 'ontology curator') has a specific number of hours per week allocated to the project (although based on our experience, 1 day per week is minimum), but whether the editor has a sense of ownership, i.e. they understand that they are the primary responsible person for maintaining the ontology moving forward. Because of potential grant overlapping issues, we recommend to have at least 1 Principal Ontology Editor for every grant/funded project that has a stake in the ontology. Every effective ontology needs at least a few hours per week from an Ontology Pipeline Developer (OPD). More on that role later. The OPD does not always have as strong a sense of ownership of the ontology project, but typically has a strong sense of responsibility to members of the curation team. There should be separate meetings for curation and technical activities - both problems are hard, and need different team members being present. We recommend at least monthly technical and biweekly curation calls, but for many of the most effective ontology projects we manage, weekly technical and weekly curation calls are normal. Without the above minimum criteria, the following recommendations will be very hard to implement. The Project Management Toolbox \u00b6 We make use of three tools in the following recommendation: Project boards : Project boards, sometimes referred to as Kanban boards, GitHub boards or agile boards, are a great way to organise outstanding tickets and help maintain a clear overview of what work needs to be done. They are usually realised with either GitHub projects or ZenHub . If you have not worked with project boards before, we highly recommend watching a quick tutorial on Youtube, such as: GitHub teams . GitHub teams , alongside with organisations, are a powerfull too to organise collaborative workflows on GitHub. They allow you to communicate and organise permissions for editing your ontology in a transparent way. You can get a sense of GitHub teams by watching one of the the numerous tutorials on GitHub, such as: Markdown-based documentation system . Writing great documentation is imperative for a sustainable project. Across many of our recent projects, were are using mkdocs , which we have also integrated with the Ontology Development Kit, but there are others to consider. We deeply recommend to complete a very short introduction to Markdown, this tutorial on YouTube . What do you need for your project? \u00b6 Every ontology or group of related ontologies (sometimes it is easier to manage multiple ontologies at once, because their scope or technical workflows are quite uniform or they are heavily interrelated) should have: at least two teams , an Editorial Team and a Technical Team, with clearly defined members. We recommend to create two teams on GitHub and keep their members always up to date (i.e. remove members that are not actively participating), but many of our projects merely maintain a \"core team\", which is a more liberal team containing everyone from stakeholders, principal investigators, editors and users (for managing write permissions see later in the \"best practice\" section) and listing the members of the Editorial and Technical Teams on a page in the documentation ( example ). Note that it is a good idea to be careful of who on your team has \"admin\" rights on your repo, so sometimes, a distinct \"admin\" team can be very helpful. Admins are allowed to do \"dangerous\" things like deleting the repository. two distinct project boards . We recommend two distinct project boards, one for the Curation/Editorial Team, and one for the Technical Team. The details on how to design the boards is up to the respective teams, but we found a simple 4 stage board with sections for To Do (issues that are important but not urgent), Priority (issues that are important and urgent), In Progress (issues that are being worked on) and Under review (issues that need review). From years of experience with project boards, we recommend against the common practice of keeping a Backlog column (issues that are neither important nor urgent nor likely to be addressed in the next 6 months), nor a Done column (to keep track of closed issues) - they just clutter the view. A documentation system (often realised using mkdocs in OBO projects) with a page listing the members of the team ( example ). This page should provide links to all related team pages from Github and their project boards, as well as a table listing all current team members with the following information: Name ORCiD Funding Information Allocated FTEs (0 if on volunteering basis) Associated teams Role Responsibilities (What kind of issues can they be assigned to review? How are they involved in the Project?) Responsibilities \u00b6 Effective Ontology Pipeline Developers (OPDs) are extremely rare and are typically active across many different projects. Therefore their attention is scattered . Understanding and accepting this is key for the following points. Principal Investigators explicitly assign target weekly hours for Ontology Editors and Ontology Pipeline Developers to the project. These should be captured on the documentation systems team page (see above). The Ontology Editors are responsible for the entire Curation Team Board and the To Do and Priority columns of the Technical Team. The later is important: it is the job of the curation team to prioritise the technical issues . The Technical Team can add tickets to the To Do and Priority columns, but this usually happens only in response to a request from the Curation Team. When the technical team meets, the Principal Ontology Editor(s) (POE) are present, i.e. the POEs are members of the technical team as well . They will help clarifying the Priority tickets. The Technical Team is responsible to assign issues and reviewers among themselves (ideally, the reviewer should be decided at the same time the issue is assigned) move issues from the Priority to the In Progress and later to the Done section. communicate through the POE to the PIs when resources are insufficient to address Priority issues. The Principal Ontology Editor is responsible for ensuring that new issues on the issue tracker are dealt with. Usually this happens in the following ways: They ensure that each external issue (i.e. an issue from anyone outside the core team) is (a) responded to in a polite manner and (b) assigned to someone appropriate or politely rejected due to lack of resources. They ensure that each internal issue is assigned to the person that made them. No issue should appear unassigned. The ensure that pull requests are (a) assigned to someone to handle and (b) merged in a timely manner. Too many open PRs cause problems with conflicts. Best Practices \u00b6 The To Do issues should first be moved to the Priority section before being addressed. This prevents focusing on easy to solve tickets in favour of important ones. Even if Google Docs are used to manage team meetings, at the end of each meetings all open issues must be captured as GitHub tickets and placed in the appropriate box on the board. We recommend that Backlog items are not added at all to the board - if they ever become important, they tend to resurface all by themselves. The single most important point of failure is the absence of an Principal Ontology Editor with a strong sense of ownership . This should be the projects priority to determine first. All new members of the project should undergo an onboarding. It is a good idea to prepare walkthroughs of the project (as video or pages in the documentation system) covering everything from Curation to Technical and Project Management. The Principal Ontology Editor responsible for dealing with external issues should be named explicitly on the team page. We recommend the following practices for write permissions: The main (formerly master ) branch should be write protected with suitable rules. For example, requiring QC to pass and 1 approving review as a minimum. The curation and technical teams are mainly for social organisation, they do not have to physically exist. However, having a small team with \"admin rights\" and a team (e.g. the core team mentioned above) with \"write\" rights greatly helps with organising the permissions in a transparent manner.","title":"Managing OBO ontology projects"},{"location":"tutorial/managing-ontology-project/#tutorial-on-managing-obo-ontology-projects","text":"This tutorial is not about editing ontologies and managing the evolution of its content (aka ontology curation), but the general process of managing an ontology project overall. In this lesson, we will cover the following: How to effectively manage an ontology project using GitHub projects and teams How to coordinate the evolution of ontologies across projects and grants It is important to understand that the following is just one good way of doing project management for OBO ontologies, and most projects will do it slightly differently. We do however believe that thinking about your project management process and the roles involved will benefit your work in the long term, and hope that the following will help you as a starting point.","title":"Tutorial on Managing OBO Ontology Projects"},{"location":"tutorial/managing-ontology-project/#roles-in-obo-ontology-project-management-activities","text":"Ontology Editor (OE): manage the content of ontologies and interact with users Principal Ontology Editor (POE): coordinate the curation activities and have always fixed hours assigned to the project. Ontology Pipeline Developer (OPD): Manage the technical workflows around ontologies, such as release workflows, continuous integration and QC, and setting up data pipelines. Also helps with bulk editing activities. Principal Investigators (PI): Manage the projects that fund ontology curation activities. For an effective management of an ontology, the following criteria are recommended: There should be at least one Principal Ontology Editor for every ontology project. The importance is not whether this editor (or sometimes called 'ontology curator') has a specific number of hours per week allocated to the project (although based on our experience, 1 day per week is minimum), but whether the editor has a sense of ownership, i.e. they understand that they are the primary responsible person for maintaining the ontology moving forward. Because of potential grant overlapping issues, we recommend to have at least 1 Principal Ontology Editor for every grant/funded project that has a stake in the ontology. Every effective ontology needs at least a few hours per week from an Ontology Pipeline Developer (OPD). More on that role later. The OPD does not always have as strong a sense of ownership of the ontology project, but typically has a strong sense of responsibility to members of the curation team. There should be separate meetings for curation and technical activities - both problems are hard, and need different team members being present. We recommend at least monthly technical and biweekly curation calls, but for many of the most effective ontology projects we manage, weekly technical and weekly curation calls are normal. Without the above minimum criteria, the following recommendations will be very hard to implement.","title":"Roles in OBO Ontology project management activities"},{"location":"tutorial/managing-ontology-project/#the-project-management-toolbox","text":"We make use of three tools in the following recommendation: Project boards : Project boards, sometimes referred to as Kanban boards, GitHub boards or agile boards, are a great way to organise outstanding tickets and help maintain a clear overview of what work needs to be done. They are usually realised with either GitHub projects or ZenHub . If you have not worked with project boards before, we highly recommend watching a quick tutorial on Youtube, such as: GitHub teams . GitHub teams , alongside with organisations, are a powerfull too to organise collaborative workflows on GitHub. They allow you to communicate and organise permissions for editing your ontology in a transparent way. You can get a sense of GitHub teams by watching one of the the numerous tutorials on GitHub, such as: Markdown-based documentation system . Writing great documentation is imperative for a sustainable project. Across many of our recent projects, were are using mkdocs , which we have also integrated with the Ontology Development Kit, but there are others to consider. We deeply recommend to complete a very short introduction to Markdown, this tutorial on YouTube .","title":"The Project Management Toolbox"},{"location":"tutorial/managing-ontology-project/#what-do-you-need-for-your-project","text":"Every ontology or group of related ontologies (sometimes it is easier to manage multiple ontologies at once, because their scope or technical workflows are quite uniform or they are heavily interrelated) should have: at least two teams , an Editorial Team and a Technical Team, with clearly defined members. We recommend to create two teams on GitHub and keep their members always up to date (i.e. remove members that are not actively participating), but many of our projects merely maintain a \"core team\", which is a more liberal team containing everyone from stakeholders, principal investigators, editors and users (for managing write permissions see later in the \"best practice\" section) and listing the members of the Editorial and Technical Teams on a page in the documentation ( example ). Note that it is a good idea to be careful of who on your team has \"admin\" rights on your repo, so sometimes, a distinct \"admin\" team can be very helpful. Admins are allowed to do \"dangerous\" things like deleting the repository. two distinct project boards . We recommend two distinct project boards, one for the Curation/Editorial Team, and one for the Technical Team. The details on how to design the boards is up to the respective teams, but we found a simple 4 stage board with sections for To Do (issues that are important but not urgent), Priority (issues that are important and urgent), In Progress (issues that are being worked on) and Under review (issues that need review). From years of experience with project boards, we recommend against the common practice of keeping a Backlog column (issues that are neither important nor urgent nor likely to be addressed in the next 6 months), nor a Done column (to keep track of closed issues) - they just clutter the view. A documentation system (often realised using mkdocs in OBO projects) with a page listing the members of the team ( example ). This page should provide links to all related team pages from Github and their project boards, as well as a table listing all current team members with the following information: Name ORCiD Funding Information Allocated FTEs (0 if on volunteering basis) Associated teams Role Responsibilities (What kind of issues can they be assigned to review? How are they involved in the Project?)","title":"What do you need for your project?"},{"location":"tutorial/managing-ontology-project/#responsibilities","text":"Effective Ontology Pipeline Developers (OPDs) are extremely rare and are typically active across many different projects. Therefore their attention is scattered . Understanding and accepting this is key for the following points. Principal Investigators explicitly assign target weekly hours for Ontology Editors and Ontology Pipeline Developers to the project. These should be captured on the documentation systems team page (see above). The Ontology Editors are responsible for the entire Curation Team Board and the To Do and Priority columns of the Technical Team. The later is important: it is the job of the curation team to prioritise the technical issues . The Technical Team can add tickets to the To Do and Priority columns, but this usually happens only in response to a request from the Curation Team. When the technical team meets, the Principal Ontology Editor(s) (POE) are present, i.e. the POEs are members of the technical team as well . They will help clarifying the Priority tickets. The Technical Team is responsible to assign issues and reviewers among themselves (ideally, the reviewer should be decided at the same time the issue is assigned) move issues from the Priority to the In Progress and later to the Done section. communicate through the POE to the PIs when resources are insufficient to address Priority issues. The Principal Ontology Editor is responsible for ensuring that new issues on the issue tracker are dealt with. Usually this happens in the following ways: They ensure that each external issue (i.e. an issue from anyone outside the core team) is (a) responded to in a polite manner and (b) assigned to someone appropriate or politely rejected due to lack of resources. They ensure that each internal issue is assigned to the person that made them. No issue should appear unassigned. The ensure that pull requests are (a) assigned to someone to handle and (b) merged in a timely manner. Too many open PRs cause problems with conflicts.","title":"Responsibilities"},{"location":"tutorial/managing-ontology-project/#best-practices","text":"The To Do issues should first be moved to the Priority section before being addressed. This prevents focusing on easy to solve tickets in favour of important ones. Even if Google Docs are used to manage team meetings, at the end of each meetings all open issues must be captured as GitHub tickets and placed in the appropriate box on the board. We recommend that Backlog items are not added at all to the board - if they ever become important, they tend to resurface all by themselves. The single most important point of failure is the absence of an Principal Ontology Editor with a strong sense of ownership . This should be the projects priority to determine first. All new members of the project should undergo an onboarding. It is a good idea to prepare walkthroughs of the project (as video or pages in the documentation system) covering everything from Curation to Technical and Project Management. The Principal Ontology Editor responsible for dealing with external issues should be named explicitly on the team page. We recommend the following practices for write permissions: The main (formerly master ) branch should be write protected with suitable rules. For example, requiring QC to pass and 1 approving review as a minimum. The curation and technical teams are mainly for social organisation, they do not have to physically exist. However, having a small team with \"admin rights\" and a team (e.g. the core team mentioned above) with \"write\" rights greatly helps with organising the permissions in a transparent manner.","title":"Best Practices"},{"location":"tutorial/managing-ontology-releases-odk/","text":"Managing Ontology Releases with the Ontology Development Kit \u00b6 In this tutorial, we discuss the general workflow of ontology releases. Tutorial \u00b6 Follow instructions for the PATO release process here .","title":"ODK - Managing ontology releases"},{"location":"tutorial/managing-ontology-releases-odk/#managing-ontology-releases-with-the-ontology-development-kit","text":"In this tutorial, we discuss the general workflow of ontology releases.","title":"Managing Ontology Releases with the Ontology Development Kit"},{"location":"tutorial/managing-ontology-releases-odk/#tutorial","text":"Follow instructions for the PATO release process here .","title":"Tutorial"},{"location":"tutorial/migrating-ontology-to-odk/","text":"Migrating your old Ontology Release System to the Ontology Development Kit \u00b6 Content TBP, recording exists on request.","title":"ODK - Migrating to ODK"},{"location":"tutorial/migrating-ontology-to-odk/#migrating-your-old-ontology-release-system-to-the-ontology-development-kit","text":"Content TBP, recording exists on request.","title":"Migrating your old Ontology Release System to the Ontology Development Kit"},{"location":"tutorial/robot_tutorial_1/","text":"ROBOT Mini-Tutorial 1: Convert, Extract and Template \u00b6 Last week, we were introduced to ROBOT for quality control and generating reports about terms in an ontology. This week, we will learn about three new ROBOT commands: * Convert * Extract * Template Before starting this tutorial, either: * make sure Docker is running and you are in the container * download and install ROBOT for your operating system We will be using the files from the Ontologies 101 Tutorial . In your terminal, navigate to the repository that you cloned and then into the BDK14_exercises folder. Convert \u00b6 So far, we have been saving our ontologies in Protege using the default RDF/XML syntax, but there are many flavors of OWL. We will discuss each of these serializations in more detail during the class session, but ROBOT supports the following: owl - RDF/XML owx - OWL/XML ttl - Turtle obo - OBO Format ofn - OWL Functional omn - OWL Manchester json - obographs JSON Let's Try It! \u00b6 Navigate to the basic-subclass/ folder. Open chromosome-parts.owl in your text editor and you will see it's in RDF/XML format. We're going to take this file and convert it to Turtle ( ttl ) serialization. Return to your terminal and enter the following command: robot convert --input chromosome-parts.owl --format ttl --output chromosome-parts.ttl ROBOT convert is smart about detecting formats, so since the output file ends with .ttl , the --format ttl parameter isn't really required. If you wanted to use a different file ending, say .owl , you will need to include the format flag to force ROBOT to write Turtle. Now open chromosome-parts.ttl in your text editor and see what's changed! RDF/XML and Turtle are very different serializations, but the actual data that is contained in these two files is exactly the same. On Your Own \u00b6 Convert chromosome-parts.owl into the following formats: obo (OBO Format), ofn (OWL Functional), and omn (OWL Manchester). Open each file and take a minute to scroll through (we don't expect you to be able to read these, they're mostly meant for computers!) Why do you think we need these different serializations? What do you think the purpose of OWL Manchester vs. RDF/XML is? Extract \u00b6 Sometimes we only want to browse or share a subset of an ontology, especially with some of the larger OBO Foundry ontologies. There are two main methods for creating subsets: MIREOT SLME Right now, we will use use MIREOT and talk more about SLME in our class session. MIREOT makes sure that you have the minimal amount of information you need to reuse an existing ontology term. It allows us to extract a small portion of the class hierarchy by specifying upper and lower boundaries, which you will see in the example below. We need to know the identifiers (as CURIEs) of the terms that we want to set as our boundaries. Let's Try It! \u00b6 Open chromosome-parts.owl in Protege and open the Class hierarchy. We are going to create a subset relevant to the term \"chromosome\". First, we will find the CURIE of our desired term. Search for \"chromosome\" and find the \"id\" annotation property. This will be our lower term. Right now, we won't set an upper boundary. That means this subset will go all the way up to the top-level ancestor of \"chromosome\". Return to your terminal and enter the following command (where the --lower-term is the CURIE that we just found): robot extract --method MIREOT --input chromosome-parts.owl --lower-term GO:0005694 --output chromosome-full.owl Now open chromosome-full.owl in Protege and open the Class hierarchy. When you open the \"cellular_component\" node, you'll notice that most of the terms are gone! Both \"organelle\" and \"intracellular part\" remain because they are in the path between \"chromosome\" and the top-level \"cellular_component\". Keep clicking down and you'll find \"chromosome\" at the very bottom. Since \"chromosome\" has two named parents, both of those parents are included, which is why we ended up with \"organelle\" and \"intracellular part\". Now let's try it with an upper term. This time, we want \"organelle\" to be the upper boundary. Find the CURIE for \"organelle\". Return to your terminal and enter the following command (where the --upper-term is the new CURIE we just found): robot extract --method MIREOT \\ --input chromosome-parts.owl \\ --lower-term GO:0005694 \\ --upper-term GO:0043226 \\ --output chromosome.owl Open chromosome.owl and again return to the Class hierarchy. This time, we see \"organelle\" directly below owl:Thing . \"intracellular part\" is also now missing because it does not fall under \"organelle\". On Your Own \u00b6 Play with different upper- and lower-level terms to create different subsets Compare the terms that are in the subsets to the terms in the original chromosome-parts.owl file. What is missing from the terms in the subsets? What has been included as our \"minimal\" information? Template \u00b6 Most of the knowledge encapsulated in ontologies comes from domain experts. Often, these domain experts are not computer scientists and are not familiar with the command line. Luckily, most domain experts are familiar with spreadsheets! ROBOT provides a way to convert spreadsheets into OWL ontologies using template strings . We'll get more into these during the class session, but if you want to get a head start, they are all documented here . Essentially, the first row of a ROBOT template is a human-readable header. The second row is the ROBOT template string. Each row below that represents an entity to be created in the output ontology. We can create new entities by giving them new IDs, but we can also reference existing entities just by label. For now, we're going to create a new, small ontology with new terms using a template. Let's Try It! \u00b6 Download (or copy/paste) the animals.tsv file and move it to the basic-subclass/ folder (or whatever folder you would like to work in; we will not be using any of the Ontology 101 files anymore). This contains the following data: CURIE Label Parent Comment ID LABEL SC % A rdfs:comment obo:0000001 animal Any animal in the world. obo:0000002 canine animal A member of the genus Canis. obo:0000003 feline animal A member of the genus Felis. In the first column, we use the special ID keyword to say that this is our term's unique identifier. The second column contains the LABEL keyword which is a shortcut for the rdfs:label annotation property. The third column uses the SC keyword to state that this column will be a subclass statement. The % sign is replaced by the value in the cell. We'll talk more about this keyword and the % symbol during the class session. Finally, the last column begins with A to denote that this will be an annotation, and then is followed by the annotation property we're using. Just looking at the template, you can begin to predict what a class hierarchy using these terms would look like in an ontology. We can turn this into reality! In your terminal, enter the following command: robot template --template animals.tsv --output animals.owl Note that in this command, we don't use the --input parameter. That parameter is reserved for input ontologies, and we are not using one right now. More on this later. Open animals.owl in Protege, and you'll be able to see the class hierarchy we defined in the template as an actual structure. Now let's make another small ontology that reuses some terms from our animals.owl file. Download (or copy/paste) animals2.tsv into the same folder. This contains the following: CURIE Label Parent Comment ID LABEL SC % A rdfs:comment obo:0000004 dog canine A member of the subspecies Canis lupus familiaris. obo:0000005 cat feline A member of the species Felis catus. You'll notice that we are referencing two terms from our other spreadsheet in this one. In your terminal, enter the following command: robot template --input animals.owl --template animals2.tsv --output animals2.owl This time, we did use the --input parameter and provided the animals ontology we just created. This allows us to use any term in the animals.owl file in our animals2.tsv template and ROBOT will know what we're talking about. Go ahead and open animals2.owl in Protege. What's missing? The parent classes for \"dog\" and \"cat\" don't have labels, and the \"animal\" term is missing entirely. This is because, even though ROBOT knew about these classes, we didn't ask for the original ontology to be included in the output, so no axioms from that ontology can be found in this newly-created one. Next week, we'll learn about combining ontologies with the Merge command. For now, let's add the original animals.owl file as an import: 1. Go to the \"Active ontology\" tab and find the \"Imported ontologies\" section at the bottom 2. Click the + next to \"Direct imports\" 3. Select \"Import an ontology contained in a local file\" and click Continue 4. Browse for the path to animals.owl , click Continue, and then click Finish Prot\u00e9g\u00e9 will now load animals.owl as an import. When you return to the Entities tab, you'll see all those upper-level terms. Note the difference in how the terms are displayed in the class hierarchy. On Your Own \u00b6 Try adding another class or two to the animals.tsv template and regenerating animals.owl . Can you create your own template?","title":"ROBOT Mini-Tutorial 1 - Convert, Extract and Template"},{"location":"tutorial/robot_tutorial_1/#robot-mini-tutorial-1-convert-extract-and-template","text":"Last week, we were introduced to ROBOT for quality control and generating reports about terms in an ontology. This week, we will learn about three new ROBOT commands: * Convert * Extract * Template Before starting this tutorial, either: * make sure Docker is running and you are in the container * download and install ROBOT for your operating system We will be using the files from the Ontologies 101 Tutorial . In your terminal, navigate to the repository that you cloned and then into the BDK14_exercises folder.","title":"ROBOT Mini-Tutorial 1: Convert, Extract and Template"},{"location":"tutorial/robot_tutorial_1/#convert","text":"So far, we have been saving our ontologies in Protege using the default RDF/XML syntax, but there are many flavors of OWL. We will discuss each of these serializations in more detail during the class session, but ROBOT supports the following: owl - RDF/XML owx - OWL/XML ttl - Turtle obo - OBO Format ofn - OWL Functional omn - OWL Manchester json - obographs JSON","title":"Convert"},{"location":"tutorial/robot_tutorial_1/#lets-try-it","text":"Navigate to the basic-subclass/ folder. Open chromosome-parts.owl in your text editor and you will see it's in RDF/XML format. We're going to take this file and convert it to Turtle ( ttl ) serialization. Return to your terminal and enter the following command: robot convert --input chromosome-parts.owl --format ttl --output chromosome-parts.ttl ROBOT convert is smart about detecting formats, so since the output file ends with .ttl , the --format ttl parameter isn't really required. If you wanted to use a different file ending, say .owl , you will need to include the format flag to force ROBOT to write Turtle. Now open chromosome-parts.ttl in your text editor and see what's changed! RDF/XML and Turtle are very different serializations, but the actual data that is contained in these two files is exactly the same.","title":"Let's Try It!"},{"location":"tutorial/robot_tutorial_1/#on-your-own","text":"Convert chromosome-parts.owl into the following formats: obo (OBO Format), ofn (OWL Functional), and omn (OWL Manchester). Open each file and take a minute to scroll through (we don't expect you to be able to read these, they're mostly meant for computers!) Why do you think we need these different serializations? What do you think the purpose of OWL Manchester vs. RDF/XML is?","title":"On Your Own"},{"location":"tutorial/robot_tutorial_1/#extract","text":"Sometimes we only want to browse or share a subset of an ontology, especially with some of the larger OBO Foundry ontologies. There are two main methods for creating subsets: MIREOT SLME Right now, we will use use MIREOT and talk more about SLME in our class session. MIREOT makes sure that you have the minimal amount of information you need to reuse an existing ontology term. It allows us to extract a small portion of the class hierarchy by specifying upper and lower boundaries, which you will see in the example below. We need to know the identifiers (as CURIEs) of the terms that we want to set as our boundaries.","title":"Extract"},{"location":"tutorial/robot_tutorial_1/#lets-try-it_1","text":"Open chromosome-parts.owl in Protege and open the Class hierarchy. We are going to create a subset relevant to the term \"chromosome\". First, we will find the CURIE of our desired term. Search for \"chromosome\" and find the \"id\" annotation property. This will be our lower term. Right now, we won't set an upper boundary. That means this subset will go all the way up to the top-level ancestor of \"chromosome\". Return to your terminal and enter the following command (where the --lower-term is the CURIE that we just found): robot extract --method MIREOT --input chromosome-parts.owl --lower-term GO:0005694 --output chromosome-full.owl Now open chromosome-full.owl in Protege and open the Class hierarchy. When you open the \"cellular_component\" node, you'll notice that most of the terms are gone! Both \"organelle\" and \"intracellular part\" remain because they are in the path between \"chromosome\" and the top-level \"cellular_component\". Keep clicking down and you'll find \"chromosome\" at the very bottom. Since \"chromosome\" has two named parents, both of those parents are included, which is why we ended up with \"organelle\" and \"intracellular part\". Now let's try it with an upper term. This time, we want \"organelle\" to be the upper boundary. Find the CURIE for \"organelle\". Return to your terminal and enter the following command (where the --upper-term is the new CURIE we just found): robot extract --method MIREOT \\ --input chromosome-parts.owl \\ --lower-term GO:0005694 \\ --upper-term GO:0043226 \\ --output chromosome.owl Open chromosome.owl and again return to the Class hierarchy. This time, we see \"organelle\" directly below owl:Thing . \"intracellular part\" is also now missing because it does not fall under \"organelle\".","title":"Let's Try It!"},{"location":"tutorial/robot_tutorial_1/#on-your-own_1","text":"Play with different upper- and lower-level terms to create different subsets Compare the terms that are in the subsets to the terms in the original chromosome-parts.owl file. What is missing from the terms in the subsets? What has been included as our \"minimal\" information?","title":"On Your Own"},{"location":"tutorial/robot_tutorial_1/#template","text":"Most of the knowledge encapsulated in ontologies comes from domain experts. Often, these domain experts are not computer scientists and are not familiar with the command line. Luckily, most domain experts are familiar with spreadsheets! ROBOT provides a way to convert spreadsheets into OWL ontologies using template strings . We'll get more into these during the class session, but if you want to get a head start, they are all documented here . Essentially, the first row of a ROBOT template is a human-readable header. The second row is the ROBOT template string. Each row below that represents an entity to be created in the output ontology. We can create new entities by giving them new IDs, but we can also reference existing entities just by label. For now, we're going to create a new, small ontology with new terms using a template.","title":"Template"},{"location":"tutorial/robot_tutorial_1/#lets-try-it_2","text":"Download (or copy/paste) the animals.tsv file and move it to the basic-subclass/ folder (or whatever folder you would like to work in; we will not be using any of the Ontology 101 files anymore). This contains the following data: CURIE Label Parent Comment ID LABEL SC % A rdfs:comment obo:0000001 animal Any animal in the world. obo:0000002 canine animal A member of the genus Canis. obo:0000003 feline animal A member of the genus Felis. In the first column, we use the special ID keyword to say that this is our term's unique identifier. The second column contains the LABEL keyword which is a shortcut for the rdfs:label annotation property. The third column uses the SC keyword to state that this column will be a subclass statement. The % sign is replaced by the value in the cell. We'll talk more about this keyword and the % symbol during the class session. Finally, the last column begins with A to denote that this will be an annotation, and then is followed by the annotation property we're using. Just looking at the template, you can begin to predict what a class hierarchy using these terms would look like in an ontology. We can turn this into reality! In your terminal, enter the following command: robot template --template animals.tsv --output animals.owl Note that in this command, we don't use the --input parameter. That parameter is reserved for input ontologies, and we are not using one right now. More on this later. Open animals.owl in Protege, and you'll be able to see the class hierarchy we defined in the template as an actual structure. Now let's make another small ontology that reuses some terms from our animals.owl file. Download (or copy/paste) animals2.tsv into the same folder. This contains the following: CURIE Label Parent Comment ID LABEL SC % A rdfs:comment obo:0000004 dog canine A member of the subspecies Canis lupus familiaris. obo:0000005 cat feline A member of the species Felis catus. You'll notice that we are referencing two terms from our other spreadsheet in this one. In your terminal, enter the following command: robot template --input animals.owl --template animals2.tsv --output animals2.owl This time, we did use the --input parameter and provided the animals ontology we just created. This allows us to use any term in the animals.owl file in our animals2.tsv template and ROBOT will know what we're talking about. Go ahead and open animals2.owl in Protege. What's missing? The parent classes for \"dog\" and \"cat\" don't have labels, and the \"animal\" term is missing entirely. This is because, even though ROBOT knew about these classes, we didn't ask for the original ontology to be included in the output, so no axioms from that ontology can be found in this newly-created one. Next week, we'll learn about combining ontologies with the Merge command. For now, let's add the original animals.owl file as an import: 1. Go to the \"Active ontology\" tab and find the \"Imported ontologies\" section at the bottom 2. Click the + next to \"Direct imports\" 3. Select \"Import an ontology contained in a local file\" and click Continue 4. Browse for the path to animals.owl , click Continue, and then click Finish Prot\u00e9g\u00e9 will now load animals.owl as an import. When you return to the Entities tab, you'll see all those upper-level terms. Note the difference in how the terms are displayed in the class hierarchy.","title":"Let's Try It!"},{"location":"tutorial/robot_tutorial_1/#on-your-own_2","text":"Try adding another class or two to the animals.tsv template and regenerating animals.owl . Can you create your own template?","title":"On Your Own"},{"location":"tutorial/robot_tutorial_2/","text":"ROBOT Tutorial 2: Annotate, Merge, Reason and Diff \u00b6 In week 6, we got some hands-on experience with ROBOT using convert , extract , and template . This week, we will learn four new ROBOT commands: * Annotate * Merge * Reason * Diff The goal of these and previous commands is to build up to creating an ontology release workflow. Before starting this tutorial, either: * make sure Docker is running and you are in the container * download and install ROBOT for your operating system To start, we will be working in the same folder as the first ROBOT Mini-Tutorial . Navigate to this folder in your terminal and list the contents of the current directory by running ls . You should see catalog-v001.xml listed as one of these files. We want to delete this so that we can fix the ontology IRI problem we ran into last week! Before going any further with this tutorial, do this by running either del catalog-v001.xml for Windows or rm catalog-v001.xml if you're using Docker, MacOS, or other Linux system. Annotate \u00b6 The annotate command allows you to attach metadata to your ontology in the form of IRIs and ontology annotations. Like the annotations on a term, ontology annotations help users to understand how they can use the ontology. Ontology IRIs \u00b6 As we discussed during previous parts of the course, ontology IRIs are very important! We saw how importing an ontology without an IRI into another ontology without an IRI can cause some problems in the catalog-v001.xml file. We're going to fix that problem by giving IRIs to both our animals.owl and animals2.owl files. Let's start with animals.owl : robot annotate --input animals.owl \\ --ontology-iri http://example.com/animals.owl \\ --output animals.owl You'll notice we gave the same file name as the input file; we're just updating our previous file so we don't need to do this in a separate OWL file. On your own, give animals2.owl the ontology IRI http://example.com/animals2.owl . Remember that, in reality, we always want our ontology IRIs to be resolvable , so these would be pretty bad IRIs for an actual ontology. Let's fix our import statement now. Open animals2.owl in Prot\u00e9g\u00e9 and go to the Entities tab. You'll see that even though we still have the import statement in the Active ontology tab, the top-level terms are no longer labeled. Since we changed the ontology IRI, Prot\u00e9g\u00e9 can no longer resolve our local file (because the catalog-v001.xml file was not updated). Go back to the Active ontology tab and click the X to the right of our original import. Then, re-add animals.owl as an import using the same steps as last time. When you return to the Entities tab, you'll once again see the labels of the top-level terms. Version IRIs \u00b6 When we release our ontologies, we want to make sure to include a version IRI. Like the ontology IRI, this should always resolve to the version of the ontology at the time of the release. For clarity, we usually use dates in our version IRIs in the OBO Foundry. That way, you know when you navigate to a specific version IRI, that's what the ontology looked like on that date. (Note: edit files don't usually have version IRIs as they are always changing, and we don't expect to be able to point to a stable version) While you can add a version IRI in Prot\u00e9g\u00e9, if you're trying to create an automated release workflow, this is a manual step you don't want to have to include. Keeping it in your release workflow also makes sure that the verion IRIs are consistent (we'll see how to do this with make later). For now, let's add a version IRI to animals.owl (feel free to replace the 2021-05-20 with today's date): robot annotate --input animals.owl \\ --version-iri http://example.com/animals/2021-05-20/animals.owl \\ --output animals.owl Let's break down this version IRI. We have the host ( http://example.com/ ) followed by our ontology's namespace ( animals ). Next, we provided the date in the format of YYYY-MM-DD . Finally, we have the name of the file. This is standard for OBO Foundry, except with a different host. For example, you can find a release of OBI from April 6, 2021 at http://purl.obolibrary.org/obo/obi/2021-04-06/obi.owl . In this case, the host is http://purl.obolibrary.org/obo/ . Of course, you may see different patterns in non-OBO-Foundry ontologies, but they should always resolve (hopefully!). Go ahead and open or reload animals.owl in Protege. You'll see in the Active Ontology tab that now both the ontology IRI and version IRI fields are filled out. Ontology Annotations \u00b6 In addition to ontology and version IRIs, you may also want to add some other metadata to your ontology. For example, when we were introduced to report , we added a description to the ontology to fix one of the report problems. The three ontology annotations that are required by the OBO Foundry are: * Title ( dc11:title ) * License ( dc:license ) * Description ( dc11:description ) These three annotation properties all come from the Dublin Core , but they have slightly different namespaces. This is because DC is split into two parts: the /terms/ and /elements/1.1/ namespaces. Just remember to double check that you're using the correct namespace. If you click on the DC link, you can find the complete list of DC terms in their respective namespaces. ROBOT contains some built-in prefixes, which can be found here . The prefix dc: corresponds to the /terms/ namespace and dc11: to /elements/1.1/ . You may see different prefixes used (for example, /terms/ is sometimes dcterms: or just terms: ), but the full namespace is what really matters as long as the prefix is defined somewhere. Let's go ahead and add a title and description to our animals.owl file. We'll do this using the --annotation option, which expects two arguments: (1) the CURIE of the annotation property, (2) the value of the annotation. The value of the annotation must be enclosed in double quotes if there are spaces. You can use any annotation property you want here, and include as many as you want! For now, we'll start with two: robot annotate --input animals.owl \\ --annotation dc11:title \"Animal Ontology\" \\ --annotation dc11:description \"An ontology about animals\" \\ --output animals.owl --annotation adds these as strings, but remember that an annotation can also point to an link or IRI. We want our license to be a link, so we'll use --link-annotation instead to add that: robot annotate --input animals.owl \\ --link-annotation dc:license https://creativecommons.org/licenses/by/4.0/ \\ --output animals.owl OBO Foundry recommends using Creative Commons for all licenses. We just gave our ontology the most permissive of these, CC-BY . When you open animals.owl in Prot\u00e9g\u00e9 again, you'll see these annotations added to the Active ontology tab. You can also click on the CC-BY link! Merge \u00b6 We've already learned how to include external ontologies as imports. Usually, for the released version of an ontology, the imports are merged in so that all contents are in one file. Another reason you may want to merge two ontologies is if you're adding new terms to an ontology using template , like how we created new animal terms in animals2.tsv last time. We're going to demonstrate two methods of merging now. The first involves merging two (or more!) separate files and the second involves merging all imports into the current input ontology. Merging Multiple Files \u00b6 First, copy animals2.owl to animals-new.owl . In Windows, this command is copy animals2.owl animals-new.owl . For Docker and other Linux operating systems, this is cp animals2.owl animals-new.owl . Open animals-new.owl in Prot\u00e9g\u00e9 and remove the import we added last time. This is done in the Imported ontologies section of the Active ontology tab. Just click the X on the right side of the imported animals ontology. Don't forget to save! Continuing with the animals.owl file we created last week, now run the following command: robot merge --input animals.owl --input animals-new.owl --output animals-full.owl When you just import an external ontology into your ontology, you'll notice in the Prot\u00e9g\u00e9 class hierarchy that all terms from the external ontology are a less-bold text than internal terms. This can be seen when you open animals2.owl , where we imported animals.owl . This is simply Prot\u00e9g\u00e9's way of telling us that these terms are not part of your current ontology . Now that we've merged these two ontologies together, when you open animals-full.owl in Prot\u00e9g\u00e9, you'll see that all the terms are bold. By default, the output ontology will get the ontology IRI of the first input ontology. We picked animals.owl as our first ontology here because this is the ontology that we're adding terms to, so we want our new output ontology to replace the original while keeping the same IRI. merge will also copy over all the ontology annotations from animals.owl (the first input) into the new file. The annotations from animals2.owl are ignored, but we'll talk more about this in our class session. If we were editing an ontology in the wild, we'd probably now replace the original with this new file using cp or copy . For now, don't replace animals.owl because we'll need it for this next part. IMPORTANT : Be very careful to check that the format is the same if you're replacing a file! Remember, you can always output OWL Functional syntax or another syntax by ending your output with .ofn , for example: --output animals-full.ofn . Merging Imports \u00b6 When we want to merge all our imports into our working ontology, we call this collapsing the import closure . Luckily (since we're lazy), you don't need to type out each of your imports as an input to do this. We already have animals.owl imported into animals2.owl . Let's collapse the import closure: robot merge --input animals2.owl --collapse-import-closure true --output animals-full-2.owl Even though we gave this a different file name, if you open animals-full-2.owl in Prot\u00e9g\u00e9, you'll notice that it's exactly the same as animals-full.owl ! This is because we merged the same files together, just in a slightly different way. This time, though, the ontology IRI is the one for animals2.owl , not animals.owl . That is because that was our first input file. Reason \u00b6 As we saw in the prepwork for Week 5, running a reasoner in Prot\u00e9g\u00e9 creates an inferred class hierarchy. In the OBO Foundry, releases versions of ontologies usually have this inferred hierarchy asserted , so you see the full inferred hierarchy when you open the ontology without running the reasoner. ROBOT reason allows us to output a version of the ontology with these inferences asserted. As we discussed, ELK and HermiT are the two main reasoners you'll be using. Instead of using our example ontologies (the asserted and inferred hierarchies for these will look exactly the same), we're going to use another ontology from the Ontologies 101 tutorial from week 5. Navigate back to that directory and then navigate to BDK14_exercises/basic-classification . Like running the reasoner in Prot\u00e9g\u00e9, running reason does three things: 1. Check for inconsistency 2. Check for unsatisfiable classes 3. Assert the inferred class hierarchy Remember, when we run the reasoner in Prot\u00e9g\u00e9, if the ontology is inconsistent, reason will fail. If there are unsatisfiable classes, these will be asserted as owl:Nothing . ROBOT will always fail in both cases, but has some tools to help us figure out why. Let's introduce an unsatifiable class into our test and see what happens. First, let's make a copy of ubiq-ligase-complex.owl and call this new file unreasoned.owl ( copy or cp ). Open unreasoned.owl in Prot\u00e9g\u00e9 and follow the steps below. These are things we've covered in past exercises, but if you get stuck, please don't hesitate to reach out. Find 'organelle' in the class hierarchy below 'cellular_component' (or just search for it by label) Make 'organelle' disjoint with 'organelle part' (either use the class hierarchy or type it in the expression editor) Find 'intracellular organelle part' below 'intracellular part' or 'organelle part' (or search for it by label) Add 'organelle' as a parent class to 'intracellular organelle part' (remember that you only need to include the single quotes if the label has spaces) Like we did in the Disjointness part of the Ontologies 101 tutorial, we've made 'intracellular organelle part' a subclass of two classes that should have no overlap based on the disjointness axiom. Save the ontology and return to your terminal. Now, we'll run reason . The default reasoner is ELK, but you can specify the reasoner you want to use with the --reasoner option. For now, we'll just use ELK. robot reason --input unreasoned.owl --output unsatisfiable.owl You'll notice that ROBOT printed an error message telling us that the term with the IRI http://purl.obolibrary.org/obo/GO_0044446 is unsatisfiable and ROBOT didn't create unsatisfiable.owl . This is ideal for automated pipelines where we don't want to be releasing unsatisfiable classes. We can still use ROBOT to investigate the issue, though. It already gave us the IRI, but we can get more details using the --dump-unsatisfiable option. We won't provide an output this time because we know it won't succeed. robot reason --input unreasoned.owl --dump-unsatisfiable unsatisfiable.owl You can open unsatisfiable.owl in Prot\u00e9g\u00e9 and see that 'intracellular organelle part' is not the only term included, even though it was the only unsatisfiable class. Like with the SLME method of extraction, all the terms used in unsatisfiable class or classes logic are included in this unsatisfiable module. We can then use Prot\u00e9g\u00e9 to dig a little deeper in this small module. This is especially useful when working with large ontologies and/or the HermiT reasoner, which both can take quite some time. By extracting a smaller module, we can run the reasoner again in Prot\u00e9g\u00e9 to get detailed explanations. In this case, we already know the problem, so we don't need to investigate any more. Now let's reason over the original ubiq-ligase-complex.owl and see what happens: robot reason --input ubiq-ligase-complex.owl --output reasoned.owl If you just open reasoned.owl in Prot\u00e9g\u00e9, you won't really notice a different between this and the input file unless you do some digging. This takes us to our next command... Diff \u00b6 The diff command can be used to compare the axioms in two ontologies to see what has been added and what has been removed. While the diffs on GitHub are useful for seeing what changed, it can be really tough for a human to read the raw OWL formats. Using ROBOT, we can output these diffs in a few different formats (using the --format option): - plain : plain text with just the added and removed axioms listed in OWL functional syntax (still tough for a human to read, but could be good for passing to other scripts) - pretty : similar to plain , but the IRIs are replaced with CURIEs and labels where available (still hard to read) - html : a nice, sharable HTML file with the diffs sorted by term - markdown : like the HTML diff, but in markdown for easy sharing on platforms like GitHub (perfect for pull requests!) We're going to generate an HTML diff of ubiq-ligase-complex.owl compared to the new reasoned.owl file to see what inferences have been asserted. diff takes a left (\"original\") and a right (\"new\") input to compare. robot diff --left ubiq-ligase-complex.owl \\ --right reasoned.owl \\ --format html \\ --output diff.html Open diff.html in your browser side-by-side with reasoned.owl and you can see how the changes look in both. Homework question : Running reason should assert inferences, yet there are some removed axioms in our diff. Why do you think these axioms were removed?","title":"ROBOT Mini-Tutorial 2 - Annotate, Merge, Reason and Diff"},{"location":"tutorial/robot_tutorial_2/#robot-tutorial-2-annotate-merge-reason-and-diff","text":"In week 6, we got some hands-on experience with ROBOT using convert , extract , and template . This week, we will learn four new ROBOT commands: * Annotate * Merge * Reason * Diff The goal of these and previous commands is to build up to creating an ontology release workflow. Before starting this tutorial, either: * make sure Docker is running and you are in the container * download and install ROBOT for your operating system To start, we will be working in the same folder as the first ROBOT Mini-Tutorial . Navigate to this folder in your terminal and list the contents of the current directory by running ls . You should see catalog-v001.xml listed as one of these files. We want to delete this so that we can fix the ontology IRI problem we ran into last week! Before going any further with this tutorial, do this by running either del catalog-v001.xml for Windows or rm catalog-v001.xml if you're using Docker, MacOS, or other Linux system.","title":"ROBOT Tutorial 2: Annotate, Merge, Reason and Diff"},{"location":"tutorial/robot_tutorial_2/#annotate","text":"The annotate command allows you to attach metadata to your ontology in the form of IRIs and ontology annotations. Like the annotations on a term, ontology annotations help users to understand how they can use the ontology.","title":"Annotate"},{"location":"tutorial/robot_tutorial_2/#ontology-iris","text":"As we discussed during previous parts of the course, ontology IRIs are very important! We saw how importing an ontology without an IRI into another ontology without an IRI can cause some problems in the catalog-v001.xml file. We're going to fix that problem by giving IRIs to both our animals.owl and animals2.owl files. Let's start with animals.owl : robot annotate --input animals.owl \\ --ontology-iri http://example.com/animals.owl \\ --output animals.owl You'll notice we gave the same file name as the input file; we're just updating our previous file so we don't need to do this in a separate OWL file. On your own, give animals2.owl the ontology IRI http://example.com/animals2.owl . Remember that, in reality, we always want our ontology IRIs to be resolvable , so these would be pretty bad IRIs for an actual ontology. Let's fix our import statement now. Open animals2.owl in Prot\u00e9g\u00e9 and go to the Entities tab. You'll see that even though we still have the import statement in the Active ontology tab, the top-level terms are no longer labeled. Since we changed the ontology IRI, Prot\u00e9g\u00e9 can no longer resolve our local file (because the catalog-v001.xml file was not updated). Go back to the Active ontology tab and click the X to the right of our original import. Then, re-add animals.owl as an import using the same steps as last time. When you return to the Entities tab, you'll once again see the labels of the top-level terms.","title":"Ontology IRIs"},{"location":"tutorial/robot_tutorial_2/#version-iris","text":"When we release our ontologies, we want to make sure to include a version IRI. Like the ontology IRI, this should always resolve to the version of the ontology at the time of the release. For clarity, we usually use dates in our version IRIs in the OBO Foundry. That way, you know when you navigate to a specific version IRI, that's what the ontology looked like on that date. (Note: edit files don't usually have version IRIs as they are always changing, and we don't expect to be able to point to a stable version) While you can add a version IRI in Prot\u00e9g\u00e9, if you're trying to create an automated release workflow, this is a manual step you don't want to have to include. Keeping it in your release workflow also makes sure that the verion IRIs are consistent (we'll see how to do this with make later). For now, let's add a version IRI to animals.owl (feel free to replace the 2021-05-20 with today's date): robot annotate --input animals.owl \\ --version-iri http://example.com/animals/2021-05-20/animals.owl \\ --output animals.owl Let's break down this version IRI. We have the host ( http://example.com/ ) followed by our ontology's namespace ( animals ). Next, we provided the date in the format of YYYY-MM-DD . Finally, we have the name of the file. This is standard for OBO Foundry, except with a different host. For example, you can find a release of OBI from April 6, 2021 at http://purl.obolibrary.org/obo/obi/2021-04-06/obi.owl . In this case, the host is http://purl.obolibrary.org/obo/ . Of course, you may see different patterns in non-OBO-Foundry ontologies, but they should always resolve (hopefully!). Go ahead and open or reload animals.owl in Protege. You'll see in the Active Ontology tab that now both the ontology IRI and version IRI fields are filled out.","title":"Version IRIs"},{"location":"tutorial/robot_tutorial_2/#ontology-annotations","text":"In addition to ontology and version IRIs, you may also want to add some other metadata to your ontology. For example, when we were introduced to report , we added a description to the ontology to fix one of the report problems. The three ontology annotations that are required by the OBO Foundry are: * Title ( dc11:title ) * License ( dc:license ) * Description ( dc11:description ) These three annotation properties all come from the Dublin Core , but they have slightly different namespaces. This is because DC is split into two parts: the /terms/ and /elements/1.1/ namespaces. Just remember to double check that you're using the correct namespace. If you click on the DC link, you can find the complete list of DC terms in their respective namespaces. ROBOT contains some built-in prefixes, which can be found here . The prefix dc: corresponds to the /terms/ namespace and dc11: to /elements/1.1/ . You may see different prefixes used (for example, /terms/ is sometimes dcterms: or just terms: ), but the full namespace is what really matters as long as the prefix is defined somewhere. Let's go ahead and add a title and description to our animals.owl file. We'll do this using the --annotation option, which expects two arguments: (1) the CURIE of the annotation property, (2) the value of the annotation. The value of the annotation must be enclosed in double quotes if there are spaces. You can use any annotation property you want here, and include as many as you want! For now, we'll start with two: robot annotate --input animals.owl \\ --annotation dc11:title \"Animal Ontology\" \\ --annotation dc11:description \"An ontology about animals\" \\ --output animals.owl --annotation adds these as strings, but remember that an annotation can also point to an link or IRI. We want our license to be a link, so we'll use --link-annotation instead to add that: robot annotate --input animals.owl \\ --link-annotation dc:license https://creativecommons.org/licenses/by/4.0/ \\ --output animals.owl OBO Foundry recommends using Creative Commons for all licenses. We just gave our ontology the most permissive of these, CC-BY . When you open animals.owl in Prot\u00e9g\u00e9 again, you'll see these annotations added to the Active ontology tab. You can also click on the CC-BY link!","title":"Ontology Annotations"},{"location":"tutorial/robot_tutorial_2/#merge","text":"We've already learned how to include external ontologies as imports. Usually, for the released version of an ontology, the imports are merged in so that all contents are in one file. Another reason you may want to merge two ontologies is if you're adding new terms to an ontology using template , like how we created new animal terms in animals2.tsv last time. We're going to demonstrate two methods of merging now. The first involves merging two (or more!) separate files and the second involves merging all imports into the current input ontology.","title":"Merge"},{"location":"tutorial/robot_tutorial_2/#merging-multiple-files","text":"First, copy animals2.owl to animals-new.owl . In Windows, this command is copy animals2.owl animals-new.owl . For Docker and other Linux operating systems, this is cp animals2.owl animals-new.owl . Open animals-new.owl in Prot\u00e9g\u00e9 and remove the import we added last time. This is done in the Imported ontologies section of the Active ontology tab. Just click the X on the right side of the imported animals ontology. Don't forget to save! Continuing with the animals.owl file we created last week, now run the following command: robot merge --input animals.owl --input animals-new.owl --output animals-full.owl When you just import an external ontology into your ontology, you'll notice in the Prot\u00e9g\u00e9 class hierarchy that all terms from the external ontology are a less-bold text than internal terms. This can be seen when you open animals2.owl , where we imported animals.owl . This is simply Prot\u00e9g\u00e9's way of telling us that these terms are not part of your current ontology . Now that we've merged these two ontologies together, when you open animals-full.owl in Prot\u00e9g\u00e9, you'll see that all the terms are bold. By default, the output ontology will get the ontology IRI of the first input ontology. We picked animals.owl as our first ontology here because this is the ontology that we're adding terms to, so we want our new output ontology to replace the original while keeping the same IRI. merge will also copy over all the ontology annotations from animals.owl (the first input) into the new file. The annotations from animals2.owl are ignored, but we'll talk more about this in our class session. If we were editing an ontology in the wild, we'd probably now replace the original with this new file using cp or copy . For now, don't replace animals.owl because we'll need it for this next part. IMPORTANT : Be very careful to check that the format is the same if you're replacing a file! Remember, you can always output OWL Functional syntax or another syntax by ending your output with .ofn , for example: --output animals-full.ofn .","title":"Merging Multiple Files"},{"location":"tutorial/robot_tutorial_2/#merging-imports","text":"When we want to merge all our imports into our working ontology, we call this collapsing the import closure . Luckily (since we're lazy), you don't need to type out each of your imports as an input to do this. We already have animals.owl imported into animals2.owl . Let's collapse the import closure: robot merge --input animals2.owl --collapse-import-closure true --output animals-full-2.owl Even though we gave this a different file name, if you open animals-full-2.owl in Prot\u00e9g\u00e9, you'll notice that it's exactly the same as animals-full.owl ! This is because we merged the same files together, just in a slightly different way. This time, though, the ontology IRI is the one for animals2.owl , not animals.owl . That is because that was our first input file.","title":"Merging Imports"},{"location":"tutorial/robot_tutorial_2/#reason","text":"As we saw in the prepwork for Week 5, running a reasoner in Prot\u00e9g\u00e9 creates an inferred class hierarchy. In the OBO Foundry, releases versions of ontologies usually have this inferred hierarchy asserted , so you see the full inferred hierarchy when you open the ontology without running the reasoner. ROBOT reason allows us to output a version of the ontology with these inferences asserted. As we discussed, ELK and HermiT are the two main reasoners you'll be using. Instead of using our example ontologies (the asserted and inferred hierarchies for these will look exactly the same), we're going to use another ontology from the Ontologies 101 tutorial from week 5. Navigate back to that directory and then navigate to BDK14_exercises/basic-classification . Like running the reasoner in Prot\u00e9g\u00e9, running reason does three things: 1. Check for inconsistency 2. Check for unsatisfiable classes 3. Assert the inferred class hierarchy Remember, when we run the reasoner in Prot\u00e9g\u00e9, if the ontology is inconsistent, reason will fail. If there are unsatisfiable classes, these will be asserted as owl:Nothing . ROBOT will always fail in both cases, but has some tools to help us figure out why. Let's introduce an unsatifiable class into our test and see what happens. First, let's make a copy of ubiq-ligase-complex.owl and call this new file unreasoned.owl ( copy or cp ). Open unreasoned.owl in Prot\u00e9g\u00e9 and follow the steps below. These are things we've covered in past exercises, but if you get stuck, please don't hesitate to reach out. Find 'organelle' in the class hierarchy below 'cellular_component' (or just search for it by label) Make 'organelle' disjoint with 'organelle part' (either use the class hierarchy or type it in the expression editor) Find 'intracellular organelle part' below 'intracellular part' or 'organelle part' (or search for it by label) Add 'organelle' as a parent class to 'intracellular organelle part' (remember that you only need to include the single quotes if the label has spaces) Like we did in the Disjointness part of the Ontologies 101 tutorial, we've made 'intracellular organelle part' a subclass of two classes that should have no overlap based on the disjointness axiom. Save the ontology and return to your terminal. Now, we'll run reason . The default reasoner is ELK, but you can specify the reasoner you want to use with the --reasoner option. For now, we'll just use ELK. robot reason --input unreasoned.owl --output unsatisfiable.owl You'll notice that ROBOT printed an error message telling us that the term with the IRI http://purl.obolibrary.org/obo/GO_0044446 is unsatisfiable and ROBOT didn't create unsatisfiable.owl . This is ideal for automated pipelines where we don't want to be releasing unsatisfiable classes. We can still use ROBOT to investigate the issue, though. It already gave us the IRI, but we can get more details using the --dump-unsatisfiable option. We won't provide an output this time because we know it won't succeed. robot reason --input unreasoned.owl --dump-unsatisfiable unsatisfiable.owl You can open unsatisfiable.owl in Prot\u00e9g\u00e9 and see that 'intracellular organelle part' is not the only term included, even though it was the only unsatisfiable class. Like with the SLME method of extraction, all the terms used in unsatisfiable class or classes logic are included in this unsatisfiable module. We can then use Prot\u00e9g\u00e9 to dig a little deeper in this small module. This is especially useful when working with large ontologies and/or the HermiT reasoner, which both can take quite some time. By extracting a smaller module, we can run the reasoner again in Prot\u00e9g\u00e9 to get detailed explanations. In this case, we already know the problem, so we don't need to investigate any more. Now let's reason over the original ubiq-ligase-complex.owl and see what happens: robot reason --input ubiq-ligase-complex.owl --output reasoned.owl If you just open reasoned.owl in Prot\u00e9g\u00e9, you won't really notice a different between this and the input file unless you do some digging. This takes us to our next command...","title":"Reason"},{"location":"tutorial/robot_tutorial_2/#diff","text":"The diff command can be used to compare the axioms in two ontologies to see what has been added and what has been removed. While the diffs on GitHub are useful for seeing what changed, it can be really tough for a human to read the raw OWL formats. Using ROBOT, we can output these diffs in a few different formats (using the --format option): - plain : plain text with just the added and removed axioms listed in OWL functional syntax (still tough for a human to read, but could be good for passing to other scripts) - pretty : similar to plain , but the IRIs are replaced with CURIEs and labels where available (still hard to read) - html : a nice, sharable HTML file with the diffs sorted by term - markdown : like the HTML diff, but in markdown for easy sharing on platforms like GitHub (perfect for pull requests!) We're going to generate an HTML diff of ubiq-ligase-complex.owl compared to the new reasoned.owl file to see what inferences have been asserted. diff takes a left (\"original\") and a right (\"new\") input to compare. robot diff --left ubiq-ligase-complex.owl \\ --right reasoned.owl \\ --format html \\ --output diff.html Open diff.html in your browser side-by-side with reasoned.owl and you can see how the changes look in both. Homework question : Running reason should assert inferences, yet there are some removed axioms in our diff. Why do you think these axioms were removed?","title":"Diff"},{"location":"tutorial/robot_tutorial_qc/","text":"ROBOT Tutorial: Quality Control with report and query \u00b6 Review & answer questions about the homework (~30 minutes) OWL axioms (restrictions, equivalent & disjoint class axioms) Reasoning: classification and debugging ROBOT report (~45 minutes) Download example.owl , or get it via the command line: curl https://raw.githubusercontent.com/jamesaoverton/obook/master/docs/tutorial/robot_tutorial_qc/example.owl > example.owl What report checks for Let's try it out! Hands-on practice fixing common errors ROBOT query (~45 minutes) Writing simple SPARQL queries (review) Let's try it out!","title":"ROBOT Mini-Tutorial QC - Quality Control with report and query"},{"location":"tutorial/robot_tutorial_qc/#robot-tutorial-quality-control-with-report-and-query","text":"Review & answer questions about the homework (~30 minutes) OWL axioms (restrictions, equivalent & disjoint class axioms) Reasoning: classification and debugging ROBOT report (~45 minutes) Download example.owl , or get it via the command line: curl https://raw.githubusercontent.com/jamesaoverton/obook/master/docs/tutorial/robot_tutorial_qc/example.owl > example.owl What report checks for Let's try it out! Hands-on practice fixing common errors ROBOT query (~45 minutes) Writing simple SPARQL queries (review) Let's try it out!","title":"ROBOT Tutorial: Quality Control with report and query"},{"location":"tutorial/setting-up-project-odk/","text":"Tutorial: How to get started with your own ODK-style repository \u00b6 Preparation: Installing docker, installing ODK and setting memory. Follow the steps here . Creating your first ontology repository The tutorial uses example tailored for users of UNIX systems, like Mac and Linux. Users of Windows generally have analogous steps - wherever we talk about an sh file in the following there exists a corresponding bat file that can be run in the windows powershell, or CMD. Prerequisites \u00b6 You have: A Github account Completed the \"Preparation\" steps above Video \u00b6 A recording of a demo of creating a ODK-repo is available here Your first repository \u00b6 Create temporary directory to get started On your machine, create a new folder somewhere: cd ~ mkdir odk_tutorial cd odk_tutorial Download the seed-my-repo wrapper script Now download the seed-my-repo wrapper script from the ODK GitHub repository. A detailed explanation of how to do that can be found here . For simplicity, we just use wget here to download the seed-my-repo file, but you can do it manually: wget https://raw.githubusercontent.com/INCATools/ontology-development-kit/master/seed-via-docker.sh Download a basic config to start from and start building your own The last ingredient we need is an ODK config file. While you can, in theory, create an empty repo entirely without a config file (one will be generated for you), we recommend to just start right with one. You can find many examples of configs here . For the sake of this tutorial, we will start with a simple config: id: cato title: \"Cat Anatomy Ontology\" github_org: obophenotype git_main_branch: main repo: cat_anatomy_ontology release_artefacts: - base - full - simple primary_release: full export_formats: - owl - obo - json import_group: products: - id: ro - id: pato - id: omo robot_java_args: '-Xmx8G' Safe this config file as in your temporary directory, e.g. ~/odk_tutorial/cato-odk.yaml . Most of your work managing your ODK in the future will involve editing this file. There are dozens of cool options that do magical things in there. For now, lets focus on the most essential: General config: \u00b6 id: cato title: \"Cat Anatomy Ontology\" The id is essential, as it will determine how files will be named, which default term IDs to assume, and many more. It should be a lowercase string which is, by convention at least 4 characters long - 5 is not unheard of. The title field is used to generate various default values in the repository, like the README and others. There are other fields, like description , but let's start minimal for now. A full list of elements can be found in this schema: https://github.com/INCATools/ontology-development-kit/blob/master/schema/project-schema.json Git config: \u00b6 github_org: obophenotype git_main_branch: main repo: cat_anatomy_ontology The github_org (the GitHub or GitLab organisation) and the repo (repository name) will be used for some basic config of the git repo. Enter your own github_org here rather than obophenotype . Your default github_org is your GitHub username. If you are not creating a new repo, but working on a repo that predates renaming the GitHub main branch from master to main , you may want to set the git_main_branch as well. Pipeline configuration \u00b6 release_artefacts: - base - full - simple primary_release: full export_formats: - owl - obo - json With this configuration, we tell the ODK that we wish to automatically generate the base, full and simple release files for our ontology. We also say that we want the primary_release to be the full release (which is also the default). The primary release will be materialised as cato.owl , and is what most users of your ontology will interact with. More information and what these are can be found here . We always want to create a base , i.e. the release variant that contains all the axioms that belong to the ontology, and none of the imported ones, but we do not want to make it the primary_release , because it will be unclassified and missing a lot of the important inferences. We also configure export products: we always want to export to OWL ( owl ), but we can also chose to export to OBO ( obo ) format and OBOGraphs JSON ( json ). Imports config: \u00b6 import_group: products: - id: ro - id: pato - id: omo This is a central part of the ODK, and the section of the config file you will interact with the most. Please see here for details. What we are asking the ODK here, in essence, to set us up for dynamically importing from the Relation Ontology (RO), the Phenotype And Trait Ontology (PATO) and the OBO Metadata Ontology (OMO). Memory management: \u00b6 robot_java_args: '-Xmx8G' Here we say that we allow ROBOT to consume up to 8GB of memory. Make sure that your docker is set up to permit at least ~20% more memory than that, i.e. 9GB or 10GB, otherwise, some cryptic Docker errors may come up. Generate the repo \u00b6 Run the following: cd ~/odk_tutorial sh seed-via-docker.sh -c -C cato-odk.yaml This will create a basic layout of your repo under target/cato/* Note: after this run, you wont need cato-odk.yaml anymore as it will have been added to your ontology repo, which we will see later. Publish on GitHub \u00b6 You can now move the target/cato directory to a more suitable location. For the sake of this tutorial we will move it to the Home directory. mv target/cato ~/ Using GitHub Desktop \u00b6 If you use GitHub Desktop, you can now simply add this repo by selecting File -> Add local repository and select the directory you moved the repo to (as an aside, you should really have a nice workspace directory like ~/git or ~/ws or some such to organise your projects). Then click Publish the repository on Using the Command Line \u00b6 Follow the instructions you see on the Terminal (they are printed after your seed-my-repo run). Finish! \u00b6 Congratulations, you have successfully jump-started your very own ODK repository and can start developing. Next steps: \u00b6 Start editing ~/cato/src/ontology/cato-edit.owl using Protege. Run a release","title":"ODK - Getting started with your own repo"},{"location":"tutorial/setting-up-project-odk/#tutorial-how-to-get-started-with-your-own-odk-style-repository","text":"Preparation: Installing docker, installing ODK and setting memory. Follow the steps here . Creating your first ontology repository The tutorial uses example tailored for users of UNIX systems, like Mac and Linux. Users of Windows generally have analogous steps - wherever we talk about an sh file in the following there exists a corresponding bat file that can be run in the windows powershell, or CMD.","title":"Tutorial: How to get started with your own ODK-style repository"},{"location":"tutorial/setting-up-project-odk/#prerequisites","text":"You have: A Github account Completed the \"Preparation\" steps above","title":"Prerequisites"},{"location":"tutorial/setting-up-project-odk/#video","text":"A recording of a demo of creating a ODK-repo is available here","title":"Video"},{"location":"tutorial/setting-up-project-odk/#your-first-repository","text":"Create temporary directory to get started On your machine, create a new folder somewhere: cd ~ mkdir odk_tutorial cd odk_tutorial Download the seed-my-repo wrapper script Now download the seed-my-repo wrapper script from the ODK GitHub repository. A detailed explanation of how to do that can be found here . For simplicity, we just use wget here to download the seed-my-repo file, but you can do it manually: wget https://raw.githubusercontent.com/INCATools/ontology-development-kit/master/seed-via-docker.sh Download a basic config to start from and start building your own The last ingredient we need is an ODK config file. While you can, in theory, create an empty repo entirely without a config file (one will be generated for you), we recommend to just start right with one. You can find many examples of configs here . For the sake of this tutorial, we will start with a simple config: id: cato title: \"Cat Anatomy Ontology\" github_org: obophenotype git_main_branch: main repo: cat_anatomy_ontology release_artefacts: - base - full - simple primary_release: full export_formats: - owl - obo - json import_group: products: - id: ro - id: pato - id: omo robot_java_args: '-Xmx8G' Safe this config file as in your temporary directory, e.g. ~/odk_tutorial/cato-odk.yaml . Most of your work managing your ODK in the future will involve editing this file. There are dozens of cool options that do magical things in there. For now, lets focus on the most essential:","title":"Your first repository"},{"location":"tutorial/setting-up-project-odk/#general-config","text":"id: cato title: \"Cat Anatomy Ontology\" The id is essential, as it will determine how files will be named, which default term IDs to assume, and many more. It should be a lowercase string which is, by convention at least 4 characters long - 5 is not unheard of. The title field is used to generate various default values in the repository, like the README and others. There are other fields, like description , but let's start minimal for now. A full list of elements can be found in this schema: https://github.com/INCATools/ontology-development-kit/blob/master/schema/project-schema.json","title":"General config:"},{"location":"tutorial/setting-up-project-odk/#git-config","text":"github_org: obophenotype git_main_branch: main repo: cat_anatomy_ontology The github_org (the GitHub or GitLab organisation) and the repo (repository name) will be used for some basic config of the git repo. Enter your own github_org here rather than obophenotype . Your default github_org is your GitHub username. If you are not creating a new repo, but working on a repo that predates renaming the GitHub main branch from master to main , you may want to set the git_main_branch as well.","title":"Git config:"},{"location":"tutorial/setting-up-project-odk/#pipeline-configuration","text":"release_artefacts: - base - full - simple primary_release: full export_formats: - owl - obo - json With this configuration, we tell the ODK that we wish to automatically generate the base, full and simple release files for our ontology. We also say that we want the primary_release to be the full release (which is also the default). The primary release will be materialised as cato.owl , and is what most users of your ontology will interact with. More information and what these are can be found here . We always want to create a base , i.e. the release variant that contains all the axioms that belong to the ontology, and none of the imported ones, but we do not want to make it the primary_release , because it will be unclassified and missing a lot of the important inferences. We also configure export products: we always want to export to OWL ( owl ), but we can also chose to export to OBO ( obo ) format and OBOGraphs JSON ( json ).","title":"Pipeline configuration"},{"location":"tutorial/setting-up-project-odk/#imports-config","text":"import_group: products: - id: ro - id: pato - id: omo This is a central part of the ODK, and the section of the config file you will interact with the most. Please see here for details. What we are asking the ODK here, in essence, to set us up for dynamically importing from the Relation Ontology (RO), the Phenotype And Trait Ontology (PATO) and the OBO Metadata Ontology (OMO).","title":"Imports config:"},{"location":"tutorial/setting-up-project-odk/#memory-management","text":"robot_java_args: '-Xmx8G' Here we say that we allow ROBOT to consume up to 8GB of memory. Make sure that your docker is set up to permit at least ~20% more memory than that, i.e. 9GB or 10GB, otherwise, some cryptic Docker errors may come up.","title":"Memory management:"},{"location":"tutorial/setting-up-project-odk/#generate-the-repo","text":"Run the following: cd ~/odk_tutorial sh seed-via-docker.sh -c -C cato-odk.yaml This will create a basic layout of your repo under target/cato/* Note: after this run, you wont need cato-odk.yaml anymore as it will have been added to your ontology repo, which we will see later.","title":"Generate the repo"},{"location":"tutorial/setting-up-project-odk/#publish-on-github","text":"You can now move the target/cato directory to a more suitable location. For the sake of this tutorial we will move it to the Home directory. mv target/cato ~/","title":"Publish on GitHub"},{"location":"tutorial/setting-up-project-odk/#using-github-desktop","text":"If you use GitHub Desktop, you can now simply add this repo by selecting File -> Add local repository and select the directory you moved the repo to (as an aside, you should really have a nice workspace directory like ~/git or ~/ws or some such to organise your projects). Then click Publish the repository on","title":"Using GitHub Desktop"},{"location":"tutorial/setting-up-project-odk/#using-the-command-line","text":"Follow the instructions you see on the Terminal (they are printed after your seed-my-repo run).","title":"Using the Command Line"},{"location":"tutorial/setting-up-project-odk/#finish","text":"Congratulations, you have successfully jump-started your very own ODK repository and can start developing.","title":"Finish!"},{"location":"tutorial/setting-up-project-odk/#next-steps","text":"Start editing ~/cato/src/ontology/cato-edit.owl using Protege. Run a release","title":"Next steps:"},{"location":"tutorial/sparql/","text":"Basic SPARQL for OBO Engineers \u00b6 In this tutorial we introduce SPARQL, with a particular spin on how we use it across OBO ontologies. Preparation \u00b6 Watch Linked Data Engineering: Querying RDF with SPARQL Complete Running Basic SPARQL Queries tutorial (~45 minutes - 1 hour) SPARQL tools for OBO Engineers \u00b6 RENCI Ubergraph Endpoint : Many key OBO ontologies are loaded here with lots of materialised inferences ( docs ). Ontobee SPARQL endpoint : Useful to run queries across all OBO Foundry ontologies. Yasgui : Yasgui is a simple and beautiful front-end for SPARQL endpoints which can be used not only to query, but also to share queries with others. For example this simple SPARQL query runs across the RENCI Ubergraph Endpoint. ROBOT query : ROBOT method to generate TSV reports from SPARQL queries, and applying data transformations ( --update ). ROBOT uses Jena internally to execute SPARQL queries. ROBOT verify : ROBOT method to run SPARQL QC queries. If the query returns a result, the QC test fails. ROBOT report : ROBOT report is a more powerful approach to running OBO QC queries. The default OBO report which ships with ROBOT can be customised by changing the error level, removing a test entirely and even extending the report to custom (SPARQL) checks. Robot report can generate beautiful HTML reports which are easy to read. SPARQL in the OBO-sphere \u00b6 SPARQL has many uses in the OBO-sphere, but the following in particular: Quality control checking Creating summary tables for ontologies Sophisticated data transformations in ontology pipelines We will discuss each of these in the following and give examples. An informal discussion of SPARQL in OBO can be followed here: Quality control checking \u00b6 For us, ROBOT + SPARQL were a game changer for our quality control (QC) pipelines. This is how this works. First, we encode the error in the form of a SPARQL query. For example, the following check simply looks for entities that have more than one definition : PREFIX obo: <http://purl.obolibrary.org/obo/> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT DISTINCT ?entity ?property ?value WHERE { VALUES ?property { obo:IAO_0000115 obo:IAO_0000600 } ?entity ?property ?value . ?entity ?property ?value2 . FILTER (?value != ?value2) FILTER NOT EXISTS { ?entity owl:deprecated true } FILTER (!isBlank(?entity)) } ORDER BY ?entity This is a typical workflow. Think of an ontology editor working on an ontology. Often, that curator notices that the same problem happens repeatedly and tell us, the Ontology Pipeline Developer, that they would like a check to prevent the error. We then capture the erroneous situation as a SPARQL query. Then, we add it to our ontology repository , and execute it with ROBOT report or ROBOT verify (see above) in our CI pipelines, usually based on GitHub actions or Travis. Note that the Ontology Development Kit provides a built-in framework for for such queries build on ROBOT verify and report. Creating summary tables for ontologies \u00b6 Many times, we need to create tabular reports of our ontologies to share with stakeholders or to help with internal reviews, e.g.: create lists of ontology terms with their definitions and labels create summaries of ontologies, like aggregate statistics Sometimes using Yasgui, for example in conjunction with the RENCI Ubergraph Endpoint, is enough, but often, using ROBOT query is the better choice, especially if you want to make sure the right version of the ontology is used (Ubergraph occasionally is out of date). Using ROBOT in conjunction with a Workflows Automation system like Github actions helps with generating up-to-date reports. Here is an example of a GitHub action that generates a few reports with ROBOT and pushes them back to the repository. Sophisticated data transformations in ontology pipelines \u00b6 Lastly, we use ROBOT query to implement complex ontology transformation processes. For example the following complex query transforms related synonyms to exact synonyms if some complex condition is met: prefix owl: <http://www.w3.org/2002/07/owl#> prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> DELETE { ?term oboInOwl:hasRelatedSynonym ?related . ?relax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasRelatedSynonym ; owl:annotatedTarget ?related ; oboInOwl:hasDbXref ?xref2 . } INSERT { ?relax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasExactSynonym ; owl:annotatedTarget ?related ; oboInOwl:hasDbXref ?xref2 . } WHERE { { ?term oboInOwl:hasRelatedSynonym ?related ; oboInOwl:hasExactSynonym ?exact ; a owl:Class . ?exax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasExactSynonym ; owl:annotatedTarget ?exact ; oboInOwl:hasDbXref ?xref1 . ?relax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasRelatedSynonym ; owl:annotatedTarget ?related ; oboInOwl:hasDbXref ?xref2 . FILTER (str(?related)=str(?exact)) FILTER (isIRI(?term) && regex(str(?term), \"^http://purl.obolibrary.org/obo/MONDO_\")) } }","title":"Basic SPARQL for OBO Engineers"},{"location":"tutorial/sparql/#basic-sparql-for-obo-engineers","text":"In this tutorial we introduce SPARQL, with a particular spin on how we use it across OBO ontologies.","title":"Basic SPARQL for OBO Engineers"},{"location":"tutorial/sparql/#preparation","text":"Watch Linked Data Engineering: Querying RDF with SPARQL Complete Running Basic SPARQL Queries tutorial (~45 minutes - 1 hour)","title":"Preparation"},{"location":"tutorial/sparql/#sparql-tools-for-obo-engineers","text":"RENCI Ubergraph Endpoint : Many key OBO ontologies are loaded here with lots of materialised inferences ( docs ). Ontobee SPARQL endpoint : Useful to run queries across all OBO Foundry ontologies. Yasgui : Yasgui is a simple and beautiful front-end for SPARQL endpoints which can be used not only to query, but also to share queries with others. For example this simple SPARQL query runs across the RENCI Ubergraph Endpoint. ROBOT query : ROBOT method to generate TSV reports from SPARQL queries, and applying data transformations ( --update ). ROBOT uses Jena internally to execute SPARQL queries. ROBOT verify : ROBOT method to run SPARQL QC queries. If the query returns a result, the QC test fails. ROBOT report : ROBOT report is a more powerful approach to running OBO QC queries. The default OBO report which ships with ROBOT can be customised by changing the error level, removing a test entirely and even extending the report to custom (SPARQL) checks. Robot report can generate beautiful HTML reports which are easy to read.","title":"SPARQL tools for OBO Engineers"},{"location":"tutorial/sparql/#sparql-in-the-obo-sphere","text":"SPARQL has many uses in the OBO-sphere, but the following in particular: Quality control checking Creating summary tables for ontologies Sophisticated data transformations in ontology pipelines We will discuss each of these in the following and give examples. An informal discussion of SPARQL in OBO can be followed here:","title":"SPARQL in the OBO-sphere"},{"location":"tutorial/sparql/#quality-control-checking","text":"For us, ROBOT + SPARQL were a game changer for our quality control (QC) pipelines. This is how this works. First, we encode the error in the form of a SPARQL query. For example, the following check simply looks for entities that have more than one definition : PREFIX obo: <http://purl.obolibrary.org/obo/> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT DISTINCT ?entity ?property ?value WHERE { VALUES ?property { obo:IAO_0000115 obo:IAO_0000600 } ?entity ?property ?value . ?entity ?property ?value2 . FILTER (?value != ?value2) FILTER NOT EXISTS { ?entity owl:deprecated true } FILTER (!isBlank(?entity)) } ORDER BY ?entity This is a typical workflow. Think of an ontology editor working on an ontology. Often, that curator notices that the same problem happens repeatedly and tell us, the Ontology Pipeline Developer, that they would like a check to prevent the error. We then capture the erroneous situation as a SPARQL query. Then, we add it to our ontology repository , and execute it with ROBOT report or ROBOT verify (see above) in our CI pipelines, usually based on GitHub actions or Travis. Note that the Ontology Development Kit provides a built-in framework for for such queries build on ROBOT verify and report.","title":"Quality control checking"},{"location":"tutorial/sparql/#creating-summary-tables-for-ontologies","text":"Many times, we need to create tabular reports of our ontologies to share with stakeholders or to help with internal reviews, e.g.: create lists of ontology terms with their definitions and labels create summaries of ontologies, like aggregate statistics Sometimes using Yasgui, for example in conjunction with the RENCI Ubergraph Endpoint, is enough, but often, using ROBOT query is the better choice, especially if you want to make sure the right version of the ontology is used (Ubergraph occasionally is out of date). Using ROBOT in conjunction with a Workflows Automation system like Github actions helps with generating up-to-date reports. Here is an example of a GitHub action that generates a few reports with ROBOT and pushes them back to the repository.","title":"Creating summary tables for ontologies"},{"location":"tutorial/sparql/#sophisticated-data-transformations-in-ontology-pipelines","text":"Lastly, we use ROBOT query to implement complex ontology transformation processes. For example the following complex query transforms related synonyms to exact synonyms if some complex condition is met: prefix owl: <http://www.w3.org/2002/07/owl#> prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> DELETE { ?term oboInOwl:hasRelatedSynonym ?related . ?relax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasRelatedSynonym ; owl:annotatedTarget ?related ; oboInOwl:hasDbXref ?xref2 . } INSERT { ?relax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasExactSynonym ; owl:annotatedTarget ?related ; oboInOwl:hasDbXref ?xref2 . } WHERE { { ?term oboInOwl:hasRelatedSynonym ?related ; oboInOwl:hasExactSynonym ?exact ; a owl:Class . ?exax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasExactSynonym ; owl:annotatedTarget ?exact ; oboInOwl:hasDbXref ?xref1 . ?relax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasRelatedSynonym ; owl:annotatedTarget ?related ; oboInOwl:hasDbXref ?xref2 . FILTER (str(?related)=str(?exact)) FILTER (isIRI(?term) && regex(str(?term), \"^http://purl.obolibrary.org/obo/MONDO_\")) } }","title":"Sophisticated data transformations in ontology pipelines"}]}